{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "class LQR:\n",
    "    def __init__(self, H: torch.Tensor, M: torch.Tensor, sigma: torch.Tensor, \n",
    "                 C: torch.Tensor, D: torch.Tensor, R: torch.Tensor, \n",
    "                 T: float, time_grid: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Initialize the LQR problem.\n",
    "        \n",
    "        Args:\n",
    "            H: System dynamics matrix (d x d)\n",
    "            M: Control input matrix (d x m)\n",
    "            sigma: Noise matrix (d x d')\n",
    "            C: State cost matrix (d x d)\n",
    "            D: Control cost matrix (m x m)\n",
    "            R: Terminal state cost matrix (d x d)\n",
    "            T: Terminal time\n",
    "            time_grid: Grid of time points\n",
    "        \"\"\"\n",
    "        self.H = H\n",
    "        self.M = M\n",
    "        self.sigma = sigma\n",
    "        self.C = C\n",
    "        self.D = D\n",
    "        self.R = R\n",
    "        self.T = T\n",
    "        self.time_grid = time_grid\n",
    "        \n",
    "        # Check dimensions\n",
    "        d, m = M.shape\n",
    "        assert H.shape == (d, d), \"H must be d x d\"\n",
    "        assert sigma.shape[0] == d, \"sigma must be d x d'\"\n",
    "        assert C.shape == (d, d), \"C must be d x d\"\n",
    "        assert D.shape == (m, m), \"D must be m x m\"\n",
    "        assert R.shape == (d, d), \"R must be d x d\"\n",
    "        \n",
    "        # Store dimensions\n",
    "        self.d = d\n",
    "        self.m = m\n",
    "        \n",
    "        # Compute inverse of D once for efficiency\n",
    "        self.D_inv = torch.inverse(D)\n",
    "        \n",
    "        # Initialize solution placeholders\n",
    "        self.S_grid = None  # Will be populated when solve_ricatti is called\n",
    "        self.int_term_grid = None  # Will store the integral term\n",
    "        \n",
    "    def ricatti_rhs(self, t: float, S_flat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Right-hand side of the Ricatti ODE: S'(t) = S(t)MD^(-1)M^TS(t) - H^TS(t) - S(t)H - C\n",
    "        \n",
    "        Args:\n",
    "            t: Time\n",
    "            S_flat: Flattened S matrix\n",
    "            \n",
    "        Returns:\n",
    "            Flattened derivative of S\n",
    "        \"\"\"\n",
    "        # Reshape S from flattened form\n",
    "        S = torch.tensor(S_flat.reshape(self.d, self.d), dtype=torch.float64)\n",
    "        \n",
    "        # Compute right-hand side\n",
    "        term1 = S @ self.M @ self.D_inv @ self.M.T @ S\n",
    "        term2 = self.H.T @ S\n",
    "        term3 = S @ self.H\n",
    "        term4 = self.C\n",
    "        \n",
    "        # Compute derivative\n",
    "        dS = term1 - term2 - term3 - term4\n",
    "        \n",
    "        # Return flattened result\n",
    "        return dS.flatten().numpy()\n",
    "    \n",
    "    def solve_ricatti(self) -> None:\n",
    "        \"\"\"\n",
    "        Solve the Ricatti ODE using scipy's solve_ivp for high accuracy.\n",
    "        \"\"\"\n",
    "        # Convert matrices to double precision if they aren't already\n",
    "        self.H = self.H.to(torch.float64)\n",
    "        self.M = self.M.to(torch.float64)\n",
    "        self.sigma = self.sigma.to(torch.float64)\n",
    "        self.C = self.C.to(torch.float64)\n",
    "        self.D = self.D.to(torch.float64)\n",
    "        self.R = self.R.to(torch.float64)\n",
    "        self.D_inv = self.D_inv.to(torch.float64)\n",
    "        \n",
    "        # Terminal condition: S(T) = R\n",
    "        S_T_flat = self.R.flatten().numpy()\n",
    "        \n",
    "        # Time points for ODE solver (reversed for backward integration)\n",
    "        t_points = self.time_grid.numpy()\n",
    "        t_reversed = self.T - t_points[::-1]\n",
    "        \n",
    "        # Solve the ODE backward in time (from T to 0)\n",
    "        solution = solve_ivp(\n",
    "            lambda t, y: -self.ricatti_rhs(self.T - t, y),  # Negative for backward integration\n",
    "            [0, self.T],\n",
    "            S_T_flat,\n",
    "            t_eval=t_reversed,\n",
    "            method='RK45',\n",
    "            rtol=1e-11,\n",
    "            atol=1e-11\n",
    "        )\n",
    "        \n",
    "        # Convert solution back to PyTorch tensors and reshape\n",
    "        S_values = solution.y.T\n",
    "        S_matrices = [S.reshape(self.d, self.d) for S in S_values]\n",
    "        S_matrices.reverse()  # Reverse back to forward time\n",
    "        \n",
    "        self.S_grid = torch.tensor(S_matrices, dtype=torch.float64)\n",
    "        \n",
    "        # Compute integral term for value function\n",
    "        self.compute_integral_term()\n",
    "    \n",
    "    def compute_integral_term(self) -> None:\n",
    "        \"\"\"\n",
    "        Compute the integral term for the value function: int_t^T tr(sigma sigma^T S(r)) dr\n",
    "        \"\"\"\n",
    "        if self.S_grid is None:\n",
    "            self.solve_ricatti()\n",
    "        \n",
    "        # Compute trace term at each time point\n",
    "        trace_terms = torch.zeros(len(self.time_grid))\n",
    "        sigma_sigma_T = self.sigma @ self.sigma.T\n",
    "        \n",
    "        for i in range(len(self.time_grid)):\n",
    "            trace_terms[i] = torch.trace(sigma_sigma_T @ self.S_grid[i])\n",
    "        \n",
    "        # Compute integral using trapezoidal rule (backward from T)\n",
    "        integral_term = torch.zeros(len(self.time_grid))\n",
    "        \n",
    "        for i in range(len(self.time_grid) - 1, 0, -1):\n",
    "            dt = self.time_grid[i] - self.time_grid[i-1]\n",
    "            integral_term[i-1] = integral_term[i] + 0.5 * (trace_terms[i] + trace_terms[i-1]) * dt\n",
    "        \n",
    "        self.int_term_grid = integral_term\n",
    "    \n",
    "    def get_S_at_time(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get the S matrix at a given time by finding the nearest time point in the grid.\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            \n",
    "        Returns:\n",
    "            S matrices at the specified times (batch x d x d)\n",
    "        \"\"\"\n",
    "        if self.S_grid is None:\n",
    "            self.solve_ricatti()\n",
    "            \n",
    "        # Find indices of nearest time points for each t\n",
    "        indices = torch.argmin(torch.abs(t.unsqueeze(1) - self.time_grid.unsqueeze(0)), dim=1)\n",
    "        \n",
    "        # Get the corresponding S matrices\n",
    "        return self.S_grid[indices]\n",
    "    \n",
    "    def get_integral_term_at_time(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get the integral term at a given time by finding the nearest time point in the grid.\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            \n",
    "        Returns:\n",
    "            Integral terms at the specified times (batch)\n",
    "        \"\"\"\n",
    "        if self.int_term_grid is None:\n",
    "            self.compute_integral_term()\n",
    "            \n",
    "        # Find indices of nearest time points for each t\n",
    "        indices = torch.argmin(torch.abs(t.unsqueeze(1) - self.time_grid.unsqueeze(0)), dim=1)\n",
    "        \n",
    "        # Get the corresponding integral terms\n",
    "        return self.int_term_grid[indices]\n",
    "    \n",
    "    def value_function(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the value function v(t, x) = x^T S(t) x + int_t^T tr(sigma sigma^T S(r)) dr\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            x: State tensor (batch x d)\n",
    "            \n",
    "        Returns:\n",
    "            Value function at (t, x) (batch)\n",
    "        \"\"\"\n",
    "        # Get S matrices at the specified times\n",
    "        S_matrices = self.get_S_at_time(t)\n",
    "        \n",
    "        # Compute quadratic term x^T S(t) x\n",
    "        batch_size = x.shape[0]\n",
    "        values = torch.zeros(batch_size, device=x.device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            values[i] = x[i] @ S_matrices[i] @ x[i]\n",
    "        \n",
    "        # Add integral term\n",
    "        values = values + self.get_integral_term_at_time(t)\n",
    "        \n",
    "        return values\n",
    "    \n",
    "    def optimal_control(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the optimal control a(t, x) = -D^(-1)M^TS(t)x\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            x: State tensor (batch x d)\n",
    "            \n",
    "        Returns:\n",
    "            Optimal control at (t, x) (batch x m)\n",
    "        \"\"\"\n",
    "        # Get S matrices at the specified times\n",
    "        S_matrices = self.get_S_at_time(t)\n",
    "        \n",
    "        # Compute optimal control for each (t, x) pair\n",
    "        batch_size = x.shape[0]\n",
    "        controls = torch.zeros((batch_size, self.m), device=x.device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            controls[i] = -self.D_inv @ self.M.T @ S_matrices[i] @ x[i]\n",
    "        \n",
    "        return controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_sde_explicit(lqr: LQR, x0: torch.Tensor, num_steps: int, num_samples: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Simulate the controlled SDE using the explicit scheme.\n",
    "    \n",
    "    Args:\n",
    "        lqr: LQR instance\n",
    "        x0: Initial state (batch x d)\n",
    "        num_steps: Number of time steps\n",
    "        num_samples: Number of Monte Carlo samples\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (trajectory, cost)\n",
    "    \"\"\"\n",
    "    dt = lqr.T / num_steps\n",
    "    t_grid = torch.linspace(0, lqr.T, num_steps + 1, dtype=torch.float64)\n",
    "    \n",
    "    batch_size = x0.shape[0]\n",
    "    d = lqr.d\n",
    "    \n",
    "    # Ensure all tensors are double precision\n",
    "    H = lqr.H.to(torch.float64)\n",
    "    M = lqr.M.to(torch.float64)\n",
    "    sigma = lqr.sigma.to(torch.float64)\n",
    "    C = lqr.C.to(torch.float64)\n",
    "    D = lqr.D.to(torch.float64)\n",
    "    \n",
    "    # Initialize trajectories and costs\n",
    "    X = torch.zeros((num_samples, batch_size, num_steps + 1, d), dtype=torch.float64)\n",
    "    X[:, :, 0, :] = x0.unsqueeze(0).repeat(num_samples, 1, 1)\n",
    "    \n",
    "    costs = torch.zeros((num_samples, batch_size), dtype=torch.float64)\n",
    "    \n",
    "    # Generate Brownian increments\n",
    "    dW = torch.randn((num_samples, batch_size, num_steps, sigma.shape[1]), dtype=torch.float64) * np.sqrt(dt)\n",
    "    \n",
    "    # Simulate trajectories\n",
    "    for n in range(num_steps):\n",
    "        t_n = t_grid[n]\n",
    "        X_n = X[:, :, n, :]\n",
    "        \n",
    "        # Reshape for batch processing\n",
    "        X_n_flat = X_n.reshape(-1, d)\n",
    "        t_flat = t_n.repeat(num_samples * batch_size)\n",
    "        \n",
    "        # Compute optimal control\n",
    "        control_flat = lqr.optimal_control(t_flat, X_n_flat).to(torch.float64)\n",
    "        control = control_flat.reshape(num_samples, batch_size, lqr.m)\n",
    "        \n",
    "        # Compute drift and apply update for each sample\n",
    "        for i in range(num_samples):\n",
    "            for j in range(batch_size):\n",
    "                # Compute drift term: HX + Ma\n",
    "                drift = H @ X_n[i, j] + M @ control[i, j]\n",
    "                \n",
    "                # Update state using explicit scheme\n",
    "                X[i, j, n+1] = X_n[i, j] + drift * dt + sigma @ dW[i, j, n]\n",
    "                \n",
    "                # Accumulate running cost\n",
    "                state_cost = X_n[i, j] @ C @ X_n[i, j]\n",
    "                control_cost = control[i, j] @ D @ control[i, j]\n",
    "                costs[i, j] += (state_cost + control_cost) * dt\n",
    "    \n",
    "    # Add terminal cost\n",
    "    X_T = X[:, :, -1, :]\n",
    "    for i in range(num_samples):\n",
    "        for j in range(batch_size):\n",
    "            terminal_cost = X_T[i, j] @ lqr.R @ X_T[i, j]\n",
    "            costs[i, j] += terminal_cost\n",
    "    \n",
    "    return X, costs\n",
    "\n",
    "def simulate_sde_implicit(lqr: LQR, x0: torch.Tensor, num_steps: int, num_samples: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Simulate the controlled SDE using the implicit scheme.\n",
    "    \n",
    "    Args:\n",
    "        lqr: LQR instance\n",
    "        x0: Initial state (batch x d)\n",
    "        num_steps: Number of time steps\n",
    "        num_samples: Number of Monte Carlo samples\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (trajectory, cost)\n",
    "    \"\"\"\n",
    "    dt = lqr.T / num_steps\n",
    "    t_grid = torch.linspace(0, lqr.T, num_steps + 1, dtype=torch.float64)\n",
    "    \n",
    "    batch_size = x0.shape[0]\n",
    "    d = lqr.d\n",
    "    \n",
    "    # Ensure all tensors are double precision\n",
    "    H = lqr.H.to(torch.float64)\n",
    "    M = lqr.M.to(torch.float64)\n",
    "    sigma = lqr.sigma.to(torch.float64)\n",
    "    C = lqr.C.to(torch.float64)\n",
    "    D = lqr.D.to(torch.float64)\n",
    "    D_inv = lqr.D_inv.to(torch.float64)\n",
    "    \n",
    "    # Initialize trajectories and costs\n",
    "    X = torch.zeros((num_samples, batch_size, num_steps + 1, d), dtype=torch.float64)\n",
    "    X[:, :, 0, :] = x0.unsqueeze(0).repeat(num_samples, 1, 1)\n",
    "    \n",
    "    costs = torch.zeros((num_samples, batch_size), dtype=torch.float64)\n",
    "    \n",
    "    # Generate Brownian increments\n",
    "    dW = torch.randn((num_samples, batch_size, num_steps, sigma.shape[1]), dtype=torch.float64) * np.sqrt(dt)\n",
    "    \n",
    "    # Identity matrix for linear system\n",
    "    I = torch.eye(d, dtype=torch.float64)\n",
    "    \n",
    "    # Simulate trajectories\n",
    "    for n in range(num_steps):\n",
    "        t_n = t_grid[n]\n",
    "        t_np1 = t_grid[n+1] \n",
    "        X_n = X[:, :, n, :]\n",
    "        \n",
    "        # Reshape for batch processing for cost calculation\n",
    "        X_n_flat = X_n.reshape(-1, d)\n",
    "        t_flat = t_n.repeat(num_samples * batch_size)\n",
    "        \n",
    "        # Compute optimal control for cost calculation\n",
    "        control_flat = lqr.optimal_control(t_flat, X_n_flat).to(torch.float64)\n",
    "        control = control_flat.reshape(num_samples, batch_size, lqr.m)\n",
    "        \n",
    "        # For implicit scheme, we need to solve a linear system for each sample\n",
    "        S_np1 = lqr.get_S_at_time(torch.tensor([t_np1], dtype=torch.float64))[0].to(torch.float64)\n",
    "        \n",
    "        # Construct system matrix: (I - dt*H + dt*M*D^(-1)*M^T*S(t_{n+1}))\n",
    "        MD_inv_MT = M @ D_inv @ M.T\n",
    "        A = I - dt * H + dt * MD_inv_MT @ S_np1\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            for j in range(batch_size):\n",
    "                # Compute right-hand side: X_n + sigma * dW_n\n",
    "                b = X_n[i, j] + sigma @ dW[i, j, n]\n",
    "                \n",
    "                # Solve the linear system: A * X_{n+1} = b\n",
    "                X[i, j, n+1] = torch.linalg.solve(A, b)\n",
    "                \n",
    "                # Accumulate running cost\n",
    "                state_cost = X_n[i, j] @ C @ X_n[i, j]\n",
    "                control_cost = control[i, j] @ D @ control[i, j]\n",
    "                costs[i, j] += (state_cost + control_cost) * dt\n",
    "    \n",
    "    # Add terminal cost\n",
    "    X_T = X[:, :, -1, :]\n",
    "    for i in range(num_samples):\n",
    "        for j in range(batch_size):\n",
    "            terminal_cost = X_T[i, j] @ lqr.R @ X_T[i, j]\n",
    "            costs[i, j] += terminal_cost\n",
    "    \n",
    "    return X, costs\n",
    "\n",
    "def run_monte_carlo_tests(lqr: LQR, x0: torch.Tensor, scheme: str = 'explicit') -> None:\n",
    "    \"\"\"\n",
    "    Run Monte Carlo tests for the LQR problem.\n",
    "    \n",
    "    Args:\n",
    "        lqr: LQR instance\n",
    "        x0: Initial states to test\n",
    "        scheme: 'explicit' or 'implicit'\n",
    "    \"\"\"\n",
    "    # 1. Test varying time steps with fixed number of samples\n",
    "    num_samples = 2500 # should be 10k\n",
    "    time_steps_list = [2**i for i in range(1, 10)] # should be up to 12\n",
    "    time_step_errors = []\n",
    "    \n",
    "    # Compute true value function at t=0, x=x0\n",
    "    t0 = torch.zeros(x0.shape[0], dtype=torch.float64)\n",
    "    true_values = lqr.value_function(t0, x0)\n",
    "    \n",
    "    print(f\"\\n--- Testing convergence for {scheme} scheme ---\")\n",
    "    print(\"Varying time steps...\")\n",
    "    \n",
    "    for num_steps in time_steps_list:\n",
    "        print(f\"Running with {num_steps} time steps...\")\n",
    "        \n",
    "        # Run simulation with current parameters\n",
    "        if scheme == 'explicit':\n",
    "            _, costs = simulate_sde_explicit(lqr, x0, num_steps, num_samples)\n",
    "        else:\n",
    "            _, costs = simulate_sde_implicit(lqr, x0, num_steps, num_samples)\n",
    "        \n",
    "        # Compute mean cost\n",
    "        mean_costs = costs.mean(dim=0)\n",
    "        \n",
    "        # Compute error\n",
    "        error = torch.abs(mean_costs - true_values).mean().item()\n",
    "        time_step_errors.append(error)\n",
    "        \n",
    "        print(f\"  Error: {error:.6f}\")\n",
    "    \n",
    "    # Plot time step convergence\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.loglog(time_steps_list, time_step_errors, 'o-', label=f'{scheme.capitalize()} Scheme')\n",
    "    \n",
    "    # Add trend line \n",
    "    if scheme == 'explicit':\n",
    "        ref_line = [time_step_errors[0] * (time_steps_list[0] / n) for n in time_steps_list]\n",
    "        plt.loglog(time_steps_list, ref_line, '--', label='O(1/N)')\n",
    "    else:\n",
    "        ref_line = [time_step_errors[0] * (time_steps_list[0] / n)**2 for n in time_steps_list]\n",
    "        plt.loglog(time_steps_list, ref_line, '--', label='O(1/N²)')\n",
    "    \n",
    "    plt.xlabel('Number of Time Steps (N)')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.title(f'Convergence with Varying Time Steps ({scheme.capitalize()} Scheme)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Test varying number of samples with fixed number of time steps\n",
    "    num_steps = 2500  # Large number for accuracy should be 10k\n",
    "    sample_counts = [2 * 4**i for i in range(5)] # should be range(6)\n",
    "    sample_errors = []\n",
    "    \n",
    "    print(\"\\nVarying sample counts...\")\n",
    "    \n",
    "    for num_samples in sample_counts:\n",
    "        print(f\"Running with {num_samples} samples...\")\n",
    "        \n",
    "        # Run simulation with current parameters\n",
    "        if scheme == 'explicit':\n",
    "            _, costs = simulate_sde_explicit(lqr, x0, num_steps, num_samples)\n",
    "        else:\n",
    "            _, costs = simulate_sde_implicit(lqr, x0, num_steps, num_samples)\n",
    "        \n",
    "        # Compute mean cost\n",
    "        mean_costs = costs.mean(dim=0)\n",
    "        \n",
    "        # Compute error\n",
    "        error = torch.abs(mean_costs - true_values).mean().item()\n",
    "        sample_errors.append(error)\n",
    "        \n",
    "        print(f\"  Error: {error:.6f}\")\n",
    "    \n",
    "    # Plot sample count convergence\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.loglog(sample_counts, sample_errors, 'o-', label=f'{scheme.capitalize()} Scheme')\n",
    "    \n",
    "    # Add trend line (should be O(1/sqrt(M)) for Monte Carlo)\n",
    "    ref_line = [sample_errors[0] * np.sqrt(sample_counts[0] / n) for n in sample_counts]\n",
    "    plt.loglog(sample_counts, ref_line, '--', label='O(1/√M)')\n",
    "    \n",
    "    plt.xlabel('Number of Monte Carlo Samples (M)')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.title(f'Convergence with Varying Sample Counts ({scheme.capitalize()} Scheme)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def run_monte_carlo_comparison(lqr: LQR, x0: torch.Tensor) -> None:\n",
    "    \"\"\"\n",
    "    Run Monte Carlo tests comparing explicit and implicit schemes on the same plots.\n",
    "    \n",
    "    Args:\n",
    "        lqr: LQR instance\n",
    "        x0: Initial states to test\n",
    "    \"\"\"\n",
    "    # 1. Test varying time steps with fixed number of samples\n",
    "    num_samples = 1000 # Should be 10k\n",
    "    time_steps_list = [2**i for i in range(1, 10)] # Should be 12\n",
    "    \n",
    "    # Compute true value function at t=0, x=x0\n",
    "    t0 = torch.zeros(x0.shape[0], dtype=torch.float64)\n",
    "    true_values = lqr.value_function(t0, x0)\n",
    "    \n",
    "    # Arrays to store results for both schemes\n",
    "    explicit_time_errors = []\n",
    "    implicit_time_errors = []\n",
    "    \n",
    "    print(\"\\n--- Testing convergence for both schemes with varying time steps ---\")\n",
    "    \n",
    "    for num_steps in time_steps_list:\n",
    "        print(f\"Running with {num_steps} time steps...\")\n",
    "        \n",
    "        # Run simulation with explicit scheme\n",
    "        _, costs_explicit = simulate_sde_explicit(lqr, x0, num_steps, num_samples)\n",
    "        mean_costs_explicit = costs_explicit.mean(dim=0)\n",
    "        error_explicit = torch.abs(mean_costs_explicit - true_values).mean().item()\n",
    "        explicit_time_errors.append(error_explicit)\n",
    "        \n",
    "        # Run simulation with implicit scheme\n",
    "        _, costs_implicit = simulate_sde_implicit(lqr, x0, num_steps, num_samples)\n",
    "        mean_costs_implicit = costs_implicit.mean(dim=0)\n",
    "        error_implicit = torch.abs(mean_costs_implicit - true_values).mean().item()\n",
    "        implicit_time_errors.append(error_implicit)\n",
    "        \n",
    "        print(f\"  Explicit scheme error: {error_explicit:.6f}\")\n",
    "        print(f\"  Implicit scheme error: {error_implicit:.6f}\")\n",
    "    \n",
    "    # Plot time step convergence for both schemes on the same graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.loglog(time_steps_list, explicit_time_errors, 'o-', color='blue', label='Explicit Scheme')\n",
    "    plt.loglog(time_steps_list, implicit_time_errors, 's-', color='red', label='Implicit Scheme')\n",
    "    \n",
    "    # Add reference lines for O(1/N) and O(1/N²) convergence\n",
    "    ref_line_order1 = [explicit_time_errors[0] * (time_steps_list[0] / n) for n in time_steps_list]\n",
    "    ref_line_order2 = [explicit_time_errors[0] * (time_steps_list[0] / n)**2 for n in time_steps_list]\n",
    "    \n",
    "    plt.loglog(time_steps_list, ref_line_order1, '--', color='blue', alpha=0.5, label='O(1/N)')\n",
    "    plt.loglog(time_steps_list, ref_line_order2, '--', color='red', alpha=0.5, label='O(1/N²)')\n",
    "    \n",
    "    plt.xlabel('Number of Time Steps (N)')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.title('Convergence with Varying Time Steps - Comparison of Schemes')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Test varying number of samples with fixed number of time steps\n",
    "    num_steps = 1000  # Large number for accuracy should be 10k\n",
    "    sample_counts = [2 * 4**i for i in range(5)] # Should be 6\n",
    "    \n",
    "    # Arrays to store results for both schemes\n",
    "    explicit_sample_errors = []\n",
    "    implicit_sample_errors = []\n",
    "    \n",
    "    print(\"\\n--- Testing convergence for both schemes with varying sample counts ---\")\n",
    "    \n",
    "    for num_samples in sample_counts:\n",
    "        print(f\"Running with {num_samples} samples...\")\n",
    "        \n",
    "        # Run simulation with explicit scheme\n",
    "        _, costs_explicit = simulate_sde_explicit(lqr, x0, num_steps, num_samples)\n",
    "        mean_costs_explicit = costs_explicit.mean(dim=0)\n",
    "        error_explicit = torch.abs(mean_costs_explicit - true_values).mean().item()\n",
    "        explicit_sample_errors.append(error_explicit)\n",
    "        \n",
    "        # Run simulation with implicit scheme\n",
    "        _, costs_implicit = simulate_sde_implicit(lqr, x0, num_steps, num_samples)\n",
    "        mean_costs_implicit = costs_implicit.mean(dim=0)\n",
    "        error_implicit = torch.abs(mean_costs_implicit - true_values).mean().item()\n",
    "        implicit_sample_errors.append(error_implicit)\n",
    "        \n",
    "        print(f\"  Explicit scheme error: {error_explicit:.6f}\")\n",
    "        print(f\"  Implicit scheme error: {error_implicit:.6f}\")\n",
    "    \n",
    "    # Plot sample count convergence for both schemes on the same graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.loglog(sample_counts, explicit_sample_errors, 'o-', color='blue', label='Explicit Scheme')\n",
    "    plt.loglog(sample_counts, implicit_sample_errors, 's-', color='red', label='Implicit Scheme')\n",
    "    \n",
    "    # Add reference line for O(1/sqrt(M)) convergence (Monte Carlo rate)\n",
    "    ref_line_mc = [explicit_sample_errors[0] * np.sqrt(sample_counts[0] / n) for n in sample_counts]\n",
    "    plt.loglog(sample_counts, ref_line_mc, '--', color='green', label='O(1/√M)')\n",
    "    \n",
    "    plt.xlabel('Number of Monte Carlo Samples (M)')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.title('Convergence with Varying Sample Counts - Comparison of Schemes')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S(0):\n",
      " tensor([[ 0.3698, -0.1901],\n",
      "        [-0.1901,  0.5432]], dtype=torch.float64)\n",
      "S(T/2):\n",
      " tensor([[ 0.4916, -0.3262],\n",
      "        [-0.3262,  0.7793]], dtype=torch.float64)\n",
      "S(T):\n",
      " tensor([[10.,  3.],\n",
      "        [ 3., 10.]], dtype=torch.float64)\n",
      "\n",
      "Value function at t=0:\n",
      "v(0, [1.0, 1.0]) = 0.782136\n",
      "v(0, [2.0, 2.0]) = 2.380319\n",
      "\n",
      "Optimal control at t=0:\n",
      "u(0, [1.0, 1.0]) = [-1.2770086526870728, -5.199576377868652]\n",
      "u(0, [2.0, 2.0]) = [-2.5540173053741455, -10.399152755737305]\n",
      "\n",
      "--- Testing convergence for explicit scheme ---\n",
      "Varying time steps...\n",
      "Running with 2 time steps...\n",
      "  Error: 5.045467\n",
      "Running with 4 time steps...\n",
      "  Error: 1.133618\n",
      "Running with 8 time steps...\n",
      "  Error: 0.491513\n",
      "Running with 16 time steps...\n",
      "  Error: 0.224379\n",
      "Running with 32 time steps...\n",
      "  Error: 0.103346\n",
      "Running with 64 time steps...\n",
      "  Error: 0.053396\n",
      "Running with 128 time steps...\n",
      "  Error: 0.024888\n",
      "Running with 256 time steps...\n",
      "  Error: 0.012114\n",
      "Running with 512 time steps...\n",
      "  Error: 0.004284\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Varying sample counts...\n",
      "Running with 2 samples...\n",
      "  Error: 0.251542\n",
      "Running with 8 samples...\n",
      "  Error: 0.129240\n",
      "Running with 32 samples...\n",
      "  Error: 0.038412\n",
      "Running with 128 samples...\n",
      "  Error: 0.019944\n",
      "Running with 512 samples...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mu(0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx0[i].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mu0[i].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Run Monte Carlo tests for explicit scheme\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mrun_monte_carlo_tests\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlqr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheme\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mexplicit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Run Monte Carlo tests for implicit scheme\u001b[39;00m\n\u001b[32m     48\u001b[39m run_monte_carlo_tests(lqr, x0, scheme=\u001b[33m'\u001b[39m\u001b[33mimplicit\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 224\u001b[39m, in \u001b[36mrun_monte_carlo_tests\u001b[39m\u001b[34m(lqr, x0, scheme)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# Run simulation with current parameters\u001b[39;00m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scheme == \u001b[33m'\u001b[39m\u001b[33mexplicit\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     _, costs = \u001b[43msimulate_sde_explicit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlqr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    226\u001b[39m     _, costs = simulate_sde_implicit(lqr, x0, num_steps, num_samples)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36msimulate_sde_explicit\u001b[39m\u001b[34m(lqr, x0, num_steps, num_samples)\u001b[39m\n\u001b[32m     59\u001b[39m             state_cost = X_n[i, j] @ C @ X_n[i, j]\n\u001b[32m     60\u001b[39m             control_cost = control[i, j] @ D @ control[i, j]\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m             costs[i, j] += (state_cost + control_cost) * dt\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Add terminal cost\u001b[39;00m\n\u001b[32m     64\u001b[39m X_T = X[:, :, -\u001b[32m1\u001b[39m, :]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def main_init():\n",
    "    # Set the problem matrices as specified in Figure 1\n",
    "    H = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64) * 0.5\n",
    "    M = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64)\n",
    "    sigma = torch.eye(2, dtype=torch.float64) * 0.5\n",
    "    C = torch.tensor([[1.0, 0.1], [0.1, 1.0]], dtype=torch.float64) * 1.0\n",
    "    D = torch.tensor([[1.0, 0.1], [0.1, 1.0]], dtype=torch.float64) * 0.1\n",
    "    R = torch.tensor([[1.0, 0.3], [0.3, 1.0]], dtype=torch.float64) * 10.0\n",
    "\n",
    "    # Set the terminal time and time grid\n",
    "    T = 0.5\n",
    "    grid_size = 1000\n",
    "    time_grid = torch.linspace(0, T, grid_size, dtype=torch.float64)\n",
    "\n",
    "    # Create LQR instance\n",
    "    lqr = LQR(H, M, sigma, C, D, R, T, time_grid)\n",
    "\n",
    "    # Solve Ricatti ODE\n",
    "    lqr.solve_ricatti()\n",
    "\n",
    "    # Print S matrices at key time points\n",
    "    print(\"S(0):\\n\", lqr.S_grid[0])\n",
    "    print(\"S(T/2):\\n\", lqr.S_grid[grid_size//2])\n",
    "    print(\"S(T):\\n\", lqr.S_grid[-1])\n",
    "\n",
    "    # Test points\n",
    "    x0 = torch.tensor([\n",
    "        [1.0, 1.0],\n",
    "        [2.0, 2.0]\n",
    "    ], dtype=torch.float64)\n",
    "\n",
    "    # Compute value function at test points\n",
    "    t0 = torch.zeros(x0.shape[0], dtype=torch.float64)\n",
    "    v0 = lqr.value_function(t0, x0)\n",
    "    print(\"\\nValue function at t=0:\")\n",
    "    for i in range(x0.shape[0]):\n",
    "        print(f\"v(0, {x0[i].tolist()}) = {v0[i].item():.6f}\")\n",
    "\n",
    "    # Get the optimal control for the test points\n",
    "    u0 = lqr.optimal_control(t0, x0)\n",
    "    print(\"\\nOptimal control at t=0:\")\n",
    "    for i in range(x0.shape[0]):\n",
    "        print(f\"u(0, {x0[i].tolist()}) = {u0[i].tolist()}\")\n",
    "\n",
    "    # Run Monte Carlo tests for explicit scheme\n",
    "    run_monte_carlo_tests(lqr, x0, scheme='explicit')\n",
    "\n",
    "    # Run Monte Carlo tests for implicit scheme\n",
    "    run_monte_carlo_tests(lqr, x0, scheme='implicit')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S(0):\n",
      " tensor([[ 0.3698, -0.1901],\n",
      "        [-0.1901,  0.5432]], dtype=torch.float64)\n",
      "S(T/2):\n",
      " tensor([[ 0.4916, -0.3262],\n",
      "        [-0.3262,  0.7793]], dtype=torch.float64)\n",
      "S(T):\n",
      " tensor([[10.,  3.],\n",
      "        [ 3., 10.]], dtype=torch.float64)\n",
      "\n",
      "Value function at t=0:\n",
      "v(0, [1.0, 1.0]) = 0.782136\n",
      "v(0, [2.0, 2.0]) = 2.380319\n",
      "\n",
      "Optimal control at t=0:\n",
      "u(0, [1.0, 1.0]) = [-1.2770086526870728, -5.199576377868652]\n",
      "u(0, [2.0, 2.0]) = [-2.5540173053741455, -10.399152755737305]\n",
      "\n",
      "--- Testing convergence for both schemes with varying time steps ---\n",
      "Running with 2 time steps...\n",
      "  Explicit scheme error: 4.944351\n",
      "  Implicit scheme error: 2.352849\n",
      "Running with 4 time steps...\n",
      "  Explicit scheme error: 1.136594\n",
      "  Implicit scheme error: 1.085881\n",
      "Running with 8 time steps...\n",
      "  Explicit scheme error: 0.489024\n",
      "  Implicit scheme error: 0.482751\n",
      "Running with 16 time steps...\n",
      "  Explicit scheme error: 0.222731\n",
      "  Implicit scheme error: 0.209865\n",
      "Running with 32 time steps...\n",
      "  Explicit scheme error: 0.104880\n",
      "  Implicit scheme error: 0.097218\n",
      "Running with 64 time steps...\n",
      "  Explicit scheme error: 0.049648\n",
      "  Implicit scheme error: 0.051123\n",
      "Running with 128 time steps...\n",
      "  Explicit scheme error: 0.026700\n",
      "  Implicit scheme error: 0.025713\n",
      "Running with 256 time steps...\n",
      "  Explicit scheme error: 0.001820\n",
      "  Implicit scheme error: 0.007999\n",
      "Running with 512 time steps...\n",
      "  Explicit scheme error: 0.004777\n",
      "  Implicit scheme error: 0.006919\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing convergence for both schemes with varying sample counts ---\n",
      "Running with 2 samples...\n",
      "  Explicit scheme error: 0.462984\n",
      "  Implicit scheme error: 0.226043\n",
      "Running with 8 samples...\n",
      "  Explicit scheme error: 0.095107\n",
      "  Implicit scheme error: 0.097713\n",
      "Running with 32 samples...\n",
      "  Explicit scheme error: 0.035934\n",
      "  Implicit scheme error: 0.006308\n",
      "Running with 128 samples...\n",
      "  Explicit scheme error: 0.023574\n",
      "  Implicit scheme error: 0.015609\n",
      "Running with 512 samples...\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Set the problem matrices as specified in Figure 1\n",
    "    H = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64) * 0.5\n",
    "    M = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64)\n",
    "    sigma = torch.eye(2, dtype=torch.float64) * 0.5\n",
    "    C = torch.tensor([[1.0, 0.1], [0.1, 1.0]], dtype=torch.float64) * 1.0\n",
    "    D = torch.tensor([[1.0, 0.1], [0.1, 1.0]], dtype=torch.float64) * 0.1\n",
    "    R = torch.tensor([[1.0, 0.3], [0.3, 1.0]], dtype=torch.float64) * 10.0\n",
    "    \n",
    "    # Set the terminal time and time grid\n",
    "    T = 0.5\n",
    "    grid_size = 1000\n",
    "    time_grid = torch.linspace(0, T, grid_size, dtype=torch.float64)\n",
    "    \n",
    "    # Create LQR instance\n",
    "    lqr = LQR(H, M, sigma, C, D, R, T, time_grid)\n",
    "    \n",
    "    # Solve Ricatti ODE\n",
    "    lqr.solve_ricatti()\n",
    "    \n",
    "    # Print S matrices at key time points\n",
    "    print(\"S(0):\\n\", lqr.S_grid[0])\n",
    "    print(\"S(T/2):\\n\", lqr.S_grid[grid_size//2])\n",
    "    print(\"S(T):\\n\", lqr.S_grid[-1])\n",
    "    \n",
    "    # Test points\n",
    "    x0 = torch.tensor([\n",
    "        [1.0, 1.0],\n",
    "        [2.0, 2.0]\n",
    "    ], dtype=torch.float64)\n",
    "    \n",
    "    # Compute value function at test points\n",
    "    t0 = torch.zeros(x0.shape[0], dtype=torch.float64)\n",
    "    v0 = lqr.value_function(t0, x0)\n",
    "    print(\"\\nValue function at t=0:\")\n",
    "    for i in range(x0.shape[0]):\n",
    "        print(f\"v(0, {x0[i].tolist()}) = {v0[i].item():.6f}\")\n",
    "    \n",
    "    # Get the optimal control for the test points\n",
    "    u0 = lqr.optimal_control(t0, x0)\n",
    "    print(\"\\nOptimal control at t=0:\")\n",
    "    for i in range(x0.shape[0]):\n",
    "        print(f\"u(0, {x0[i].tolist()}) = {u0[i].tolist()}\")\n",
    "    \n",
    "    # Run Monte Carlo comparison for both schemes\n",
    "    run_monte_carlo_comparison(lqr, x0)\n",
    "    \n",
    "    # Additionally, compare trajectories from both schemes\n",
    "    compare_scheme_trajectories(lqr, x0)\n",
    "\n",
    "def compare_scheme_trajectories(lqr: LQR, x0: torch.Tensor) -> None:\n",
    "    \"\"\"\n",
    "    Compare and plot trajectories from explicit and implicit schemes.\n",
    "    \n",
    "    Args:\n",
    "        lqr: LQR instance\n",
    "        x0: Initial states to test\n",
    "    \"\"\"\n",
    "    # Set simulation parameters\n",
    "    num_steps = 1000\n",
    "    num_samples = 1  # Just one sample for visualization\n",
    "    \n",
    "    # Simulate trajectories using both schemes with the same noise\n",
    "    # Generate Brownian motion for consistency between schemes\n",
    "    dt = lqr.T / num_steps\n",
    "    dW = torch.randn((num_samples, x0.shape[0], num_steps, lqr.sigma.shape[1]), \n",
    "                    dtype=torch.float64) * np.sqrt(dt)\n",
    "    \n",
    "    # Run simulations with shared noise\n",
    "    X_explicit, _ = simulate_sde_explicit(lqr, x0, num_steps, num_samples, fixed_noise=dW)\n",
    "    X_implicit, _ = simulate_sde_implicit(lqr, x0, num_steps, num_samples, fixed_noise=dW)\n",
    "    \n",
    "    # Plot trajectories for each initial state\n",
    "    fig, axes = plt.subplots(1, x0.shape[0], figsize=(15, 5))\n",
    "    \n",
    "    for i in range(x0.shape[0]):\n",
    "        ax = axes[i] if x0.shape[0] > 1 else axes\n",
    "        \n",
    "        # Extract trajectories for this initial state\n",
    "        traj_explicit = X_explicit[0, i].cpu().numpy()  # First sample, ith initial state\n",
    "        traj_implicit = X_implicit[0, i].cpu().numpy()\n",
    "        \n",
    "        # Plot trajectories\n",
    "        ax.plot(traj_explicit[:, 0], traj_explicit[:, 1], 'b-', label='Explicit Scheme')\n",
    "        ax.plot(traj_implicit[:, 0], traj_implicit[:, 1], 'r-', label='Implicit Scheme')\n",
    "        ax.scatter([x0[i, 0]], [x0[i, 1]], c='g', s=100, marker='o', label='Initial State')\n",
    "        \n",
    "        ax.set_title(f'Trajectories from Initial State {x0[i].tolist()}')\n",
    "        ax.set_xlabel('X1')\n",
    "        ax.set_ylabel('X2')\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Update the simulation functions to accept fixed noise\n",
    "def simulate_sde_explicit(lqr: LQR, x0: torch.Tensor, num_steps: int, num_samples: int, \n",
    "                         fixed_noise: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Simulate the controlled SDE using the explicit scheme with optional fixed noise.\n",
    "    \n",
    "    Args:\n",
    "        lqr: LQR instance\n",
    "        x0: Initial state (batch x d)\n",
    "        num_steps: Number of time steps\n",
    "        num_samples: Number of Monte Carlo samples\n",
    "        fixed_noise: Optional fixed Brownian increments for consistent comparisons\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (trajectory, cost)\n",
    "    \"\"\"\n",
    "    dt = lqr.T / num_steps\n",
    "    t_grid = torch.linspace(0, lqr.T, num_steps + 1, dtype=torch.float64)\n",
    "    \n",
    "    batch_size = x0.shape[0]\n",
    "    d = lqr.d\n",
    "    \n",
    "    # Ensure all tensors are double precision\n",
    "    H = lqr.H.to(torch.float64)\n",
    "    M = lqr.M.to(torch.float64)\n",
    "    sigma = lqr.sigma.to(torch.float64)\n",
    "    C = lqr.C.to(torch.float64)\n",
    "    D = lqr.D.to(torch.float64)\n",
    "    \n",
    "    # Initialize trajectories and costs\n",
    "    X = torch.zeros((num_samples, batch_size, num_steps + 1, d), dtype=torch.float64)\n",
    "    X[:, :, 0, :] = x0.unsqueeze(0).repeat(num_samples, 1, 1)\n",
    "    \n",
    "    costs = torch.zeros((num_samples, batch_size), dtype=torch.float64)\n",
    "    \n",
    "    # Generate Brownian increments or use fixed ones\n",
    "    if fixed_noise is None:\n",
    "        dW = torch.randn((num_samples, batch_size, num_steps, sigma.shape[1]), \n",
    "                        dtype=torch.float64) * np.sqrt(dt)\n",
    "    else:\n",
    "        dW = fixed_noise\n",
    "    \n",
    "    # Simulate trajectories\n",
    "    for n in range(num_steps):\n",
    "        t_n = t_grid[n]\n",
    "        X_n = X[:, :, n, :]\n",
    "        \n",
    "        # Reshape for batch processing\n",
    "        X_n_flat = X_n.reshape(-1, d)\n",
    "        t_flat = t_n.repeat(num_samples * batch_size)\n",
    "        \n",
    "        # Compute optimal control\n",
    "        control_flat = lqr.optimal_control(t_flat, X_n_flat).to(torch.float64)\n",
    "        control = control_flat.reshape(num_samples, batch_size, lqr.m)\n",
    "        \n",
    "        # Compute drift and apply update for each sample\n",
    "        for i in range(num_samples):\n",
    "            for j in range(batch_size):\n",
    "                # Compute drift term: HX + Ma\n",
    "                drift = H @ X_n[i, j] + M @ control[i, j]\n",
    "                \n",
    "                # Update state using explicit scheme\n",
    "                X[i, j, n+1] = X_n[i, j] + drift * dt + sigma @ dW[i, j, n]\n",
    "                \n",
    "                # Accumulate running cost\n",
    "                state_cost = X_n[i, j] @ C @ X_n[i, j]\n",
    "                control_cost = control[i, j] @ D @ control[i, j]\n",
    "                costs[i, j] += (state_cost + control_cost) * dt\n",
    "    \n",
    "    # Add terminal cost\n",
    "    X_T = X[:, :, -1, :]\n",
    "    for i in range(num_samples):\n",
    "        for j in range(batch_size):\n",
    "            terminal_cost = X_T[i, j] @ lqr.R @ X_T[i, j]\n",
    "            costs[i, j] += terminal_cost\n",
    "    \n",
    "    return X, costs\n",
    "\n",
    "def simulate_sde_implicit(lqr: LQR, x0: torch.Tensor, num_steps: int, num_samples: int,\n",
    "                         fixed_noise: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Simulate the controlled SDE using the implicit scheme with optional fixed noise.\n",
    "    \n",
    "    Args:\n",
    "        lqr: LQR instance\n",
    "        x0: Initial state (batch x d)\n",
    "        num_steps: Number of time steps\n",
    "        num_samples: Number of Monte Carlo samples\n",
    "        fixed_noise: Optional fixed Brownian increments for consistent comparisons\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (trajectory, cost)\n",
    "    \"\"\"\n",
    "    dt = lqr.T / num_steps\n",
    "    t_grid = torch.linspace(0, lqr.T, num_steps + 1, dtype=torch.float64)\n",
    "    \n",
    "    batch_size = x0.shape[0]\n",
    "    d = lqr.d\n",
    "    \n",
    "    # Ensure all tensors are double precision\n",
    "    H = lqr.H.to(torch.float64)\n",
    "    M = lqr.M.to(torch.float64)\n",
    "    sigma = lqr.sigma.to(torch.float64)\n",
    "    C = lqr.C.to(torch.float64)\n",
    "    D = lqr.D.to(torch.float64)\n",
    "    D_inv = lqr.D_inv.to(torch.float64)\n",
    "    \n",
    "    # Initialize trajectories and costs\n",
    "    X = torch.zeros((num_samples, batch_size, num_steps + 1, d), dtype=torch.float64)\n",
    "    X[:, :, 0, :] = x0.unsqueeze(0).repeat(num_samples, 1, 1)\n",
    "    \n",
    "    costs = torch.zeros((num_samples, batch_size), dtype=torch.float64)\n",
    "    \n",
    "    # Generate Brownian increments or use fixed ones\n",
    "    if fixed_noise is None:\n",
    "        dW = torch.randn((num_samples, batch_size, num_steps, sigma.shape[1]), \n",
    "                        dtype=torch.float64) * np.sqrt(dt)\n",
    "    else:\n",
    "        dW = fixed_noise\n",
    "    \n",
    "    # Identity matrix for linear system\n",
    "    I = torch.eye(d, dtype=torch.float64)\n",
    "    \n",
    "    # Simulate trajectories\n",
    "    for n in range(num_steps):\n",
    "        t_n = t_grid[n]\n",
    "        t_np1 = t_grid[n+1] \n",
    "        X_n = X[:, :, n, :]\n",
    "        \n",
    "        # Reshape for batch processing for cost calculation\n",
    "        X_n_flat = X_n.reshape(-1, d)\n",
    "        t_flat = t_n.repeat(num_samples * batch_size)\n",
    "        \n",
    "        # Compute optimal control for cost calculation\n",
    "        control_flat = lqr.optimal_control(t_flat, X_n_flat).to(torch.float64)\n",
    "        control = control_flat.reshape(num_samples, batch_size, lqr.m)\n",
    "        \n",
    "        # For implicit scheme, we need to solve a linear system for each sample\n",
    "        S_np1 = lqr.get_S_at_time(torch.tensor([t_np1], dtype=torch.float64))[0].to(torch.float64)\n",
    "        \n",
    "        # Construct system matrix: (I - dt*H + dt*M*D^(-1)*M^T*S(t_{n+1}))\n",
    "        MD_inv_MT = M @ D_inv @ M.T\n",
    "        A = I - dt * H + dt * MD_inv_MT @ S_np1\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            for j in range(batch_size):\n",
    "                # Compute right-hand side: X_n + sigma * dW_n\n",
    "                b = X_n[i, j] + sigma @ dW[i, j, n]\n",
    "                \n",
    "                # Solve the linear system: A * X_{n+1} = b\n",
    "                X[i, j, n+1] = torch.linalg.solve(A, b)\n",
    "                \n",
    "                # Accumulate running cost\n",
    "                state_cost = X_n[i, j] @ C @ X_n[i, j]\n",
    "                control_cost = control[i, j] @ D @ control[i, j]\n",
    "                costs[i, j] += (state_cost + control_cost) * dt\n",
    "    \n",
    "    # Add terminal cost\n",
    "    X_T = X[:, :, -1, :]\n",
    "    for i in range(num_samples):\n",
    "        for j in range(batch_size):\n",
    "            terminal_cost = X_T[i, j] @ lqr.R @ X_T[i, j]\n",
    "            costs[i, j] += terminal_cost\n",
    "    \n",
    "    return X, costs\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "from typing import Tuple, Optional\n",
    "import math\n",
    "\n",
    "class SoftLQR(LQR):\n",
    "    def __init__(self, H: torch.Tensor, M: torch.Tensor, sigma: torch.Tensor, \n",
    "                 C: torch.Tensor, D: torch.Tensor, R: torch.Tensor, \n",
    "                 T: float, time_grid: torch.Tensor,\n",
    "                 tau: float, gamma: float):\n",
    "        \"\"\"\n",
    "        Initialize the soft LQR problem with entropy regularization.\n",
    "        \n",
    "        Args:\n",
    "            H: System dynamics matrix (d x d)\n",
    "            M: Control input matrix (d x m)\n",
    "            sigma: Noise matrix (d x d')\n",
    "            C: State cost matrix (d x d)\n",
    "            D: Control cost matrix (m x m)\n",
    "            R: Terminal state cost matrix (d x d)\n",
    "            T: Terminal time\n",
    "            time_grid: Grid of time points\n",
    "            tau: Entropy regularization strength\n",
    "            gamma: Variance of prior normal density\n",
    "        \"\"\"\n",
    "        super().__init__(H, M, sigma, C, D, R, T, time_grid)\n",
    "        \n",
    "        # Store additional parameters\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Compute modified inverse term for the soft LQR\n",
    "        # Σ⁻¹ = D + τ/(2γ²)I\n",
    "        self.sigma_inv = self.D + (tau / (2 * gamma**2)) * torch.eye(self.m, dtype=self.D.dtype, device=self.D.device)\n",
    "        self.sigma_term = torch.inverse(self.sigma_inv)\n",
    "        \n",
    "        # Compute determinant term for value function\n",
    "        # C_D,τ,γ = -τ ln(τ^(m/2)/(γ^m * det(Σ)^(1/2)))\n",
    "        self.CD_tau_gamma = -tau * math.log((tau**(self.m/2)) / (gamma**self.m * torch.sqrt(torch.det(self.sigma_term)).item()))\n",
    "\n",
    "    def ricatti_rhs(self, t: float, S_flat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Right-hand side of the modified Ricatti ODE for soft LQR:\n",
    "        S'(t) = S(t)M(D + τ/(2γ²)I)^(-1)M^TS(t) - H^TS(t) - S(t)H - C\n",
    "        \n",
    "        Args:\n",
    "            t: Time\n",
    "            S_flat: Flattened S matrix\n",
    "            \n",
    "        Returns:\n",
    "            Flattened derivative of S\n",
    "        \"\"\"\n",
    "        # Reshape S from flattened form\n",
    "        S = torch.tensor(S_flat.reshape(self.d, self.d), dtype=torch.float64)\n",
    "        \n",
    "        # Compute right-hand side with modified term\n",
    "        term1 = S @ self.M @ self.sigma_term @ self.M.T @ S\n",
    "        term2 = self.H.T @ S\n",
    "        term3 = S @ self.H\n",
    "        term4 = self.C\n",
    "        \n",
    "        # Compute derivative\n",
    "        dS = term1 - term2 - term3 - term4\n",
    "        \n",
    "        # Return flattened result\n",
    "        return dS.flatten().numpy()\n",
    "    \n",
    "    def compute_integral_term(self) -> None:\n",
    "        \"\"\"\n",
    "        Compute the integral term for the value function: \n",
    "        int_t^T tr(sigma sigma^T S(r)) dr + (T-t) * C_D,τ,γ\n",
    "        \"\"\"\n",
    "        if self.S_grid is None:\n",
    "            self.solve_ricatti()\n",
    "        \n",
    "        # Compute trace term at each time point\n",
    "        trace_terms = torch.zeros(len(self.time_grid))\n",
    "        sigma_sigma_T = self.sigma @ self.sigma.T\n",
    "        \n",
    "        for i in range(len(self.time_grid)):\n",
    "            trace_terms[i] = torch.trace(sigma_sigma_T @ self.S_grid[i])\n",
    "        \n",
    "        # Compute integral using trapezoidal rule (backward from T)\n",
    "        integral_term = torch.zeros(len(self.time_grid))\n",
    "        \n",
    "        for i in range(len(self.time_grid) - 1, 0, -1):\n",
    "            dt = self.time_grid[i] - self.time_grid[i-1]\n",
    "            integral_term[i-1] = integral_term[i] + 0.5 * (trace_terms[i] + trace_terms[i-1]) * dt\n",
    "        \n",
    "        # Add the constant term proportional to (T-t)\n",
    "        for i in range(len(self.time_grid)):\n",
    "            integral_term[i] += (self.T - self.time_grid[i]) * self.CD_tau_gamma\n",
    "        \n",
    "        self.int_term_grid = integral_term\n",
    "    \n",
    "    def optimal_control_distribution(self, t: torch.Tensor, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute the parameters of the optimal control distribution:\n",
    "        π*(·|t, x) = N(-(D + τ/(2γ²)I)^(-1)M^TS(t)x, τ(D + τ/(2γ²)I))\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            x: State tensor (batch x d)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (mean, covariance) of optimal control distribution\n",
    "        \"\"\"\n",
    "        # Get S matrices at the specified times\n",
    "        S_matrices = self.get_S_at_time(t)\n",
    "        \n",
    "        # Compute mean for each (t, x) pair\n",
    "        batch_size = x.shape[0]\n",
    "        means = torch.zeros((batch_size, self.m), dtype=torch.float64, device=x.device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            means[i] = -self.sigma_term @ self.M.T @ S_matrices[i] @ x[i]\n",
    "        \n",
    "        # The covariance is constant for all (t, x)\n",
    "        covariance = self.tau * self.sigma_term\n",
    "        \n",
    "        return means, covariance\n",
    "    \n",
    "    def optimal_control(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample from the optimal control distribution.\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            x: State tensor (batch x d)\n",
    "            \n",
    "        Returns:\n",
    "            Sampled control actions (batch x m)\n",
    "        \"\"\"\n",
    "        means, covariance = self.optimal_control_distribution(t, x)\n",
    "        \n",
    "        # Create multivariate normal distribution\n",
    "        batch_size = means.shape[0]\n",
    "        samples = torch.zeros_like(means)\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        L = torch.linalg.cholesky(covariance)  # Lower triangular Cholesky factor\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Generate standard normal samples\n",
    "            z = torch.randn(self.m, dtype=torch.float64, device=means.device)\n",
    "            # Transform to multivariate normal with the required covariance\n",
    "            samples[i] = means[i] + L @ z\n",
    "        \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would require installing torchdiffeq: pip install torchdiffeq\n",
    "class GPUSoftLQR(SoftLQR):\n",
    "    def solve_ricatti(self) -> None:\n",
    "        \"\"\"\n",
    "        Solve the Ricatti ODE using torchdiffeq for GPU acceleration.\n",
    "        \"\"\"\n",
    "        from torchdiffeq import odeint\n",
    "\n",
    "        # Convert matrices to double precision and move to the proper device\n",
    "        self.H = self.H.to(torch.float64, device=self.device)\n",
    "        self.M = self.M.to(torch.float64, device=self.device)\n",
    "        self.sigma = self.sigma.to(torch.float64, device=self.device)\n",
    "        self.C = self.C.to(torch.float64, device=self.device)\n",
    "        self.D = self.D.to(torch.float64, device=self.device)\n",
    "        self.R = self.R.to(torch.float64, device=self.device)\n",
    "        self.D_inv = self.D_inv.to(torch.float64, device=self.device)\n",
    "        self.sigma_inv = self.sigma_inv.to(torch.float64, device=self.device)\n",
    "        self.sigma_term = self.sigma_term.to(torch.float64, device=self.device)\n",
    "        \n",
    "        # Define RiccatiODE as a torch.nn.Module for torchdiffeq\n",
    "        class RiccatiODE(torch.nn.Module):\n",
    "\n",
    "            def __init__(self, H, M, sigma_term, C, d):\n",
    "                super().__init__()\n",
    "                self.H = H\n",
    "                self.M = M\n",
    "                self.sigma_term = sigma_term\n",
    "                self.C = C\n",
    "                self.d = d\n",
    "            \n",
    "            def forward(self, t, S_flat):\n",
    "                # Reshape S from flattened form\n",
    "                S = S_flat.reshape(self.d, self.d)\n",
    "                \n",
    "                # Compute right-hand side\n",
    "                term1 = S @ self.M @ self.sigma_term @ self.M.T @ S\n",
    "                term2 = self.H.T @ S\n",
    "                term3 = S @ self.H\n",
    "                term4 = self.C\n",
    "                \n",
    "                # Compute derivative\n",
    "                dS = term1 - term2 - term3 - term4\n",
    "                \n",
    "                # Return flattened result\n",
    "                return dS.flatten()\n",
    "        \n",
    "        # Create the ODE module\n",
    "        ode_fn = RiccatiODE(\n",
    "            self.H, self.M, self.sigma_term, self.C, self.d\n",
    "        )\n",
    "        \n",
    "        # Terminal condition: S(T) = R\n",
    "        S_T_flat = self.R.flatten()\n",
    "        \n",
    "        # Time points for ODE solver (reversed for backward integration)\n",
    "        t_points = self.time_grid\n",
    "        t_reversed = self.T - t_points.flip(0)\n",
    "        \n",
    "        # Solve the ODE backward in time (from T to 0) using torchdiffeq\n",
    "        with torch.no_grad():\n",
    "            S_solution = odeint(\n",
    "                ode_fn, \n",
    "                S_T_flat, \n",
    "                t_reversed, \n",
    "                method='dopri5',\n",
    "                rtol=1e-10, \n",
    "                atol=1e-10\n",
    "            )\n",
    "        \n",
    "        # Reshape solution and reverse back to forward time\n",
    "        S_matrices = S_solution.reshape(-1, self.d, self.d).flip(0)\n",
    "        \n",
    "        self.S_grid = S_matrices\n",
    "        \n",
    "        # Compute integral term for value function\n",
    "        self.compute_integral_term()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_comparison(lqr: LQR, soft_lqr: SoftLQR, x0_list: list, num_steps: int, scheme: str = 'explicit') -> None:\n",
    "    \"\"\"\n",
    "    Simulate and compare the standard LQR and soft LQR trajectories.\n",
    "    \n",
    "    Args:\n",
    "        lqr: Standard LQR instance\n",
    "        soft_lqr: Soft LQR instance\n",
    "        x0_list: List of initial states to test\n",
    "        num_steps: Number of time steps\n",
    "        scheme: 'explicit' or 'implicit'\n",
    "    \"\"\"\n",
    "    dt = lqr.T / num_steps\n",
    "    t_grid = torch.linspace(0, lqr.T, num_steps + 1, dtype=torch.float64)\n",
    "    \n",
    "    # Initialize plot\n",
    "    fig, axes = plt.subplots(len(x0_list), 2, figsize=(16, 4*len(x0_list)))\n",
    "    \n",
    "    for i, x0 in enumerate(x0_list):\n",
    "        x0_tensor = torch.tensor([x0], dtype=torch.float64)\n",
    "        \n",
    "        # Ensure all matrices are double precision\n",
    "        H = lqr.H.to(torch.float64)\n",
    "        M = lqr.M.to(torch.float64)\n",
    "        sigma = lqr.sigma.to(torch.float64)\n",
    "        C = lqr.C.to(torch.float64)\n",
    "        D = lqr.D.to(torch.float64)\n",
    "        \n",
    "        # Initialize trajectories and costs\n",
    "        X_lqr = torch.zeros((num_steps + 1, 2), dtype=torch.float64)\n",
    "        X_soft = torch.zeros((num_steps + 1, 2), dtype=torch.float64)\n",
    "        X_lqr[0] = x0_tensor[0]\n",
    "        X_soft[0] = x0_tensor[0]\n",
    "        \n",
    "        costs_lqr = torch.zeros(1, dtype=torch.float64)\n",
    "        costs_soft = torch.zeros(1, dtype=torch.float64)\n",
    "        \n",
    "        # Generate same Brownian increments for both simulations\n",
    "        dW = torch.randn((num_steps, sigma.shape[1]), dtype=torch.float64) * np.sqrt(dt)\n",
    "        \n",
    "        # Simulate trajectories\n",
    "        if scheme == 'explicit':\n",
    "            # Explicit scheme\n",
    "            for n in range(num_steps):\n",
    "                t_n = t_grid[n]\n",
    "                t_tensor = torch.tensor([t_n], dtype=torch.float64)\n",
    "                \n",
    "                # Standard LQR step\n",
    "                control_lqr = lqr.optimal_control(t_tensor, X_lqr[n:n+1]).to(torch.float64)[0]\n",
    "                drift_lqr = H @ X_lqr[n] + M @ control_lqr\n",
    "                X_lqr[n+1] = X_lqr[n] + drift_lqr * dt + sigma @ dW[n]\n",
    "                \n",
    "                # Compute running cost for LQR\n",
    "                state_cost_lqr = X_lqr[n] @ C @ X_lqr[n]\n",
    "                control_cost_lqr = control_lqr @ D @ control_lqr\n",
    "                costs_lqr += (state_cost_lqr + control_cost_lqr) * dt\n",
    "                \n",
    "                # Soft LQR step\n",
    "                control_soft = soft_lqr.optimal_control(t_tensor, X_soft[n:n+1]).to(torch.float64)[0]\n",
    "                drift_soft = H @ X_soft[n] + M @ control_soft\n",
    "                X_soft[n+1] = X_soft[n] + drift_soft * dt + sigma @ dW[n]\n",
    "                \n",
    "                # Compute running cost for soft LQR (includes entropy term)\n",
    "                state_cost_soft = X_soft[n] @ C @ X_soft[n]\n",
    "                control_cost_soft = control_soft @ D @ control_soft\n",
    "                \n",
    "                # Add entropy regularization term\n",
    "                means, covariance = soft_lqr.optimal_control_distribution(t_tensor, X_soft[n:n+1])\n",
    "                entropy_term = soft_lqr.tau * torch.log(torch.det(2 * math.pi * math.e * covariance)).item() / 2\n",
    "                \n",
    "                costs_soft += (state_cost_soft + control_cost_soft + entropy_term) * dt\n",
    "        else:\n",
    "            # Implicit scheme\n",
    "            # Identity matrix for linear system\n",
    "            I = torch.eye(2, dtype=torch.float64)\n",
    "            \n",
    "            for n in range(num_steps):\n",
    "                t_n = t_grid[n]\n",
    "                t_np1 = t_grid[n+1]\n",
    "                t_tensor = torch.tensor([t_n], dtype=torch.float64)\n",
    "                t_np1_tensor = torch.tensor([t_np1], dtype=torch.float64)\n",
    "                \n",
    "                # Standard LQR\n",
    "                control_lqr = lqr.optimal_control(t_tensor, X_lqr[n:n+1]).to(torch.float64)[0]\n",
    "                S_np1_lqr = lqr.get_S_at_time(t_np1_tensor)[0].to(torch.float64)\n",
    "                A_lqr = I - dt * H + dt * M @ lqr.D_inv @ M.T @ S_np1_lqr\n",
    "                b_lqr = X_lqr[n] + sigma @ dW[n]\n",
    "                X_lqr[n+1] = torch.linalg.solve(A_lqr, b_lqr)\n",
    "                \n",
    "                # Compute running cost for LQR\n",
    "                state_cost_lqr = X_lqr[n] @ C @ X_lqr[n]\n",
    "                control_cost_lqr = control_lqr @ D @ control_lqr\n",
    "                costs_lqr += (state_cost_lqr + control_cost_lqr) * dt\n",
    "                \n",
    "                # Soft LQR\n",
    "                control_soft = soft_lqr.optimal_control(t_tensor, X_soft[n:n+1]).to(torch.float64)[0]\n",
    "                S_np1_soft = soft_lqr.get_S_at_time(t_np1_tensor)[0].to(torch.float64)\n",
    "                A_soft = I - dt * H + dt * M @ soft_lqr.sigma_term @ M.T @ S_np1_soft\n",
    "                b_soft = X_soft[n] + sigma @ dW[n]\n",
    "                X_soft[n+1] = torch.linalg.solve(A_soft, b_soft)\n",
    "                \n",
    "                # Compute running cost for soft LQR (includes entropy term)\n",
    "                state_cost_soft = X_soft[n] @ C @ X_soft[n]\n",
    "                control_cost_soft = control_soft @ D @ control_soft\n",
    "                \n",
    "                # Add entropy regularization term\n",
    "                means, covariance = soft_lqr.optimal_control_distribution(t_tensor, X_soft[n:n+1])\n",
    "                entropy_term = soft_lqr.tau * torch.log(torch.det(2 * math.pi * math.e * covariance)).item() / 2\n",
    "                \n",
    "                costs_soft += (state_cost_soft + control_cost_soft + entropy_term) * dt\n",
    "        \n",
    "        # Add terminal costs\n",
    "        terminal_cost_lqr = X_lqr[-1] @ lqr.R @ X_lqr[-1]\n",
    "        terminal_cost_soft = X_soft[-1] @ soft_lqr.R @ X_soft[-1]\n",
    "        costs_lqr += terminal_cost_lqr\n",
    "        costs_soft += terminal_cost_soft\n",
    "        \n",
    "        # Plot trajectories\n",
    "        ax1 = axes[i, 0]\n",
    "        ax1.plot(X_lqr[:, 0].numpy(), X_lqr[:, 1].numpy(), 'b-', label='Standard LQR')\n",
    "        ax1.plot(X_soft[:, 0].numpy(), X_soft[:, 1].numpy(), 'r-', label='Soft LQR')\n",
    "        ax1.scatter([x0[0]], [x0[1]], color='g', s=100, marker='o', label='Initial State')\n",
    "        ax1.set_title(f'Trajectories from Initial State {x0}')\n",
    "        ax1.set_xlabel('X1')\n",
    "        ax1.set_ylabel('X2')\n",
    "        ax1.grid(True)\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Plot running costs\n",
    "        running_costs_lqr = torch.zeros(num_steps + 1)\n",
    "        running_costs_soft = torch.zeros(num_steps + 1)\n",
    "        \n",
    "        # Recompute running costs for plotting\n",
    "        for n in range(num_steps):\n",
    "            t_n = t_grid[n]\n",
    "            t_tensor = torch.tensor([t_n], dtype=torch.float64)\n",
    "            \n",
    "            # LQR costs\n",
    "            control_lqr = lqr.optimal_control(t_tensor, X_lqr[n:n+1])[0]\n",
    "            state_cost_lqr = X_lqr[n] @ C @ X_lqr[n]\n",
    "            control_cost_lqr = control_lqr @ D @ control_lqr\n",
    "            running_costs_lqr[n+1] = running_costs_lqr[n] + (state_cost_lqr + control_cost_lqr) * dt\n",
    "            \n",
    "            # Soft LQR costs\n",
    "            control_soft = soft_lqr.optimal_control(t_tensor, X_soft[n:n+1])[0]\n",
    "            state_cost_soft = X_soft[n] @ C @ X_soft[n]\n",
    "            control_cost_soft = control_soft @ D @ control_soft\n",
    "            means, covariance = soft_lqr.optimal_control_distribution(t_tensor, X_soft[n:n+1])\n",
    "            entropy_term = soft_lqr.tau * torch.log(torch.det(2 * math.pi * math.e * covariance)).item() / 2\n",
    "            running_costs_soft[n+1] = running_costs_soft[n] + (state_cost_soft + control_cost_soft + entropy_term) * dt\n",
    "        \n",
    "        # Add terminal costs\n",
    "        running_costs_lqr[-1] += terminal_cost_lqr\n",
    "        running_costs_soft[-1] += terminal_cost_soft\n",
    "        \n",
    "        ax2 = axes[i, 1]\n",
    "        ax2.plot(t_grid.numpy(), running_costs_lqr.numpy(), 'b-', label='Standard LQR Cost')\n",
    "        ax2.plot(t_grid.numpy(), running_costs_soft.numpy(), 'r-', label='Soft LQR Cost')\n",
    "        ax2.set_title(f'Cost Over Time from Initial State {x0}')\n",
    "        ax2.set_xlabel('Time')\n",
    "        ax2.set_ylabel('Cost')\n",
    "        ax2.grid(True)\n",
    "        ax2.legend()\n",
    "        \n",
    "        print(f\"Initial state {x0}:\")\n",
    "        print(f\"  Standard LQR final cost: {costs_lqr.item():.2f}\")\n",
    "        print(f\"  Soft LQR final cost: {costs_soft.item():.2f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_soft_lqr():\n",
    "    # Set the problem matrices as specified in Figure 1\n",
    "    H = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64) * 0.5\n",
    "    M = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64)\n",
    "    sigma = torch.eye(2, dtype=torch.float64) * 0.5\n",
    "    C = torch.tensor([[1.0, 0.1], [0.1, 1.0]], dtype=torch.float64) * 1.0\n",
    "    D = torch.tensor([[1.0, 0.1], [0.1, 1.0]], dtype=torch.float64) * 0.1\n",
    "    R = torch.tensor([[1.0, 0.3], [0.3, 1.0]], dtype=torch.float64) * 10.0\n",
    "    \n",
    "    # Set the terminal time and time grid\n",
    "    T = 0.5\n",
    "    grid_size = 1000\n",
    "    time_grid = torch.linspace(0, T, grid_size, dtype=torch.float64)\n",
    "    \n",
    "    # Set entropy regularization parameters\n",
    "    tau = 0.1\n",
    "    gamma = 10.0\n",
    "    \n",
    "    # Create standard LQR instance\n",
    "    lqr = LQR(H, M, sigma, C, D, R, T, time_grid)\n",
    "    \n",
    "    # Create soft LQR instance\n",
    "    soft_lqr = SoftLQR(H, M, sigma, C, D, R, T, time_grid, tau, gamma)\n",
    "    \n",
    "    # Solve Ricatti ODEs\n",
    "    lqr.solve_ricatti()\n",
    "    soft_lqr.solve_ricatti()\n",
    "    \n",
    "    # Print S matrices at key time points\n",
    "    print(\"Standard LQR S(0):\\n\", lqr.S_grid[0])\n",
    "    print(\"Soft LQR S(0):\\n\", soft_lqr.S_grid[0])\n",
    "    \n",
    "    # Test points for trajectory comparison\n",
    "    initial_states = [\n",
    "        [2.0, 2.0],\n",
    "        [2.0, -2.0],\n",
    "        [-2.0, -2.0],\n",
    "        [-2.0, 2.0]\n",
    "    ]\n",
    "    \n",
    "    # Compare trajectories\n",
    "    simulate_comparison(lqr, soft_lqr, initial_states, num_steps=200, scheme='explicit')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_soft_lqr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size=512, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Neural network to learn the value function for the soft LQR problem.\n",
    "        \n",
    "        Args:\n",
    "            hidden_size: Size of the hidden layers\n",
    "            device: Device to run the network on\n",
    "        \"\"\"\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        \n",
    "        # Set device\n",
    "        self.device = device\n",
    "        \n",
    "        # Define layers\n",
    "        self.hidden_layer1 = nn.Linear(1, hidden_size).to(device)\n",
    "        self.hidden_layer2 = nn.Linear(hidden_size, hidden_size).to(device)\n",
    "        \n",
    "        # Output for the matrix (2x2 symmetric)\n",
    "        self.matrix_output = nn.Linear(hidden_size, 2*2).to(device)\n",
    "        \n",
    "        # Output for the scalar offset\n",
    "        self.offset_output = nn.Linear(hidden_size, 1).to(device)\n",
    "    \n",
    "    def forward(self, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (matrix, offset) defining the value function\n",
    "        \"\"\"\n",
    "        # Reshape time to (batch, 1)\n",
    "        t = t.view(-1, 1).to(self.device)\n",
    "        \n",
    "        # Forward pass through the hidden layers\n",
    "        hidden = torch.relu(self.hidden_layer1(t))\n",
    "        hidden = torch.relu(self.hidden_layer2(hidden))\n",
    "        \n",
    "        # Compute the matrix output\n",
    "        matrix_elements = self.matrix_output(hidden)\n",
    "        matrix = matrix_elements.view(-1, 2, 2)\n",
    "        \n",
    "        # Make the matrix positive semi-definite by product with transpose\n",
    "        # A^T A is always positive semi-definite\n",
    "        # Adding small constant to diagonal for stability\n",
    "        eye = torch.eye(2, device=self.device).unsqueeze(0).repeat(matrix.shape[0], 1, 1)\n",
    "        matrix = torch.bmm(matrix, matrix.transpose(1, 2)) + 1e-3 * eye\n",
    "        \n",
    "        # Compute the offset\n",
    "        offset = self.offset_output(hidden)\n",
    "        \n",
    "        return matrix, offset\n",
    "    \n",
    "    def value_function(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the value function v(t, x) = x^T S(t) x + b(t)\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            x: State tensor (batch x 2)\n",
    "            \n",
    "        Returns:\n",
    "            Value function at (t, x) (batch)\n",
    "        \"\"\"\n",
    "        # Get the matrix and offset from the network\n",
    "        S, b = self.forward(t)\n",
    "        \n",
    "        # Compute the quadratic term x^T S x\n",
    "        batch_size = x.shape[0]\n",
    "        values = torch.zeros(batch_size, device=self.device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            values[i] = torch.matmul(torch.matmul(x[i], S[i]), x[i])\n",
    "        \n",
    "        # Add the offset term\n",
    "        values = values + b.view(-1)\n",
    "        \n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticAlgorithm:\n",
    "    def __init__(self, soft_lqr: SoftLQR, value_network: ValueNetwork, \n",
    "                 learning_rate: float = 1e-3, device: str = \"cpu\"):\n",
    "        \"\"\"\n",
    "        Critic algorithm to learn the value function for a fixed policy.\n",
    "        \n",
    "        Args:\n",
    "            soft_lqr: Soft LQR instance with the fixed policy\n",
    "            value_network: Neural network to approximate the value function\n",
    "            learning_rate: Learning rate for the optimizer\n",
    "            device: Device to run the algorithm on\n",
    "        \"\"\"\n",
    "        self.soft_lqr = soft_lqr\n",
    "        self.value_network = value_network\n",
    "        self.device = device\n",
    "        \n",
    "        # Move soft_lqr matrices to the device\n",
    "        self.soft_lqr.H = self.soft_lqr.H.to(device)\n",
    "        self.soft_lqr.M = self.soft_lqr.M.to(device)\n",
    "        self.soft_lqr.sigma = self.soft_lqr.sigma.to(device)\n",
    "        self.soft_lqr.C = self.soft_lqr.C.to(device)\n",
    "        self.soft_lqr.D = self.soft_lqr.D.to(device)\n",
    "        self.soft_lqr.R = self.soft_lqr.R.to(device)\n",
    "        self.soft_lqr.D_inv = self.soft_lqr.D_inv.to(device)\n",
    "        self.soft_lqr.sigma_inv = self.soft_lqr.sigma_inv.to(device)\n",
    "        self.soft_lqr.sigma_term = self.soft_lqr.sigma_term.to(device)\n",
    "        if hasattr(self.soft_lqr, 'S_grid') and self.soft_lqr.S_grid is not None:\n",
    "            self.soft_lqr.S_grid = self.soft_lqr.S_grid.to(device)\n",
    "        if hasattr(self.soft_lqr, 'int_term_grid') and self.soft_lqr.int_term_grid is not None:\n",
    "            self.soft_lqr.int_term_grid = self.soft_lqr.int_term_grid.to(device)\n",
    "        \n",
    "        # Setup optimizer\n",
    "        self.optimizer = optim.Adam(self.value_network.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def compute_episode_values(self, initial_states: torch.Tensor, num_steps: int, \n",
    "                              num_episodes: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute the true value function and Monte Carlo estimates for a batch of initial states.\n",
    "        \n",
    "        Args:\n",
    "            initial_states: Initial states tensor (batch x 2)\n",
    "            num_steps: Number of time steps for simulation\n",
    "            num_episodes: Number of episodes for Monte Carlo estimation\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (true_values, mc_estimates, states_trajectory)\n",
    "        \"\"\"\n",
    "        batch_size = initial_states.shape[0]\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Compute true value function at t=0, x=initial_states\n",
    "        t0 = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "        true_values = self.soft_lqr.value_function(t0, initial_states)\n",
    "        \n",
    "        # Initialize trajectories and accumulated costs\n",
    "        states = torch.zeros((num_episodes, batch_size, num_steps + 1, 2), \n",
    "                             device=self.device, dtype=torch.float64)\n",
    "        states[:, :, 0, :] = initial_states.unsqueeze(0).repeat(num_episodes, 1, 1)\n",
    "        \n",
    "        accumulated_costs = torch.zeros((num_episodes, batch_size), \n",
    "                                       device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Generate time grid\n",
    "        t_grid = torch.linspace(0, self.soft_lqr.T, num_steps + 1, \n",
    "                               device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Generate Brownian increments\n",
    "        dW = torch.randn((num_episodes, batch_size, num_steps, self.soft_lqr.sigma.shape[1]), \n",
    "                        device=self.device, dtype=torch.float64) * np.sqrt(dt)\n",
    "        \n",
    "        # Simulate trajectories using explicit scheme\n",
    "        for n in range(num_steps):\n",
    "            t_n = t_grid[n]\n",
    "            t_flat = t_n.repeat(num_episodes * batch_size)\n",
    "            \n",
    "            # Reshape states for batch processing\n",
    "            states_flat = states[:, :, n, :].reshape(-1, 2)\n",
    "            \n",
    "            # Get control means and covariance\n",
    "            means, covariance = self.soft_lqr.optimal_control_distribution(t_flat, states_flat)\n",
    "            means = means.reshape(num_episodes, batch_size, -1)\n",
    "            \n",
    "            # Sample controls\n",
    "            controls = torch.zeros_like(means)\n",
    "            L = torch.linalg.cholesky(covariance)\n",
    "            \n",
    "            for i in range(num_episodes):\n",
    "                for j in range(batch_size):\n",
    "                    # Generate standard normal samples\n",
    "                    z = torch.randn(self.soft_lqr.m, device=self.device, dtype=torch.float64)\n",
    "                    # Transform to multivariate normal\n",
    "                    controls[i, j] = means[i, j] + torch.matmul(L, z)\n",
    "            \n",
    "            # Update states\n",
    "            for i in range(num_episodes):\n",
    "                for j in range(batch_size):\n",
    "                    # Compute drift\n",
    "                    drift = self.soft_lqr.H @ states[i, j, n] + self.soft_lqr.M @ controls[i, j]\n",
    "                    \n",
    "                    # Update state using explicit scheme\n",
    "                    states[i, j, n+1] = states[i, j, n] + drift * dt + self.soft_lqr.sigma @ dW[i, j, n]\n",
    "                    \n",
    "                    # Compute running cost (state cost + control cost + entropy regularization)\n",
    "                    state_cost = states[i, j, n] @ self.soft_lqr.C @ states[i, j, n]\n",
    "                    control_cost = controls[i, j] @ self.soft_lqr.D @ controls[i, j]\n",
    "                    \n",
    "                    # Log probability density of the control under the policy\n",
    "                    log_prob = -0.5 * (\n",
    "                        (controls[i, j] - means[i, j]).T @ torch.inverse(covariance) @ (controls[i, j] - means[i, j])\n",
    "                        + torch.log(torch.det(2 * np.pi * covariance))\n",
    "                    )\n",
    "                    \n",
    "                    # Entropy regularization term\n",
    "                    entropy_term = self.soft_lqr.tau * (-log_prob)\n",
    "                    \n",
    "                    # Accumulate costs\n",
    "                    accumulated_costs[i, j] += (state_cost + control_cost + entropy_term) * dt\n",
    "        \n",
    "        # Add terminal costs\n",
    "        for i in range(num_episodes):\n",
    "            for j in range(batch_size):\n",
    "                terminal_cost = states[i, j, -1] @ self.soft_lqr.R @ states[i, j, -1]\n",
    "                accumulated_costs[i, j] += terminal_cost\n",
    "        \n",
    "        # Compute Monte Carlo estimates (mean over episodes)\n",
    "        mc_estimates = accumulated_costs.mean(dim=0)\n",
    "        \n",
    "        return true_values, mc_estimates, states\n",
    "    \n",
    "    def train_step(self, initial_states_dist, num_states: int, num_steps: int, \n",
    "                  num_episodes: int) -> float:\n",
    "        \"\"\"\n",
    "        Perform one training step of the critic algorithm.\n",
    "        \n",
    "        Args:\n",
    "            initial_states_dist: Distribution to sample initial states from\n",
    "            num_states: Number of initial states to sample\n",
    "            num_steps: Number of time steps for simulation\n",
    "            num_episodes: Number of episodes for Monte Carlo estimation\n",
    "            \n",
    "        Returns:\n",
    "            Loss value for this step\n",
    "        \"\"\"\n",
    "        # Sample initial states\n",
    "        initial_states = initial_states_dist.sample((num_states,)).to(self.device)\n",
    "        \n",
    "        # Compute true values and Monte Carlo estimates\n",
    "        _, mc_estimates, states = self.compute_episode_values(\n",
    "            initial_states, num_steps, num_episodes)\n",
    "        \n",
    "        # Initialize loss\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        # Compute loss for each time step\n",
    "        for n in range(num_steps + 1):\n",
    "            # Get time and states at this step\n",
    "            t_n = torch.ones(num_states, device=self.device, dtype=torch.float64) * (n * self.soft_lqr.T / num_steps)\n",
    "            x_n = states[0, :, n, :]  # Use first episode for states\n",
    "            \n",
    "            # Compute value function prediction\n",
    "            predicted_values = self.value_network.value_function(t_n, x_n)\n",
    "            \n",
    "            # Compute target values at this time step\n",
    "            if n == num_steps:\n",
    "                # At terminal time, target is just the terminal cost\n",
    "                target_values = torch.zeros(num_states, device=self.device, dtype=torch.float64)\n",
    "                for j in range(num_states):\n",
    "                    target_values[j] = x_n[j] @ self.soft_lqr.R @ x_n[j]\n",
    "            else:\n",
    "                # For non-terminal times, target is the Monte Carlo estimate\n",
    "                remaining_costs = torch.zeros((num_episodes, num_states), \n",
    "                                            device=self.device, dtype=torch.float64)\n",
    "                \n",
    "                # Recompute remaining costs for this time step\n",
    "                for i in range(num_episodes):\n",
    "                    for j in range(num_states):\n",
    "                        for k in range(n, num_steps):\n",
    "                            # Time at step k\n",
    "                            t_k = k * self.soft_lqr.T / num_steps\n",
    "                            t_tensor = torch.tensor([t_k], device=self.device, dtype=torch.float64)\n",
    "                            \n",
    "                            # State and control at step k\n",
    "                            state_k = states[i, j, k]\n",
    "                            state_tensor = state_k.unsqueeze(0)\n",
    "                            \n",
    "                            # Get control distribution\n",
    "                            means, covariance = self.soft_lqr.optimal_control_distribution(\n",
    "                                t_tensor, state_tensor)\n",
    "                            \n",
    "                            # Sample control\n",
    "                            L = torch.linalg.cholesky(covariance)\n",
    "                            z = torch.randn(self.soft_lqr.m, device=self.device, dtype=torch.float64)\n",
    "                            control = means[0] + torch.matmul(L, z)\n",
    "                            \n",
    "                            # Compute costs\n",
    "                            state_cost = state_k @ self.soft_lqr.C @ state_k\n",
    "                            control_cost = control @ self.soft_lqr.D @ control\n",
    "                            \n",
    "                            # Log probability\n",
    "                            log_prob = -0.5 * (\n",
    "                                (control - means[0]).T @ torch.inverse(covariance) @ (control - means[0])\n",
    "                                + torch.log(torch.det(2 * np.pi * covariance))\n",
    "                            )\n",
    "                            \n",
    "                            # Entropy term\n",
    "                            entropy_term = self.soft_lqr.tau * (-log_prob)\n",
    "                            \n",
    "                            # Add to remaining costs\n",
    "                            remaining_costs[i, j] += (state_cost + control_cost + entropy_term) * (self.soft_lqr.T / num_steps)\n",
    "                        \n",
    "                        # Add terminal cost\n",
    "                        terminal_state = states[i, j, -1]\n",
    "                        terminal_cost = terminal_state @ self.soft_lqr.R @ terminal_state\n",
    "                        remaining_costs[i, j] += terminal_cost\n",
    "                \n",
    "                # Average over episodes\n",
    "                target_values = remaining_costs.mean(dim=0)\n",
    "            \n",
    "            # Compute MSE loss for this time step\n",
    "            step_loss = ((predicted_values - target_values) ** 2).mean()\n",
    "            total_loss += step_loss\n",
    "        \n",
    "        # Average loss over time steps\n",
    "        avg_loss = total_loss / (num_steps + 1)\n",
    "        \n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        avg_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return avg_loss.item()\n",
    "    \n",
    "    def train(self, epochs: int, batch_size: int, num_steps: int, num_episodes: int, \n",
    "             initial_states_dist, eval_freq: int = 10, verbose: bool = True) -> List[float]:\n",
    "        \"\"\"\n",
    "        Train the critic algorithm for multiple epochs.\n",
    "        \n",
    "        Args:\n",
    "            epochs: Number of training epochs\n",
    "            batch_size: Batch size (number of initial states) per epoch\n",
    "            num_steps: Number of time steps for simulation\n",
    "            num_episodes: Number of episodes for Monte Carlo estimation\n",
    "            initial_states_dist: Distribution to sample initial states from\n",
    "            eval_freq: Frequency of evaluation\n",
    "            verbose: Whether to print progress\n",
    "            \n",
    "        Returns:\n",
    "            List of loss values during training\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Perform one training step\n",
    "            loss = self.train_step(initial_states_dist, batch_size, num_steps, num_episodes)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch+1) % eval_freq == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4e}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def evaluate(self, test_states: torch.Tensor, plot: bool = True) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Evaluate the trained value network against the true value function.\n",
    "        \n",
    "        Args:\n",
    "            test_states: Test states tensor (batch x 2)\n",
    "            plot: Whether to plot the results\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of errors\n",
    "        \"\"\"\n",
    "        # Compute true value function at t=0, x=test_states\n",
    "        batch_size = test_states.shape[0]\n",
    "        t0 = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "        true_values = self.soft_lqr.value_function(t0, test_states)\n",
    "        \n",
    "        # Compute value function prediction\n",
    "        predicted_values = self.value_network.value_function(t0, test_states)\n",
    "        \n",
    "        # Compute errors\n",
    "        errors = torch.abs(predicted_values - true_values)\n",
    "        \n",
    "        # Print statistics\n",
    "        max_error = errors.max().item()\n",
    "        mean_error = errors.mean().item()\n",
    "        print(f\"Evaluation results:\")\n",
    "        print(f\"  Max error: {max_error:.4e}\")\n",
    "        print(f\"  Mean error: {mean_error:.4e}\")\n",
    "        \n",
    "        # Plot results if requested\n",
    "        if plot:\n",
    "            # Convert to numpy for plotting\n",
    "            test_states_np = test_states.cpu().numpy()\n",
    "            true_values_np = true_values.cpu().numpy()\n",
    "            predicted_values_np = predicted_values.cpu().numpy()\n",
    "            errors_np = errors.cpu().numpy()\n",
    "            \n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            # Plot true vs predicted values\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.scatter(true_values_np, predicted_values_np, alpha=0.7)\n",
    "            min_val = min(true_values_np.min(), predicted_values_np.min())\n",
    "            max_val = max(true_values_np.max(), predicted_values_np.max())\n",
    "            plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "            plt.title('True vs Predicted Values')\n",
    "            plt.xlabel('True Value')\n",
    "            plt.ylabel('Predicted Value')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            # Plot errors vs true values\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.scatter(true_values_np, errors_np, alpha=0.7)\n",
    "            plt.title('Errors vs True Values')\n",
    "            plt.xlabel('True Value')\n",
    "            plt.ylabel('Absolute Error')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_critic():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Problem setup as specified in the exercise\n",
    "    H = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64) * 0.5\n",
    "    M = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64)\n",
    "    sigma = torch.eye(2, dtype=torch.float64) * 0.5\n",
    "    C = torch.tensor([[1.0, 0.1], [0.1, 1.0]], dtype=torch.float64) * 1.0\n",
    "    D = torch.eye(2, dtype=torch.float64)  # D just identity, as specified\n",
    "    R = torch.tensor([[1.0, 0.3], [0.3, 1.0]], dtype=torch.float64) * 10.0\n",
    "    \n",
    "    # Set terminal time and time grid\n",
    "    T = 0.5\n",
    "    grid_size = 1000\n",
    "    time_grid = torch.linspace(0, T, grid_size, dtype=torch.float64)\n",
    "    \n",
    "    # Set entropy regularization parameters\n",
    "    tau = 0.5  # τ = 1/2 as specified\n",
    "    gamma = 1.0  # γ = 1 as specified\n",
    "    \n",
    "    # Create soft LQR instance\n",
    "    soft_lqr = SoftLQR(H, M, sigma, C, D, R, T, time_grid, tau, gamma)\n",
    "    \n",
    "    # Solve Ricatti ODE\n",
    "    print(\"Solving Ricatti ODE...\")\n",
    "    soft_lqr.solve_ricatti()\n",
    "    \n",
    "    # Create value network\n",
    "    hidden_size = 512  # As specified\n",
    "    value_network = ValueNetwork(hidden_size=hidden_size, device=device)\n",
    "    \n",
    "    # Create critic algorithm\n",
    "    learning_rate = 1e-3  # As specified\n",
    "    critic = CriticAlgorithm(soft_lqr, value_network, learning_rate, device)\n",
    "    \n",
    "    # Define initial state distribution\n",
    "    # Uniform distribution in [-3, 3] x [-3, 3]\n",
    "    class UniformInitialState:\n",
    "        def __init__(self, low=-3.0, high=3.0, dim=2):\n",
    "            self.low = low\n",
    "            self.high = high\n",
    "            self.dim = dim\n",
    "        \n",
    "        def sample(self, shape):\n",
    "            return torch.empty((*shape, self.dim), dtype=torch.float64).uniform_(self.low, self.high)\n",
    "    \n",
    "    initial_states_dist = UniformInitialState()\n",
    "    \n",
    "    # Training parameters\n",
    "    epochs = 500\n",
    "    batch_size = 64\n",
    "    num_steps = 100  # N = 100 as specified\n",
    "    num_episodes = 16  # Number of episodes for Monte Carlo estimation\n",
    "    eval_freq = 25\n",
    "    \n",
    "    # Train the critic algorithm\n",
    "    print(\"Training critic algorithm...\")\n",
    "    losses = critic.train(epochs, batch_size, num_steps, num_episodes, \n",
    "                         initial_states_dist, eval_freq, verbose=True)\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.semilogy(losses)\n",
    "    plt.title('Critic Loss During Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate on test states\n",
    "    print(\"Evaluating on test states...\")\n",
    "    \n",
    "    # Generate grid of test states in [-3, 3] x [-3, 3]\n",
    "    x1 = torch.linspace(-3, 3, 11, dtype=torch.float64)\n",
    "    x2 = torch.linspace(-3, 3, 11, dtype=torch.float64)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    test_states = torch.stack([X1.flatten(), X2.flatten()], dim=1).to(device)\n",
    "    \n",
    "    # Evaluate the critic\n",
    "    errors = critic.evaluate(test_states, plot=True)\n",
    "    \n",
    "    # Plot value function\n",
    "    t0 = torch.zeros(test_states.shape[0], device=device, dtype=torch.float64)\n",
    "    true_values = soft_lqr.value_function(t0, test_states)\n",
    "    predicted_values = value_network.value_function(t0, test_states)\n",
    "    \n",
    "    # Convert to numpy for plotting\n",
    "    X1_np = X1.cpu().numpy()\n",
    "    X2_np = X2.cpu().numpy()\n",
    "    true_values_np = true_values.cpu().numpy().reshape(X1_np.shape)\n",
    "    predicted_values_np = predicted_values.cpu().numpy().reshape(X1_np.shape)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot true value function\n",
    "    plt.subplot(1, 2, 1)\n",
    "    contour = plt.contourf(X1_np, X2_np, true_values_np, 20, cmap='viridis')\n",
    "    plt.colorbar(contour)\n",
    "    plt.title('True Value Function')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    # Plot predicted value function\n",
    "    plt.subplot(1, 2, 2)\n",
    "    contour = plt.contourf(X1_np, X2_np, predicted_values_np, 20, cmap='viridis')\n",
    "    plt.colorbar(contour)\n",
    "    plt.title('Predicted Value Function')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_critic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUOptimizedCritic(CriticAlgorithm):\n",
    "    def compute_episode_values(self, initial_states: torch.Tensor, num_steps: int, \n",
    "                              num_episodes: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        GPU-optimized version that vectorizes operations across episodes and states.\n",
    "        \"\"\"\n",
    "        batch_size = initial_states.shape[0]\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Compute true value function at t=0, x=initial_states\n",
    "        t0 = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "        true_values = self.soft_lqr.value_function(t0, initial_states)\n",
    "        \n",
    "        # Initialize trajectories and accumulated costs\n",
    "        # Shape: [num_episodes, batch_size, num_steps + 1, 2]\n",
    "        states = torch.zeros((num_episodes, batch_size, num_steps + 1, 2), \n",
    "                             device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Expand initial states for all episodes\n",
    "        states[:, :, 0, :] = initial_states.unsqueeze(0).repeat(num_episodes, 1, 1)\n",
    "        \n",
    "        accumulated_costs = torch.zeros((num_episodes, batch_size), \n",
    "                                       device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Generate time grid\n",
    "        t_grid = torch.linspace(0, self.soft_lqr.T, num_steps + 1, \n",
    "                              device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Generate all Brownian increments at once\n",
    "        dW = torch.randn((num_episodes, batch_size, num_steps, self.soft_lqr.sigma.shape[1]), \n",
    "                        device=self.device, dtype=torch.float64) * np.sqrt(dt)\n",
    "        \n",
    "        # Pre-compute Cholesky decomposition of covariance if it's constant\n",
    "        covariance = self.soft_lqr.tau * self.soft_lqr.sigma_term\n",
    "        L_chol = torch.linalg.cholesky(covariance)\n",
    "        \n",
    "        # Simulate trajectories using explicit scheme - vectorized across all episodes\n",
    "        for n in range(num_steps):\n",
    "            # Current time\n",
    "            t_n = t_grid[n]\n",
    "            \n",
    "            # Get states at current time step\n",
    "            # Shape: [num_episodes, batch_size, 2]\n",
    "            current_states = states[:, :, n, :]\n",
    "            \n",
    "            # Reshape for batched processing\n",
    "            # Shape: [num_episodes*batch_size, 2]\n",
    "            flat_states = current_states.reshape(-1, 2)\n",
    "            flat_times = t_n.repeat(num_episodes * batch_size)\n",
    "            \n",
    "            # Get control means for all states\n",
    "            # Shape: [num_episodes*batch_size, control_dim]\n",
    "            means, _ = self.soft_lqr.optimal_control_distribution(flat_times, flat_states)\n",
    "            means = means.reshape(num_episodes, batch_size, -1)\n",
    "            \n",
    "            # Generate standard normal samples for all episodes/states at once\n",
    "            # Shape: [num_episodes, batch_size, control_dim]\n",
    "            z = torch.randn((num_episodes, batch_size, self.soft_lqr.m), \n",
    "                           device=self.device, dtype=torch.float64)\n",
    "            \n",
    "            # Transform to multivariate normal - vectorized\n",
    "            # Shape: [num_episodes, batch_size, control_dim]\n",
    "            controls = means + torch.matmul(z.view(-1, self.soft_lqr.m), L_chol.T).view(num_episodes, batch_size, -1)\n",
    "            \n",
    "            # Compute all drifts at once using broadcasting\n",
    "            # Shape: [num_episodes, batch_size, 2]\n",
    "            drifts = torch.matmul(current_states, self.soft_lqr.H.T) + torch.matmul(controls, self.soft_lqr.M.T)\n",
    "            \n",
    "            # Update all states at once\n",
    "            # Shape: [num_episodes, batch_size, 2]\n",
    "            states[:, :, n+1, :] = current_states + drifts * dt + torch.matmul(dW[:, :, n, :], self.soft_lqr.sigma.T)\n",
    "            \n",
    "            # Compute all costs at once\n",
    "            \n",
    "            # State costs: x^T C x\n",
    "            # Shape: [num_episodes, batch_size]\n",
    "            state_costs = torch.sum(current_states * torch.matmul(current_states, self.soft_lqr.C.T), dim=-1)\n",
    "            \n",
    "            # Control costs: u^T D u\n",
    "            # Shape: [num_episodes, batch_size]\n",
    "            control_costs = torch.sum(controls * torch.matmul(controls, self.soft_lqr.D.T), dim=-1)\n",
    "            \n",
    "            # Entropy regularization term - computed efficiently using matrix operations\n",
    "            # Shape: [num_episodes, batch_size]\n",
    "            control_diff = (controls - means).reshape(-1, self.soft_lqr.m)\n",
    "            inv_cov = torch.inverse(covariance)\n",
    "            quadratic_term = torch.sum(control_diff * torch.matmul(control_diff, inv_cov.T), dim=-1)\n",
    "            log_det_term = torch.log(torch.det(2 * np.pi * covariance))\n",
    "            log_probs = -0.5 * (quadratic_term + log_det_term)\n",
    "            entropy_terms = self.soft_lqr.tau * (-log_probs).reshape(num_episodes, batch_size)\n",
    "            \n",
    "            # Accumulate all costs\n",
    "            accumulated_costs += (state_costs + control_costs + entropy_terms) * dt\n",
    "        \n",
    "        # Compute terminal costs for all episodes at once\n",
    "        # Shape: [num_episodes, batch_size]\n",
    "        terminal_states = states[:, :, -1, :]\n",
    "        terminal_costs = torch.sum(terminal_states * torch.matmul(terminal_states, self.soft_lqr.R.T), dim=-1)\n",
    "        accumulated_costs += terminal_costs\n",
    "        \n",
    "        # Compute Monte Carlo estimates (mean over episodes)\n",
    "        mc_estimates = accumulated_costs.mean(dim=0)\n",
    "        \n",
    "        return true_values, mc_estimates, states\n",
    "        \n",
    "    def train_batch(self, initial_states: torch.Tensor, num_steps: int, \n",
    "                   num_episodes: int) -> float:\n",
    "        \"\"\"\n",
    "        Train on a batch of initial states with parallelized computation.\n",
    "        \"\"\"\n",
    "        batch_size = initial_states.shape[0]\n",
    "        \n",
    "        # Get Monte Carlo estimates\n",
    "        _, mc_estimates, states = self.compute_episode_values(\n",
    "            initial_states, num_steps, num_episodes)\n",
    "        \n",
    "        # Generate the time grid\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        t_grid = torch.linspace(0, self.soft_lqr.T, num_steps + 1, \n",
    "                              device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Compute predictions and loss for all time steps at once\n",
    "        all_losses = []\n",
    "        \n",
    "        for n in range(num_steps + 1):\n",
    "            # Current time and states\n",
    "            t_n = torch.ones(batch_size, device=self.device, dtype=torch.float64) * t_grid[n]\n",
    "            x_n = states[0, :, n, :]  # Use first episode\n",
    "            \n",
    "            # Get value function predictions\n",
    "            predicted = self.value_network.value_function(t_n, x_n)\n",
    "            \n",
    "            # Terminal time - target is terminal cost\n",
    "            if n == num_steps:\n",
    "                target = torch.sum(x_n * torch.matmul(x_n, self.soft_lqr.R.T), dim=-1)\n",
    "            else:\n",
    "                # For non-terminal, we need to collect remaining costs\n",
    "                # This is still done with loops, but we could further optimize with custom CUDA kernels\n",
    "                remaining_costs = torch.zeros((num_episodes, batch_size), \n",
    "                                           device=self.device, dtype=torch.float64)\n",
    "                \n",
    "                for i in range(num_episodes):\n",
    "                    # Extract trajectory for this episode from n onwards\n",
    "                    traj = states[i, :, n:, :]\n",
    "                    \n",
    "                    # Time points from n onwards\n",
    "                    time_points = t_grid[n:]\n",
    "                    \n",
    "                    # Compute costs for all remaining steps\n",
    "                    for k in range(len(time_points)-1):\n",
    "                        t_k = time_points[k]\n",
    "                        state_k = traj[:, k, :]\n",
    "                        \n",
    "                        # Reshape for batched processing\n",
    "                        flat_t_k = t_k.repeat(batch_size)\n",
    "                        \n",
    "                        # Get control means\n",
    "                        means, covariance = self.soft_lqr.optimal_control_distribution(flat_t_k, state_k)\n",
    "                        \n",
    "                        # Sample controls using pre-computed Cholesky\n",
    "                        L = torch.linalg.cholesky(covariance)\n",
    "                        z = torch.randn((batch_size, self.soft_lqr.m), device=self.device, dtype=torch.float64)\n",
    "                        controls = means + torch.matmul(z, L.T)\n",
    "                        \n",
    "                        # State costs - vectorized\n",
    "                        state_costs = torch.sum(state_k * torch.matmul(state_k, self.soft_lqr.C.T), dim=-1)\n",
    "                        \n",
    "                        # Control costs - vectorized\n",
    "                        control_costs = torch.sum(controls * torch.matmul(controls, self.soft_lqr.D.T), dim=-1)\n",
    "                        \n",
    "                        # Entropy term - vectorized\n",
    "                        control_diff = controls - means\n",
    "                        inv_cov = torch.inverse(covariance)\n",
    "                        quadratic = torch.sum(control_diff * torch.matmul(control_diff, inv_cov.T), dim=-1)\n",
    "                        log_det = torch.log(torch.det(2 * np.pi * covariance))\n",
    "                        entropy = self.soft_lqr.tau * (0.5 * (quadratic + log_det))\n",
    "                        \n",
    "                        # Add costs\n",
    "                        remaining_costs[i, :] += (state_costs + control_costs + entropy) * dt\n",
    "                    \n",
    "                    # Add terminal costs\n",
    "                    terminal_states = traj[:, -1, :]\n",
    "                    terminal_costs = torch.sum(terminal_states * torch.matmul(terminal_states, self.soft_lqr.R.T), dim=-1)\n",
    "                    remaining_costs[i, :] += terminal_costs\n",
    "                \n",
    "                # Average over episodes\n",
    "                target = remaining_costs.mean(dim=0)\n",
    "            \n",
    "            # Compute MSE loss\n",
    "            loss = ((predicted - target) ** 2).mean()\n",
    "            all_losses.append(loss)\n",
    "        \n",
    "        # Sum losses over all time steps\n",
    "        total_loss = torch.stack(all_losses).sum()\n",
    "        \n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return total_loss.item() / (num_steps + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PyTorch's automatic mixed precision\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "class MixedPrecisionCritic(GPUOptimizedCritic):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Create a gradient scaler for mixed precision training\n",
    "        self.scaler = GradScaler()\n",
    "    \n",
    "    def train_batch(self, initial_states: torch.Tensor, num_steps: int, \n",
    "                   num_episodes: int) -> float:\n",
    "        \"\"\"\n",
    "        Train with mixed precision for better GPU utilization.\n",
    "        \"\"\"\n",
    "        batch_size = initial_states.shape[0]\n",
    "        \n",
    "        # The Monte Carlo simulation should still use fp64 for accuracy\n",
    "        # but we'll cast to fp32 for the neural network operations\n",
    "        initial_states_fp32 = initial_states.to(torch.float32)\n",
    "        \n",
    "        # Get Monte Carlo estimates\n",
    "        _, mc_estimates, states = self.compute_episode_values(\n",
    "            initial_states, num_steps, num_episodes)\n",
    "        \n",
    "        # Cast to fp32 for neural network training\n",
    "        mc_estimates_fp32 = mc_estimates.to(torch.float32)\n",
    "        states_fp32 = states.to(torch.float32)\n",
    "        \n",
    "        # Generate the time grid\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        t_grid = torch.linspace(0, self.soft_lqr.T, num_steps + 1, \n",
    "                              device=self.device, dtype=torch.float32)\n",
    "        \n",
    "        # Prepare for backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Total loss across all time steps\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        # Use automatic mixed precision\n",
    "        with autocast():\n",
    "            for n in range(num_steps + 1):\n",
    "                # Current time and states\n",
    "                t_n = torch.ones(batch_size, device=self.device, dtype=torch.float32) * t_grid[n]\n",
    "                x_n = states_fp32[0, :, n, :]  # Use first episode\n",
    "                \n",
    "                # Get value function predictions\n",
    "                predicted = self.value_network.value_function(t_n, x_n)\n",
    "                \n",
    "                # Terminal time - target is terminal cost\n",
    "                if n == num_steps:\n",
    "                    # Compute terminal cost directly\n",
    "                    R_fp32 = self.soft_lqr.R.to(torch.float32)\n",
    "                    target = torch.sum(x_n * torch.matmul(x_n, R_fp32.T), dim=-1)\n",
    "                else:\n",
    "                    # For non-terminal, we can use targets from our MC estimation\n",
    "                    # This can be further optimized, but we simplify for illustration\n",
    "                    target = torch.zeros_like(predicted)\n",
    "                    \n",
    "                    # Compute remaining cost from this point onward\n",
    "                    for i in range(batch_size):\n",
    "                        remaining_idx = range(n, num_steps + 1)\n",
    "                        target[i] = mc_estimates_fp32[i] * (len(remaining_idx) / (num_steps + 1))\n",
    "                \n",
    "                # Compute MSE loss\n",
    "                loss = ((predicted - target) ** 2).mean()\n",
    "                total_loss = total_loss + loss\n",
    "        \n",
    "        # Average loss\n",
    "        avg_loss = total_loss / (num_steps + 1)\n",
    "        \n",
    "        # Scale gradients and optimize with mixed precision\n",
    "        self.scaler.scale(avg_loss).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        \n",
    "        return avg_loss.item()\n",
    "        \n",
    "    def train(self, epochs: int, batch_size: int, num_steps: int, num_episodes: int, \n",
    "             initial_states_dist, eval_freq: int = 10, verbose: bool = True) -> List[float]:\n",
    "        \"\"\"\n",
    "        Train with mixed precision and learning rate scheduling.\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        # Add learning rate scheduler for better convergence\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, 'min', factor=0.5, patience=20, verbose=verbose\n",
    "        )\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Sample initial states\n",
    "            initial_states = initial_states_dist.sample((batch_size,)).to(self.device)\n",
    "            \n",
    "            # Perform one training step\n",
    "            loss = self.train_batch(initial_states, num_steps, num_episodes)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Update learning rate based on performance\n",
    "            scheduler.step(loss)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch+1) % eval_freq == 0:\n",
    "                current_lr = self.optimizer.param_groups[0]['lr']\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4e}, LR: {current_lr:.4e}\")\n",
    "        \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_critic_gpu():\n",
    "    # Ensure we use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    \n",
    "    # Problem setup as specified in the exercise\n",
    "    H = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64, device=device) * 0.5\n",
    "    M = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64, device=device)\n",
    "    sigma = torch.eye(2, dtype=torch.float64, device=device) * 0.5\n",
    "    C = torch.tensor([[1.0, 0.1], [0.1, 1.0]], dtype=torch.float64, device=device) * 1.0\n",
    "    D = torch.eye(2, dtype=torch.float64, device=device)  # D just identity, as specified\n",
    "    R = torch.tensor([[1.0, 0.3], [0.3, 1.0]], dtype=torch.float64, device=device) * 10.0\n",
    "    \n",
    "    # Set terminal time and time grid\n",
    "    T = 0.5\n",
    "    grid_size = 1000\n",
    "    time_grid = torch.linspace(0, T, grid_size, dtype=torch.float64, device=device)\n",
    "    \n",
    "    # Set entropy regularization parameters\n",
    "    tau = 0.5  # τ = 1/2 as specified\n",
    "    gamma = 1.0  # γ = 1 as specified\n",
    "    \n",
    "    # Create GPU-accelerated soft LQR instance\n",
    "    soft_lqr = GPUSoftLQR(\n",
    "        H, M, sigma, C, D, R, T, time_grid, tau, gamma, device=device\n",
    "    )\n",
    "    \n",
    "    # Solve Ricatti ODE using GPU\n",
    "    print(\"Solving Ricatti ODE on GPU...\")\n",
    "    with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "        soft_lqr.solve_ricatti()\n",
    "    \n",
    "    # Create value network with larger capacity for GPU\n",
    "    hidden_size = 1024  # Increased from 512 for GPU\n",
    "    value_network = ValueNetwork(hidden_size=hidden_size, device=device)\n",
    "    \n",
    "    # Create mixed precision critic algorithm\n",
    "    learning_rate = 2e-3  # Slightly higher for mixed precision\n",
    "    critic = MixedPrecisionCritic(soft_lqr, value_network, learning_rate, device)\n",
    "    \n",
    "    # Define initial state distribution\n",
    "    class UniformInitialState:\n",
    "        def __init__(self, low=-3.0, high=3.0, dim=2, device=device):\n",
    "            self.low = low\n",
    "            self.high = high\n",
    "            self.dim = dim\n",
    "            self.device = device\n",
    "        \n",
    "        def sample(self, shape):\n",
    "            return torch.empty(\n",
    "                (*shape, self.dim), \n",
    "                dtype=torch.float64, \n",
    "                device=self.device\n",
    "            ).uniform_(self.low, self.high)\n",
    "    \n",
    "    initial_states_dist = UniformInitialState(device=device)\n",
    "    \n",
    "    # Training parameters - larger batch and parallel processing for GPU\n",
    "    epochs = 500\n",
    "    batch_size = 256  # Increased from 64 for GPU\n",
    "    num_steps = 100  # N = 100 as specified\n",
    "    num_episodes = 64  # Increased from 16 for GPU\n",
    "    eval_freq = 25\n",
    "    \n",
    "    # Create a CUDA events for timing\n",
    "    if torch.cuda.is_available():\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "        start_event.record()\n",
    "    \n",
    "    # Train with mixed precision on GPU\n",
    "    print(\"Training critic algorithm with GPU acceleration...\")\n",
    "    losses = critic.train(\n",
    "        epochs, batch_size, num_steps, num_episodes, \n",
    "        initial_states_dist, eval_freq, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Record time\n",
    "    if torch.cuda.is_available():\n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "        training_time = start_event.elapsed_time(end_event) / 1000  # Convert to seconds\n",
    "        print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Plot loss with PyTorch-Lightning's rich logging\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.semilogy(losses)\n",
    "    plt.title('Critic Loss During Training (GPU Accelerated)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate on larger test grid for better visualization\n",
    "    print(\"Evaluating on test grid with GPU...\")\n",
    "    \n",
    "    # Generate dense grid of test states in [-3, 3] x [-3, 3]\n",
    "    x1 = torch.linspace(-3, 3, 21, dtype=torch.float64, device=device)\n",
    "    x2 = torch.linspace(-3, 3, 21, dtype=torch.float64, device=device)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    test_states = torch.stack([X1.flatten(), X2.flatten()], dim=1)\n",
    "    \n",
    "    # Evaluate the critic using GPU\n",
    "    with torch.cuda.amp.autocast('cuda', enabled=torch.cuda.is_available()):\n",
    "        errors = critic.evaluate(test_states, plot=True)\n",
    "    \n",
    "    # Create interactive 3D plot of value function\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    \n",
    "    t0 = torch.zeros(test_states.shape[0], device=device, dtype=torch.float64)\n",
    "    \n",
    "    # Get values in mixed precision for GPU efficiency\n",
    "    with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "        true_values = soft_lqr.value_function(t0, test_states)\n",
    "        predicted_values = value_network.value_function(t0.to(torch.float32), test_states.to(torch.float32))\n",
    "    \n",
    "    # Convert to numpy for plotting\n",
    "    X1_np = X1.cpu().numpy()\n",
    "    X2_np = X2.cpu().numpy()\n",
    "    true_values_np = true_values.cpu().numpy().reshape(X1_np.shape)\n",
    "    predicted_values_np = predicted_values.cpu().numpy().reshape(X1_np.shape)\n",
    "    \n",
    "    # Create 3D plots\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # True value function\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    surf1 = ax1.plot_surface(X1_np, X2_np, true_values_np, cmap='viridis', \n",
    "                            linewidth=0, antialiased=True)\n",
    "    ax1.set_title('True Value Function')\n",
    "    ax1.set_xlabel('x1')\n",
    "    ax1.set_ylabel('x2')\n",
    "    ax1.set_zlabel('Value')\n",
    "    fig.colorbar(surf1, ax=ax1, shrink=0.5, aspect=5)\n",
    "    \n",
    "    # Predicted value function\n",
    "    ax2 = fig.add_subplot(122, projection='3d')\n",
    "    surf2 = ax2.plot_surface(X1_np, X2_np, predicted_values_np, cmap='plasma', \n",
    "                            linewidth=0, antialiased=True)\n",
    "    ax2.set_title('Predicted Value Function')\n",
    "    ax2.set_xlabel('x1')\n",
    "    ax2.set_ylabel('x2')\n",
    "    ax2.set_zlabel('Value')\n",
    "    fig.colorbar(surf2, ax=ax2, shrink=0.5, aspect=5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"GPU-accelerated critic training completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_critic_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size=256, state_dim=2, control_dim=2, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Neural network to learn the policy function for the soft LQR problem.\n",
    "        \n",
    "        Args:\n",
    "            hidden_size: Size of the hidden layers\n",
    "            state_dim: Dimension of the state space\n",
    "            control_dim: Dimension of the control space\n",
    "            device: Device to run the network on\n",
    "        \"\"\"\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        # Store dimensions\n",
    "        self.state_dim = state_dim\n",
    "        self.control_dim = control_dim\n",
    "        self.device = device\n",
    "        \n",
    "        # Define layers for time input\n",
    "        self.time_layer1 = nn.Linear(1, hidden_size).to(device)\n",
    "        self.time_layer2 = nn.Linear(hidden_size, hidden_size).to(device)\n",
    "        \n",
    "        # Define layers for phi matrix\n",
    "        # Phi maps from state to control mean\n",
    "        self.phi_output = nn.Linear(hidden_size, state_dim * control_dim).to(device)\n",
    "        \n",
    "        # Define layers for the L matrix of the covariance\n",
    "        # We use a lower triangular matrix L to ensure positive-definiteness via Σ = LL^T\n",
    "        self.L_output = nn.Linear(\n",
    "            hidden_size, control_dim * (control_dim + 1) // 2\n",
    "        ).to(device)\n",
    "        \n",
    "        # Precompute indices for the lower triangular matrix\n",
    "        self.tril_indices = torch.tril_indices(control_dim, control_dim).to(device)\n",
    "    \n",
    "    def forward(self, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass through the network to get the policy parameters.\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (phi, Sigma) for the policy\n",
    "        \"\"\"\n",
    "        # Ensure t is a column vector\n",
    "        t = t.view(-1, 1).to(self.device)\n",
    "        \n",
    "        # Forward pass through the hidden layers\n",
    "        hidden = torch.relu(self.time_layer1(t))\n",
    "        hidden = torch.relu(self.time_layer2(hidden))\n",
    "        \n",
    "        # Compute phi matrix\n",
    "        phi_flat = self.phi_output(hidden)\n",
    "        phi = phi_flat.view(-1, self.control_dim, self.state_dim)\n",
    "        \n",
    "        # Compute L matrix for covariance\n",
    "        L_flat = self.L_output(hidden)\n",
    "        \n",
    "        # Create lower triangular matrices L for each batch element\n",
    "        batch_size = t.shape[0]\n",
    "        L = torch.zeros(batch_size, self.control_dim, self.control_dim, \n",
    "                      device=self.device, dtype=t.dtype)\n",
    "        \n",
    "        # Fill the lower triangular part\n",
    "        L[:, self.tril_indices[0], self.tril_indices[1]] = L_flat\n",
    "        \n",
    "        # Compute Sigma = LL^T to ensure positive-definiteness\n",
    "        Sigma = torch.bmm(L, L.transpose(1, 2))\n",
    "        \n",
    "        # Add small constant to diagonal for stability\n",
    "        eye = torch.eye(self.control_dim, device=self.device).unsqueeze(0)\n",
    "        eye = eye.expand(batch_size, -1, -1)\n",
    "        Sigma = Sigma + 1e-3 * eye\n",
    "        \n",
    "        return phi, Sigma\n",
    "    \n",
    "    def get_action_distribution(self, t: torch.Tensor, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get the action distribution parameters for given time and state.\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            x: State tensor (batch x state_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (mean, covariance) for the control distribution\n",
    "        \"\"\"\n",
    "        # Get policy parameters\n",
    "        phi, Sigma = self.forward(t)\n",
    "        \n",
    "        # Compute mean = -phi @ x for each batch element\n",
    "        batch_size = x.shape[0]\n",
    "        means = torch.zeros(batch_size, self.control_dim, device=self.device, dtype=x.dtype)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            means[i] = -torch.mv(phi[i], x[i])\n",
    "        \n",
    "        return means, Sigma\n",
    "    \n",
    "    def sample_action(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample actions from the policy distribution.\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            x: State tensor (batch x state_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Sampled actions (batch x control_dim)\n",
    "        \"\"\"\n",
    "        # Get distribution parameters\n",
    "        means, covariances = self.get_action_distribution(t, x)\n",
    "        \n",
    "        # Sample from multivariate normal\n",
    "        batch_size = means.shape[0]\n",
    "        samples = torch.zeros_like(means)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Compute Cholesky decomposition\n",
    "            L = torch.linalg.cholesky(covariances[i])\n",
    "            \n",
    "            # Generate standard normal samples\n",
    "            z = torch.randn(self.control_dim, device=self.device, dtype=means.dtype)\n",
    "            \n",
    "            # Transform to multivariate normal\n",
    "            samples[i] = means[i] + torch.mv(L, z)\n",
    "        \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorAlgorithm:\n",
    "    def __init__(self, soft_lqr: SoftLQR, policy_network: PolicyNetwork, \n",
    "                 learning_rate: float = 1e-4, device: str = \"cpu\"):\n",
    "        \"\"\"\n",
    "        Actor algorithm to learn the optimal control policy using the optimal value function.\n",
    "        \n",
    "        Args:\n",
    "            soft_lqr: Soft LQR instance with the optimal value function\n",
    "            policy_network: Neural network to approximate the policy\n",
    "            learning_rate: Learning rate for the optimizer\n",
    "            device: Device to run the algorithm on\n",
    "        \"\"\"\n",
    "        self.soft_lqr = soft_lqr\n",
    "        self.policy_network = policy_network\n",
    "        self.device = device\n",
    "        \n",
    "        # Move soft_lqr matrices to the device if needed\n",
    "        self.soft_lqr.H = self.soft_lqr.H.to(device)\n",
    "        self.soft_lqr.M = self.soft_lqr.M.to(device)\n",
    "        self.soft_lqr.sigma = self.soft_lqr.sigma.to(device)\n",
    "        self.soft_lqr.C = self.soft_lqr.C.to(device)\n",
    "        self.soft_lqr.D = self.soft_lqr.D.to(device)\n",
    "        self.soft_lqr.R = self.soft_lqr.R.to(device)\n",
    "        self.soft_lqr.D_inv = self.soft_lqr.D_inv.to(device)\n",
    "        self.soft_lqr.sigma_inv = self.soft_lqr.sigma_inv.to(device)\n",
    "        self.soft_lqr.sigma_term = self.soft_lqr.sigma_term.to(device)\n",
    "        if hasattr(self.soft_lqr, 'S_grid') and self.soft_lqr.S_grid is not None:\n",
    "            self.soft_lqr.S_grid = self.soft_lqr.S_grid.to(device)\n",
    "        if hasattr(self.soft_lqr, 'int_term_grid') and self.soft_lqr.int_term_grid is not None:\n",
    "            self.soft_lqr.int_term_grid = self.soft_lqr.int_term_grid.to(device)\n",
    "        \n",
    "        # Setup optimizer\n",
    "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def compute_value_gradient(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the gradient of the value function with respect to the state.\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            x: State tensor (batch x state_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Gradient of value function (batch x state_dim)\n",
    "        \"\"\"\n",
    "        # Get S matrices at the specified times\n",
    "        S_matrices = self.soft_lqr.get_S_at_time(t)\n",
    "        \n",
    "        # Compute gradient: ∇_x v(t,x) = 2 * S(t) * x\n",
    "        batch_size = x.shape[0]\n",
    "        gradients = torch.zeros_like(x)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            gradients[i] = 2 * torch.mv(S_matrices[i], x[i])\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def simulate_trajectory(self, initial_states: torch.Tensor, num_steps: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Simulate trajectory using the current policy.\n",
    "        \n",
    "        Args:\n",
    "            initial_states: Initial states tensor (batch x state_dim)\n",
    "            num_steps: Number of time steps for simulation\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (states, actions, time_points)\n",
    "        \"\"\"\n",
    "        batch_size = initial_states.shape[0]\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Generate time grid\n",
    "        time_grid = torch.linspace(0, self.soft_lqr.T, num_steps + 1, \n",
    "                                  device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Initialize states and actions\n",
    "        states = torch.zeros((batch_size, num_steps + 1, self.policy_network.state_dim), \n",
    "                           device=self.device, dtype=torch.float64)\n",
    "        states[:, 0, :] = initial_states\n",
    "        \n",
    "        actions = torch.zeros((batch_size, num_steps, self.policy_network.control_dim), \n",
    "                            device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Generate Brownian increments\n",
    "        dW = torch.randn((batch_size, num_steps, self.soft_lqr.sigma.shape[1]), \n",
    "                        device=self.device, dtype=torch.float64) * np.sqrt(dt)\n",
    "        \n",
    "        # Simulate trajectory using explicit scheme\n",
    "        for n in range(num_steps):\n",
    "            # Current time and states\n",
    "            t_n = time_grid[n]\n",
    "            x_n = states[:, n, :]\n",
    "            \n",
    "            # Sample actions from policy\n",
    "            t_batch = t_n.repeat(batch_size)\n",
    "            actions[:, n, :] = self.policy_network.sample_action(t_batch, x_n)\n",
    "            \n",
    "            # Update states\n",
    "            for i in range(batch_size):\n",
    "                # Compute drift term: HX + Ma\n",
    "                drift = self.soft_lqr.H @ states[i, n, :] + self.soft_lqr.M @ actions[i, n, :]\n",
    "                \n",
    "                # Update state using explicit scheme\n",
    "                states[i, n+1, :] = states[i, n, :] + drift * dt + self.soft_lqr.sigma @ dW[i, n, :]\n",
    "        \n",
    "        return states, actions, time_grid\n",
    "    \n",
    "    def compute_policy_loss(self, states: torch.Tensor, actions: torch.Tensor, \n",
    "                           time_grid: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the loss for policy optimization.\n",
    "        \n",
    "        Args:\n",
    "            states: States tensor (batch x time_steps+1 x state_dim)\n",
    "            actions: Actions tensor (batch x time_steps x control_dim)\n",
    "            time_grid: Time grid tensor (time_steps+1)\n",
    "            \n",
    "        Returns:\n",
    "            Policy loss\n",
    "        \"\"\"\n",
    "        batch_size = states.shape[0]\n",
    "        num_steps = actions.shape[1]\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Initialize loss\n",
    "        total_loss = torch.tensor(0.0, device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Compute loss at each time step\n",
    "        for n in range(num_steps):\n",
    "            # Current time, states, and actions\n",
    "            t_n = time_grid[n]\n",
    "            x_n = states[:, n, :]\n",
    "            a_n = actions[:, n, :]\n",
    "            \n",
    "            # Create batch of times\n",
    "            t_batch = t_n.repeat(batch_size)\n",
    "            \n",
    "            # 1. Compute state cost: x^T C x\n",
    "            state_costs = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "            for i in range(batch_size):\n",
    "                state_costs[i] = x_n[i] @ self.soft_lqr.C @ x_n[i]\n",
    "            \n",
    "            # 2. Compute control cost: a^T D a\n",
    "            control_costs = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "            for i in range(batch_size):\n",
    "                control_costs[i] = a_n[i] @ self.soft_lqr.D @ a_n[i]\n",
    "            \n",
    "            # 3. Compute value function gradient\n",
    "            value_gradients = self.compute_value_gradient(t_batch, x_n)\n",
    "            \n",
    "            # 4. Compute policy distribution parameters\n",
    "            means, covariances = self.policy_network.get_action_distribution(t_batch, x_n)\n",
    "            \n",
    "            # 5. Compute log probability of the actions\n",
    "            log_probs = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "            for i in range(batch_size):\n",
    "                # Log probability of multivariate normal\n",
    "                diff = a_n[i] - means[i]\n",
    "                inv_cov = torch.inverse(covariances[i])\n",
    "                \n",
    "                # Compute quadratic term: (a - μ)^T Σ^(-1) (a - μ)\n",
    "                quad_term = diff @ inv_cov @ diff\n",
    "                \n",
    "                # Compute log determinant term: log(det(2π Σ))\n",
    "                log_det_term = torch.log(torch.det(2 * np.pi * covariances[i]))\n",
    "                \n",
    "                # Log probability\n",
    "                log_probs[i] = -0.5 * (quad_term + log_det_term)\n",
    "            \n",
    "            # 6. Compute entropy regularization term\n",
    "            entropy_terms = self.soft_lqr.tau * (-log_probs)\n",
    "            \n",
    "            # 7. Compute value function drift term for each state-action pair\n",
    "            drift_terms = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "            for i in range(batch_size):\n",
    "                # Compute drift: Hx + Ma\n",
    "                drift = self.soft_lqr.H @ x_n[i] + self.soft_lqr.M @ a_n[i]\n",
    "                \n",
    "                # Compute drift term: ∇_x v(t,x)^T (Hx + Ma)\n",
    "                drift_terms[i] = value_gradients[i] @ drift\n",
    "            \n",
    "            # 8. Compute total loss for this time step\n",
    "            step_loss = (state_costs + control_costs + entropy_terms + drift_terms).mean()\n",
    "            \n",
    "            # 9. Accumulate loss with time step\n",
    "            total_loss = total_loss + step_loss * dt\n",
    "        \n",
    "        # Terminal cost is not included in the policy loss since it doesn't depend on the policy\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def train_step(self, initial_states_dist, num_states: int, num_steps: int) -> float:\n",
    "        \"\"\"\n",
    "        Perform one training step of the actor algorithm.\n",
    "        \n",
    "        Args:\n",
    "            initial_states_dist: Distribution to sample initial states from\n",
    "            num_states: Number of initial states to sample\n",
    "            num_steps: Number of time steps for simulation\n",
    "            \n",
    "        Returns:\n",
    "            Loss value for this step\n",
    "        \"\"\"\n",
    "        # Sample initial states\n",
    "        initial_states = initial_states_dist.sample((num_states,)).to(self.device)\n",
    "        \n",
    "        # Simulate trajectory using current policy\n",
    "        states, actions, time_grid = self.simulate_trajectory(initial_states, num_steps)\n",
    "        \n",
    "        # Compute policy loss\n",
    "        loss = self.compute_policy_loss(states, actions, time_grid)\n",
    "        \n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def train(self, epochs: int, batch_size: int, num_steps: int, \n",
    "             initial_states_dist, eval_freq: int = 10, verbose: bool = True) -> List[float]:\n",
    "        \"\"\"\n",
    "        Train the actor algorithm for multiple epochs.\n",
    "        \n",
    "        Args:\n",
    "            epochs: Number of training epochs\n",
    "            batch_size: Number of initial states per batch\n",
    "            num_steps: Number of time steps for simulation\n",
    "            initial_states_dist: Distribution to sample initial states from\n",
    "            eval_freq: Frequency of evaluation\n",
    "            verbose: Whether to print progress\n",
    "            \n",
    "        Returns:\n",
    "            List of loss values during training\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Perform one training step\n",
    "            loss = self.train_step(initial_states_dist, batch_size, num_steps)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch+1) % eval_freq == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.6f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def evaluate_policy(self, test_states: torch.Tensor, plot: bool = True) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the learned policy against the optimal policy from soft LQR.\n",
    "        \n",
    "        Args:\n",
    "            test_states: Test states tensor (batch x state_dim)\n",
    "            plot: Whether to plot the results\n",
    "            \n",
    "        Returns:\n",
    "            Mean error between learned and optimal policies\n",
    "        \"\"\"\n",
    "        # Set evaluation mode\n",
    "        self.policy_network.eval()\n",
    "        \n",
    "        batch_size = test_states.shape[0]\n",
    "        t0 = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Get parameters from learned policy\n",
    "        learned_means, learned_covs = self.policy_network.get_action_distribution(t0, test_states)\n",
    "        \n",
    "        # Get parameters from optimal policy\n",
    "        optimal_means, optimal_cov = self.soft_lqr.optimal_control_distribution(t0, test_states)\n",
    "        \n",
    "        # Compute errors in means\n",
    "        mean_errors = torch.norm(learned_means - optimal_means, dim=1)\n",
    "        mean_error_avg = mean_errors.mean().item()\n",
    "        \n",
    "        # Compute errors in covariances (Frobenius norm)\n",
    "        cov_errors = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "        for i in range(batch_size):\n",
    "            diff = learned_covs[i] - optimal_cov\n",
    "            cov_errors[i] = torch.norm(diff, p='fro')\n",
    "        cov_error_avg = cov_errors.mean().item()\n",
    "        \n",
    "        print(f\"Evaluation results:\")\n",
    "        print(f\"  Mean error in policy means: {mean_error_avg:.6f}\")\n",
    "        print(f\"  Mean error in policy covariances: {cov_error_avg:.6f}\")\n",
    "        \n",
    "        if plot:\n",
    "            # Convert to numpy for plotting\n",
    "            test_states_np = test_states.cpu().numpy()\n",
    "            learned_means_np = learned_means.detach().cpu().numpy()\n",
    "            optimal_means_np = optimal_means.detach().cpu().numpy()\n",
    "            mean_errors_np = mean_errors.detach().cpu().numpy()\n",
    "            \n",
    "            # Create scatter plot of errors vs state norm\n",
    "            state_norms = np.linalg.norm(test_states_np, axis=1)\n",
    "            \n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            # Plot mean errors vs state norm\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.scatter(state_norms, mean_errors_np, alpha=0.7)\n",
    "            plt.title('Policy Mean Errors vs State Norm')\n",
    "            plt.xlabel('State Norm')\n",
    "            plt.ylabel('Mean Error')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            # Plot learned vs optimal means for first control dimension\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.scatter(optimal_means_np[:, 0], learned_means_np[:, 0], alpha=0.7)\n",
    "            min_val = min(optimal_means_np[:, 0].min(), learned_means_np[:, 0].min())\n",
    "            max_val = max(optimal_means_np[:, 0].max(), learned_means_np[:, 0].max())\n",
    "            plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "            plt.title('Learned vs Optimal Policy Means (First Control Dimension)')\n",
    "            plt.xlabel('Optimal Mean')\n",
    "            plt.ylabel('Learned Mean')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # If state dimension is 2, visualize the policy in state space\n",
    "            if test_states.shape[1] == 2 and isinstance(test_states_np, np.ndarray):\n",
    "                # Check if states form a grid\n",
    "                unique_x1 = np.unique(test_states_np[:, 0])\n",
    "                unique_x2 = np.unique(test_states_np[:, 1])\n",
    "                \n",
    "                if len(unique_x1) * len(unique_x2) == test_states_np.shape[0]:\n",
    "                    # Reshape for grid\n",
    "                    X1, X2 = np.meshgrid(unique_x1, unique_x2)\n",
    "                    \n",
    "                    # Reshape means for quiver plot\n",
    "                    optimal_u = optimal_means_np[:, 0].reshape(len(unique_x2), len(unique_x1))\n",
    "                    optimal_v = optimal_means_np[:, 1].reshape(len(unique_x2), len(unique_x1))\n",
    "                    \n",
    "                    learned_u = learned_means_np[:, 0].reshape(len(unique_x2), len(unique_x1))\n",
    "                    learned_v = learned_means_np[:, 1].reshape(len(unique_x2), len(unique_x1))\n",
    "                    \n",
    "                    plt.figure(figsize=(12, 5))\n",
    "                    \n",
    "                    # Plot optimal policy\n",
    "                    plt.subplot(1, 2, 1)\n",
    "                    plt.quiver(X1, X2, optimal_u, optimal_v)\n",
    "                    plt.title('Optimal Policy')\n",
    "                    plt.xlabel('x1')\n",
    "                    plt.ylabel('x2')\n",
    "                    plt.grid(True)\n",
    "                    \n",
    "                    # Plot learned policy\n",
    "                    plt.subplot(1, 2, 2)\n",
    "                    plt.quiver(X1, X2, learned_u, learned_v)\n",
    "                    plt.title('Learned Policy')\n",
    "                    plt.xlabel('x1')\n",
    "                    plt.ylabel('x2')\n",
    "                    plt.grid(True)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "        \n",
    "        # Switch back to training mode\n",
    "        self.policy_network.train()\n",
    "        \n",
    "        return mean_error_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUActorAlgorithm(ActorAlgorithm):\n",
    "    def __init__(self, soft_lqr: SoftLQR, policy_network: PolicyNetwork, \n",
    "                 learning_rate: float = 1e-4, device: str = \"cuda\"):\n",
    "        \"\"\"\n",
    "        GPU-optimized actor algorithm for the soft LQR problem.\n",
    "        \n",
    "        Args:\n",
    "            soft_lqr: Soft LQR instance with the optimal value function\n",
    "            policy_network: Neural network to approximate the policy\n",
    "            learning_rate: Learning rate for the optimizer\n",
    "            device: Device to run the algorithm on (default: \"cuda\")\n",
    "        \"\"\"\n",
    "        super().__init__(soft_lqr, policy_network, learning_rate, device)\n",
    "        \n",
    "        # Create gradient scaler for mixed precision training\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "        # Precompute Cholesky for optimal covariance if it's constant\n",
    "        self.optimal_cov = self.soft_lqr.tau * self.soft_lqr.sigma_term\n",
    "        self.optimal_L = torch.linalg.cholesky(self.optimal_cov)\n",
    "        \n",
    "        # Create optimizer with weight decay for regularization\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.policy_network.parameters(), \n",
    "            lr=learning_rate,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "        \n",
    "        # Create learning rate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, 'min', factor=0.5, patience=20, verbose=True\n",
    "        )\n",
    "    \n",
    "    def compute_value_gradient(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        GPU-optimized computation of value function gradient.\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            x: State tensor (batch x state_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Gradient of value function (batch x state_dim)\n",
    "        \"\"\"\n",
    "        # Get S matrices at the specified times\n",
    "        S_matrices = self.soft_lqr.get_S_at_time(t)\n",
    "        \n",
    "        # Vectorized computation: ∇_x v(t,x) = 2 * S(t) * x\n",
    "        # Reshape S_matrices to batch x state_dim x state_dim\n",
    "        S_reshaped = S_matrices.view(-1, self.policy_network.state_dim, self.policy_network.state_dim)\n",
    "        \n",
    "        # Compute gradient using batched matrix-vector multiplication\n",
    "        x_unsqueezed = x.unsqueeze(2)  # batch x state_dim x 1\n",
    "        gradients = 2 * torch.bmm(S_reshaped, x_unsqueezed).squeeze(2)  # batch x state_dim\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def simulate_trajectory(self, initial_states: torch.Tensor, num_steps: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        GPU-optimized trajectory simulation using vectorized operations.\n",
    "        \n",
    "        Args:\n",
    "            initial_states: Initial states tensor (batch x state_dim)\n",
    "            num_steps: Number of time steps for simulation\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (states, actions, time_points)\n",
    "        \"\"\"\n",
    "        batch_size = initial_states.shape[0]\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Generate time grid\n",
    "        time_grid = torch.linspace(0, self.soft_lqr.T, num_steps + 1, \n",
    "                                  device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Initialize states and actions\n",
    "        states = torch.zeros((batch_size, num_steps + 1, self.policy_network.state_dim), \n",
    "                           device=self.device, dtype=torch.float64)\n",
    "        states[:, 0, :] = initial_states\n",
    "        \n",
    "        actions = torch.zeros((batch_size, num_steps, self.policy_network.control_dim), \n",
    "                            device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Generate all Brownian increments at once\n",
    "        dW = torch.randn((batch_size, num_steps, self.soft_lqr.sigma.shape[1]), \n",
    "                        device=self.device, dtype=torch.float64) * np.sqrt(dt)\n",
    "        \n",
    "        # Generate all standard normal samples for action sampling at once\n",
    "        z_samples = torch.randn(\n",
    "            (batch_size, num_steps, self.policy_network.control_dim), \n",
    "            device=self.device, dtype=torch.float64\n",
    "        )\n",
    "        \n",
    "        # Simulate trajectory using explicit scheme with vectorized operations\n",
    "        for n in range(num_steps):\n",
    "            # Current time and states\n",
    "            t_n = time_grid[n]\n",
    "            x_n = states[:, n, :]\n",
    "            \n",
    "            # Create batch of times\n",
    "            t_batch = torch.full((batch_size,), t_n, device=self.device, dtype=torch.float64)\n",
    "            \n",
    "            # Get policy distribution parameters\n",
    "            means, covariances = self.policy_network.get_action_distribution(t_batch, x_n)\n",
    "            \n",
    "            # Sample actions using pre-computed noise\n",
    "            # For each batch element, compute Cholesky and transform\n",
    "            for i in range(batch_size):\n",
    "                L = torch.linalg.cholesky(covariances[i])\n",
    "                actions[i, n, :] = means[i] + torch.mv(L, z_samples[i, n, :])\n",
    "            \n",
    "            # Compute all drifts at once using broadcasting\n",
    "            # Reshape for batch matrix multiplication\n",
    "            H_expanded = self.soft_lqr.H.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            M_expanded = self.soft_lqr.M.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            \n",
    "            # Compute Hx for all batch elements\n",
    "            x_unsqueezed = x_n.unsqueeze(2)  # batch x state_dim x 1\n",
    "            Hx = torch.bmm(H_expanded, x_unsqueezed).squeeze(2)  # batch x state_dim\n",
    "            \n",
    "            # Compute Ma for all batch elements\n",
    "            a_unsqueezed = actions[:, n, :].unsqueeze(2)  # batch x control_dim x 1\n",
    "            Ma = torch.bmm(M_expanded, a_unsqueezed).squeeze(2)  # batch x state_dim\n",
    "            \n",
    "            # Compute drift = Hx + Ma\n",
    "            drift = Hx + Ma\n",
    "            \n",
    "            # Update states using vectorized operations\n",
    "            states[:, n+1, :] = x_n + drift * dt + torch.bmm(\n",
    "                dW[:, n, :].unsqueeze(1), \n",
    "                self.soft_lqr.sigma.t().unsqueeze(0)\n",
    "            ).squeeze(1)\n",
    "        \n",
    "        return states, actions, time_grid\n",
    "    \n",
    "    def compute_policy_loss(self, states: torch.Tensor, actions: torch.Tensor, \n",
    "                           time_grid: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        GPU-optimized computation of policy loss using vectorized operations.\n",
    "        \n",
    "        Args:\n",
    "            states: States tensor (batch x time_steps+1 x state_dim)\n",
    "            actions: Actions tensor (batch x time_steps x control_dim)\n",
    "            time_grid: Time grid tensor (time_steps+1)\n",
    "            \n",
    "        Returns:\n",
    "            Policy loss\n",
    "        \"\"\"\n",
    "        batch_size = states.shape[0]\n",
    "        num_steps = actions.shape[1]\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Initialize loss\n",
    "        total_loss = torch.tensor(0.0, device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Prepare constant matrices for vectorized operations\n",
    "        C_expanded = self.soft_lqr.C.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        D_expanded = self.soft_lqr.D.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        H_expanded = self.soft_lqr.H.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        M_expanded = self.soft_lqr.M.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Process all time steps in smaller batches to avoid memory issues\n",
    "        step_batch_size = min(num_steps, 10)  # Process 10 steps at a time\n",
    "        \n",
    "        for batch_start in range(0, num_steps, step_batch_size):\n",
    "            batch_end = min(batch_start + step_batch_size, num_steps)\n",
    "            batch_steps = batch_end - batch_start\n",
    "            \n",
    "            # Get states and actions for this batch of steps\n",
    "            x_batch = states[:, batch_start:batch_end, :]  # batch x batch_steps x state_dim\n",
    "            a_batch = actions[:, batch_start:batch_end, :]  # batch x batch_steps x control_dim\n",
    "            t_batch = time_grid[batch_start:batch_end]  # batch_steps\n",
    "            \n",
    "            # Reshape for vectorized operations\n",
    "            x_flat = x_batch.reshape(-1, self.policy_network.state_dim)  # (batch*batch_steps) x state_dim\n",
    "            a_flat = a_batch.reshape(-1, self.policy_network.control_dim)  # (batch*batch_steps) x control_dim\n",
    "            t_flat = t_batch.repeat(batch_size)  # (batch*batch_steps)\n",
    "            \n",
    "            # 1. Compute state costs: x^T C x - vectorized\n",
    "            x_unsqueezed = x_flat.unsqueeze(2)  # (batch*batch_steps) x state_dim x 1\n",
    "            C_expanded_flat = C_expanded.repeat(batch_steps, 1, 1)  # (batch*batch_steps) x state_dim x state_dim\n",
    "            Cx = torch.bmm(C_expanded_flat, x_unsqueezed)  # (batch*batch_steps) x state_dim x 1\n",
    "            state_costs = torch.bmm(x_unsqueezed.transpose(1, 2), Cx).squeeze()  # (batch*batch_steps)\n",
    "            \n",
    "            # 2. Compute control costs: a^T D a - vectorized\n",
    "            a_unsqueezed = a_flat.unsqueeze(2)  # (batch*batch_steps) x control_dim x 1\n",
    "            D_expanded_flat = D_expanded.repeat(batch_steps, 1, 1)  # (batch*batch_steps) x control_dim x control_dim\n",
    "            Da = torch.bmm(D_expanded_flat, a_unsqueezed)  # (batch*batch_steps) x control_dim x 1\n",
    "            control_costs = torch.bmm(a_unsqueezed.transpose(1, 2), Da).squeeze()  # (batch*batch_steps)\n",
    "            \n",
    "            # 3. Compute value function gradient\n",
    "            value_gradients = self.compute_value_gradient(t_flat, x_flat)  # (batch*batch_steps) x state_dim\n",
    "            \n",
    "            # 4. Compute policy distribution parameters\n",
    "            means, covariances = self.policy_network.get_action_distribution(t_flat, x_flat)\n",
    "            \n",
    "            # 5. Compute log probability of the actions - vectorized\n",
    "            log_probs = torch.zeros(batch_size * batch_steps, device=self.device, dtype=torch.float64)\n",
    "            \n",
    "            for i in range(batch_size * batch_steps):\n",
    "                # Log probability of multivariate normal\n",
    "                diff = a_flat[i] - means[i]\n",
    "                inv_cov = torch.inverse(covariances[i])\n",
    "                \n",
    "                # Compute quadratic term: (a - μ)^T Σ^(-1) (a - μ)\n",
    "                quad_term = diff @ inv_cov @ diff\n",
    "                \n",
    "                # Compute log determinant term: log(det(2π Σ))\n",
    "                log_det_term = torch.log(torch.det(2 * np.pi * covariances[i]))\n",
    "                \n",
    "                # Log probability\n",
    "                log_probs[i] = -0.5 * (quad_term + log_det_term)\n",
    "            \n",
    "            # 6. Compute entropy regularization term\n",
    "            entropy_terms = self.soft_lqr.tau * (-log_probs)\n",
    "            \n",
    "            # 7. Compute value function drift term - vectorized\n",
    "            # Reshape for batch matrix operations\n",
    "            H_expanded_flat = H_expanded.repeat(batch_steps, 1, 1)  # (batch*batch_steps) x state_dim x state_dim\n",
    "            M_expanded_flat = M_expanded.repeat(batch_steps, 1, 1)  # (batch*batch_steps) x state_dim x control_dim\n",
    "            \n",
    "            # Compute Hx\n",
    "            Hx = torch.bmm(H_expanded_flat, x_unsqueezed).squeeze(2)  # (batch*batch_steps) x state_dim\n",
    "            \n",
    "            # Compute Ma\n",
    "            Ma = torch.bmm(M_expanded_flat, a_unsqueezed).squeeze(2)  # (batch*batch_steps) x state_dim\n",
    "            \n",
    "            # Compute drift = Hx + Ma\n",
    "            drift = Hx + Ma  # (batch*batch_steps) x state_dim\n",
    "            \n",
    "            # Compute drift term: ∇_x v(t,x)^T (Hx + Ma)\n",
    "            drift_terms = torch.sum(value_gradients * drift, dim=1)  # (batch*batch_steps)\n",
    "            \n",
    "            # 8. Compute step loss and reshape back to batch x batch_steps\n",
    "            step_costs = state_costs + control_costs + entropy_terms + drift_terms\n",
    "            step_costs = step_costs.reshape(batch_size, batch_steps)\n",
    "            \n",
    "            # 9. Average over batch and accumulate with time step\n",
    "            step_loss = step_costs.mean() * dt * batch_steps\n",
    "            total_loss = total_loss + step_loss\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def train_step(self, initial_states_dist, num_states: int, num_steps: int) -> float:\n",
    "        \"\"\"\n",
    "        GPU-optimized training step with mixed precision.\n",
    "        \n",
    "        Args:\n",
    "            initial_states_dist: Distribution to sample initial states from\n",
    "            num_states: Number of initial states to sample\n",
    "            num_steps: Number of time steps for simulation\n",
    "            \n",
    "        Returns:\n",
    "            Loss value for this step\n",
    "        \"\"\"\n",
    "        # Sample initial states\n",
    "        initial_states = initial_states_dist.sample((num_states,)).to(self.device)\n",
    "        \n",
    "        # Simulate trajectory using current policy (still in float64 for accuracy)\n",
    "        states, actions, time_grid = self.simulate_trajectory(initial_states, num_steps)\n",
    "        \n",
    "        # Use mixed precision for the neural network part\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Cast to float32 for network computation\n",
    "            states_fp32 = states.to(torch.float32)\n",
    "            actions_fp32 = actions.to(torch.float32)\n",
    "            time_grid_fp32 = time_grid.to(torch.float32)\n",
    "            \n",
    "            # Compute policy loss\n",
    "            loss = self.compute_policy_loss(states_fp32, actions_fp32, time_grid_fp32)\n",
    "        \n",
    "        # Backpropagation with gradient scaling\n",
    "        self.optimizer.zero_grad()\n",
    "        self.scaler.scale(loss).backward()\n",
    "        \n",
    "        # Clip gradients to avoid exploding gradients\n",
    "        self.scaler.unscale_(self.optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_network.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update weights and scaler\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def train(self, epochs: int, batch_size: int, num_steps: int, \n",
    "             initial_states_dist, eval_freq: int = 10, verbose: bool = True,\n",
    "             evaluation_states: Optional[torch.Tensor] = None) -> List[float]:\n",
    "        \"\"\"\n",
    "        GPU-optimized training with periodic evaluation.\n",
    "        \n",
    "        Args:\n",
    "            epochs: Number of training epochs\n",
    "            batch_size: Number of initial states per batch\n",
    "            num_steps: Number of time steps for simulation\n",
    "            initial_states_dist: Distribution to sample initial states from\n",
    "            eval_freq: Frequency of evaluation\n",
    "            verbose: Whether to print progress\n",
    "            evaluation_states: Optional tensor of states for periodic evaluation\n",
    "            \n",
    "        Returns:\n",
    "            List of loss values during training\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        eval_errors = []\n",
    "        \n",
    "        # Create event for timing on GPU\n",
    "        if torch.cuda.is_available():\n",
    "            start_event = torch.cuda.Event(enable_timing=True)\n",
    "            end_event = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Measure epoch time for GPU\n",
    "            if torch.cuda.is_available():\n",
    "                start_event.record()\n",
    "            \n",
    "            # Perform one training step\n",
    "            loss = self.train_step(initial_states_dist, batch_size, num_steps)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Update learning rate\n",
    "            self.scheduler.step(loss)\n",
    "            \n",
    "            # Record time and print progress\n",
    "            if torch.cuda.is_available():\n",
    "                end_event.record()\n",
    "                torch.cuda.synchronize()\n",
    "                epoch_time = start_event.elapsed_time(end_event) / 1000.0  # in seconds\n",
    "            else:\n",
    "                epoch_time = 0.0\n",
    "            \n",
    "            # Evaluate periodically\n",
    "            if (epoch+1) % eval_freq == 0:\n",
    "                if evaluation_states is not None:\n",
    "                    with torch.no_grad():\n",
    "                        error = self.evaluate_policy(evaluation_states, plot=(epoch+1) == epochs)\n",
    "                    eval_errors.append(error)\n",
    "                    \n",
    "                    if verbose:\n",
    "                        current_lr = self.optimizer.param_groups[0]['lr']\n",
    "                        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.6f}, Error: {error:.6f}, LR: {current_lr:.6f}, Time: {epoch_time:.3f}s\")\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        current_lr = self.optimizer.param_groups[0]['lr']\n",
    "                        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.6f}, LR: {current_lr:.6f}, Time: {epoch_time:.3f}s\")\n",
    "        \n",
    "        # Plot training curves\n",
    "        if len(eval_errors) > 0:\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.semilogy(losses)\n",
    "            plt.title('Policy Loss During Training')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.semilogy(range(eval_freq-1, epochs, eval_freq), eval_errors)\n",
    "            plt.title('Policy Error During Training')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Mean Error')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_actor():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Problem setup as specified in the exercise\n",
    "    H = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64) * 0.5\n",
    "    M = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64)\n",
    "    sigma = torch.eye(2, dtype=torch.float64) * 0.5\n",
    "    C = torch.tensor([[1.0, 0.1], [0.1, 1.0]], dtype=torch.float64) * 1.0\n",
    "    D = torch.eye(2, dtype=torch.float64)  # D just identity, as specified\n",
    "    R = torch.tensor([[1.0, 0.3], [0.3, 1.0]], dtype=torch.float64) * 10.0\n",
    "    \n",
    "    # Set terminal time and time grid\n",
    "    T = 0.5\n",
    "    grid_size = 1000\n",
    "    time_grid = torch.linspace(0, T, grid_size, dtype=torch.float64)\n",
    "    \n",
    "    # Set entropy regularization parameters\n",
    "    tau = 0.5  # τ = 1/2 as specified\n",
    "    gamma = 1.0  # γ = 1 as specified\n",
    "    \n",
    "    # Create soft LQR instance\n",
    "    soft_lqr = SoftLQR(H, M, sigma, C, D, R, T, time_grid, tau, gamma)\n",
    "    \n",
    "    # Solve Ricatti ODE\n",
    "    print(\"Solving Ricatti ODE...\")\n",
    "    soft_lqr.solve_ricatti()\n",
    "    \n",
    "    # Create policy network\n",
    "    hidden_size = 256  # As specified\n",
    "    policy_network = PolicyNetwork(\n",
    "        hidden_size=hidden_size, \n",
    "        state_dim=2, \n",
    "        control_dim=2, \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create actor algorithm based on device\n",
    "    learning_rate = 1e-4  # As specified\n",
    "    \n",
    "    if device.type == \"cuda\":\n",
    "        print(\"Using GPU-optimized actor algorithm...\")\n",
    "        actor = GPUActorAlgorithm(soft_lqr, policy_network, learning_rate, device)\n",
    "    else:\n",
    "        print(\"Using CPU-based actor algorithm...\")\n",
    "        actor = ActorAlgorithm(soft_lqr, policy_network, learning_rate, device)\n",
    "    \n",
    "    # Define initial state distribution\n",
    "    # Uniform distribution in [-3, 3] x [-3, 3]\n",
    "    class UniformInitialState:\n",
    "        def __init__(self, low=-3.0, high=3.0, dim=2, device=device):\n",
    "            self.low = low\n",
    "            self.high = high\n",
    "            self.dim = dim\n",
    "            self.device = device\n",
    "        \n",
    "        def sample(self, shape):\n",
    "            return torch.empty((*shape, self.dim), dtype=torch.float64, device=self.device).uniform_(self.low, self.high)\n",
    "    \n",
    "    initial_states_dist = UniformInitialState()\n",
    "    \n",
    "    # Create evaluation grid for periodic assessment\n",
    "    x1 = torch.linspace(-3, 3, 11, device=device, dtype=torch.float64)\n",
    "    x2 = torch.linspace(-3, 3, 11, device=device, dtype=torch.float64)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    eval_states = torch.stack([X1.flatten(), X2.flatten()], dim=1)\n",
    "    \n",
    "    # Training parameters\n",
    "    if device.type == \"cuda\":\n",
    "        epochs = 500\n",
    "        batch_size = 128  # Larger for GPU\n",
    "        num_steps = 100  # N = 100 as specified\n",
    "        eval_freq = 25\n",
    "    else:\n",
    "        epochs = 300  # Fewer epochs for CPU\n",
    "        batch_size = 64  # Smaller for CPU\n",
    "        num_steps = 100  # N = 100 as specified\n",
    "        eval_freq = 25\n",
    "    \n",
    "    # Train the actor algorithm\n",
    "    print(\"Training actor algorithm...\")\n",
    "    losses = actor.train(\n",
    "        epochs, batch_size, num_steps, initial_states_dist, \n",
    "        eval_freq, verbose=True, evaluation_states=eval_states\n",
    "    )\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.semilogy(losses)\n",
    "    plt.title('Actor Loss During Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"Final evaluation of learned policy...\")\n",
    "    actor.evaluate_policy(eval_states, plot=True)\n",
    "    \n",
    "    # Visualize policy quiver plot for state space\n",
    "    print(\"Visualizing policies...\")\n",
    "    \n",
    "    # Sample grid of states\n",
    "    x1_dense = torch.linspace(-3, 3, 21, device=device, dtype=torch.float64)\n",
    "    x2_dense = torch.linspace(-3, 3, 21, device=device, dtype=torch.float64)\n",
    "    X1_dense, X2_dense = torch.meshgrid(x1_dense, x2_dense, indexing='ij')\n",
    "    grid_states = torch.stack([X1_dense.flatten(), X2_dense.flatten()], dim=1)\n",
    "    \n",
    "    # Set time to zero\n",
    "    t0 = torch.zeros(grid_states.shape[0], device=device, dtype=torch.float64)\n",
    "    \n",
    "    # Get means from both policies\n",
    "    with torch.no_grad():\n",
    "        learned_means, _ = policy_network.get_action_distribution(t0, grid_states)\n",
    "        optimal_means, _ = soft_lqr.optimal_control_distribution(t0, grid_states)\n",
    "    \n",
    "    # Convert to numpy for plotting\n",
    "    X1_np = X1_dense.cpu().numpy()\n",
    "    X2_np = X2_dense.cpu().numpy()\n",
    "    \n",
    "    learned_u = learned_means.cpu().numpy()[:, 0].reshape(X1_np.shape)\n",
    "    learned_v = learned_means.cpu().numpy()[:, 1].reshape(X1_np.shape)\n",
    "    \n",
    "    optimal_u = optimal_means.cpu().numpy()[:, 0].reshape(X1_np.shape)\n",
    "    optimal_v = optimal_means.cpu().numpy()[:, 1].reshape(X1_np.shape)\n",
    "    \n",
    "    # Create quiver plots\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot optimal policy\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.quiver(X1_np, X2_np, optimal_u, optimal_v)\n",
    "    plt.title('Optimal Policy Control Means')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot learned policy\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.quiver(X1_np, X2_np, learned_u, learned_v)\n",
    "    plt.title('Learned Policy Control Means')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Test policy on trajectory\n",
    "    print(\"Simulating trajectories with learned policy...\")\n",
    "    \n",
    "    # Sample test initial states\n",
    "    test_initial_states = torch.tensor([\n",
    "        [2.0, 2.0],\n",
    "        [2.0, -2.0],\n",
    "        [-2.0, -2.0],\n",
    "        [-2.0, 2.0]\n",
    "    ], device=device, dtype=torch.float64)\n",
    "    \n",
    "    # Simulate trajectories with both policies\n",
    "    num_steps_sim = 200\n",
    "    dt = T / num_steps_sim\n",
    "    \n",
    "    # Generate same noise for both simulations\n",
    "    dW = torch.randn((4, num_steps_sim, sigma.shape[1]), \n",
    "                    device=device, dtype=torch.float64) * np.sqrt(dt)\n",
    "    \n",
    "    # Time grid for simulation\n",
    "    t_sim = torch.linspace(0, T, num_steps_sim + 1, device=device, dtype=torch.float64)\n",
    "    \n",
    "    # Initialize trajectories\n",
    "    opt_traj = torch.zeros((4, num_steps_sim + 1, 2), device=device, dtype=torch.float64)\n",
    "    learned_traj = torch.zeros((4, num_steps_sim + 1, 2), device=device, dtype=torch.float64)\n",
    "    \n",
    "    # Set initial states\n",
    "    opt_traj[:, 0, :] = test_initial_states\n",
    "    learned_traj[:, 0, :] = test_initial_states\n",
    "    \n",
    "    # Simulate trajectories\n",
    "    for n in range(num_steps_sim):\n",
    "        # Current time and states\n",
    "        t_n = t_sim[n]\n",
    "        t_batch = torch.full((4,), t_n, device=device, dtype=torch.float64)\n",
    "        \n",
    "        # Get control from optimal policy\n",
    "        opt_means, opt_cov = soft_lqr.optimal_control_distribution(t_batch, opt_traj[:, n, :])\n",
    "        opt_L = torch.linalg.cholesky(opt_cov)\n",
    "        \n",
    "        # Get control from learned policy\n",
    "        learned_means, learned_covs = policy_network.get_action_distribution(t_batch, learned_traj[:, n, :])\n",
    "        \n",
    "        # Use same noise for both policies\n",
    "        z = torch.randn((4, 2), device=device, dtype=torch.float64)\n",
    "        \n",
    "        # Sample controls\n",
    "        opt_actions = torch.zeros((4, 2), device=device, dtype=torch.float64)\n",
    "        learned_actions = torch.zeros((4, 2), device=device, dtype=torch.float64)\n",
    "        \n",
    "        for i in range(4):\n",
    "            opt_actions[i] = opt_means[i] + torch.mv(opt_L, z[i])\n",
    "            \n",
    "            # For learned policy, need to compute Cholesky for each covariance\n",
    "            learned_L = torch.linalg.cholesky(learned_covs[i])\n",
    "            learned_actions[i] = learned_means[i] + torch.mv(learned_L, z[i])\n",
    "        \n",
    "        # Update states\n",
    "        for i in range(4):\n",
    "            # Optimal trajectory update\n",
    "            opt_drift = H @ opt_traj[i, n, :] + M @ opt_actions[i]\n",
    "            opt_traj[i, n+1, :] = opt_traj[i, n, :] + opt_drift * dt + sigma @ dW[i, n, :]\n",
    "            \n",
    "            # Learned trajectory update\n",
    "            learned_drift = H @ learned_traj[i, n, :] + M @ learned_actions[i]\n",
    "            learned_traj[i, n+1, :] = learned_traj[i, n, :] + learned_drift * dt + sigma @ dW[i, n, :]\n",
    "    \n",
    "    # Convert to numpy for plotting\n",
    "    opt_traj_np = opt_traj.cpu().numpy()\n",
    "    learned_traj_np = learned_traj.cpu().numpy()\n",
    "    \n",
    "    # Plot trajectories\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    for i in range(4):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.plot(opt_traj_np[i, :, 0], opt_traj_np[i, :, 1], 'b-', label='Optimal Policy')\n",
    "        plt.plot(learned_traj_np[i, :, 0], learned_traj_np[i, :, 1], 'r-', label='Learned Policy')\n",
    "        plt.scatter(test_initial_states[i, 0].cpu().numpy(), test_initial_states[i, 1].cpu().numpy(), \n",
    "                   c='g', s=100, marker='o', label='Initial State')\n",
    "        plt.title(f'Trajectory from Initial State {test_initial_states[i].cpu().numpy()}')\n",
    "        plt.xlabel('x1')\n",
    "        plt.ylabel('x2')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_actor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "\n",
    "class ActorCriticAlgorithm:\n",
    "    def __init__(self, soft_lqr: SoftLQR, policy_network: PolicyNetwork, value_network: ValueNetwork,\n",
    "                 actor_lr: float = 1e-4, critic_lr: float = 1e-3, device: str = \"cpu\"):\n",
    "        \"\"\"\n",
    "        Actor-Critic algorithm for the soft LQR problem.\n",
    "        \n",
    "        Args:\n",
    "            soft_lqr: Soft LQR instance (used for system dynamics and cost parameters)\n",
    "            policy_network: Neural network to approximate the policy (actor)\n",
    "            value_network: Neural network to approximate the value function (critic)\n",
    "            actor_lr: Learning rate for the actor optimizer\n",
    "            critic_lr: Learning rate for the critic optimizer\n",
    "            device: Device to run the algorithm on\n",
    "        \"\"\"\n",
    "        self.soft_lqr = soft_lqr\n",
    "        self.policy_network = policy_network\n",
    "        self.value_network = value_network\n",
    "        self.device = device\n",
    "        \n",
    "        # Move soft_lqr matrices to the device\n",
    "        self.soft_lqr.H = self.soft_lqr.H.to(device)\n",
    "        self.soft_lqr.M = self.soft_lqr.M.to(device)\n",
    "        self.soft_lqr.sigma = self.soft_lqr.sigma.to(device)\n",
    "        self.soft_lqr.C = self.soft_lqr.C.to(device)\n",
    "        self.soft_lqr.D = self.soft_lqr.D.to(device)\n",
    "        self.soft_lqr.R = self.soft_lqr.R.to(device)\n",
    "        self.soft_lqr.D_inv = self.soft_lqr.D_inv.to(device)\n",
    "        if hasattr(self.soft_lqr, 'sigma_inv'):\n",
    "            self.soft_lqr.sigma_inv = self.soft_lqr.sigma_inv.to(device)\n",
    "        if hasattr(self.soft_lqr, 'sigma_term'):\n",
    "            self.soft_lqr.sigma_term = self.soft_lqr.sigma_term.to(device)\n",
    "        \n",
    "        # Setup optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.policy_network.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.value_network.parameters(), lr=critic_lr)\n",
    "    \n",
    "    def compute_value_gradient(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the gradient of the value function with respect to the state.\n",
    "        Uses the critic network to differentiate through automatic differentiation.\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            x: State tensor (batch x state_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Gradient of value function (batch x state_dim)\n",
    "        \"\"\"\n",
    "        # Ensure x requires gradient\n",
    "        x_with_grad = x.detach().clone().requires_grad_(True)\n",
    "        \n",
    "        # Forward pass through the value network\n",
    "        values = self.value_network.value_function(t, x_with_grad)\n",
    "        \n",
    "        # Initialize gradients\n",
    "        gradients = torch.zeros_like(x)\n",
    "        \n",
    "        # Compute gradient for each element in the batch\n",
    "        for i in range(values.shape[0]):\n",
    "            # Zero existing gradients\n",
    "            if x_with_grad.grad is not None:\n",
    "                x_with_grad.grad.zero_()\n",
    "            \n",
    "            # Backpropagate for this element\n",
    "            values[i].backward(retain_graph=(i < values.shape[0] - 1))\n",
    "            \n",
    "            # Store gradient\n",
    "            if x_with_grad.grad is not None:\n",
    "                gradients[i] = x_with_grad.grad[i].clone()\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def simulate_trajectory(self, initial_states: torch.Tensor, \n",
    "                           num_steps: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Simulate trajectory using the current policy.\n",
    "        \n",
    "        Args:\n",
    "            initial_states: Initial states tensor (batch x state_dim)\n",
    "            num_steps: Number of time steps for simulation\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (states, actions, time_points)\n",
    "        \"\"\"\n",
    "        batch_size = initial_states.shape[0]\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Generate time grid\n",
    "        time_grid = torch.linspace(0, self.soft_lqr.T, num_steps + 1, \n",
    "                                  device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Initialize states and actions\n",
    "        states = torch.zeros((batch_size, num_steps + 1, self.policy_network.state_dim), \n",
    "                           device=self.device, dtype=torch.float64)\n",
    "        states[:, 0, :] = initial_states\n",
    "        \n",
    "        actions = torch.zeros((batch_size, num_steps, self.policy_network.control_dim), \n",
    "                            device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Generate Brownian increments\n",
    "        dW = torch.randn((batch_size, num_steps, self.soft_lqr.sigma.shape[1]), \n",
    "                        device=self.device, dtype=torch.float64) * np.sqrt(dt)\n",
    "        \n",
    "        # Simulate trajectory using explicit scheme\n",
    "        for n in range(num_steps):\n",
    "            # Current time and states\n",
    "            t_n = time_grid[n]\n",
    "            x_n = states[:, n, :]\n",
    "            \n",
    "            # Sample actions from policy\n",
    "            t_batch = t_n.repeat(batch_size)\n",
    "            actions[:, n, :] = self.policy_network.sample_action(t_batch, x_n)\n",
    "            \n",
    "            # Update states\n",
    "            for i in range(batch_size):\n",
    "                # Compute drift term: HX + Ma\n",
    "                drift = self.soft_lqr.H @ states[i, n, :] + self.soft_lqr.M @ actions[i, n, :]\n",
    "                \n",
    "                # Update state using explicit scheme\n",
    "                states[i, n+1, :] = states[i, n, :] + drift * dt + self.soft_lqr.sigma @ dW[i, n, :]\n",
    "        \n",
    "        return states, actions, time_grid\n",
    "    \n",
    "    def compute_critic_targets(self, states: torch.Tensor, actions: torch.Tensor, \n",
    "                              time_grid: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute target values for critic training using Monte Carlo estimation.\n",
    "        \n",
    "        Args:\n",
    "            states: States tensor (batch x time_steps+1 x state_dim)\n",
    "            actions: Actions tensor (batch x time_steps x control_dim)\n",
    "            time_grid: Time grid tensor (time_steps+1)\n",
    "            \n",
    "        Returns:\n",
    "            Target values for each state and time step (batch x time_steps+1)\n",
    "        \"\"\"\n",
    "        batch_size = states.shape[0]\n",
    "        num_steps = time_grid.shape[0] - 1\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Initialize target values\n",
    "        targets = torch.zeros((batch_size, num_steps + 1), device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Compute costs at each time step\n",
    "        for n in range(num_steps):\n",
    "            # Current states and actions\n",
    "            x_n = states[:, n, :]\n",
    "            a_n = actions[:, n, :]\n",
    "            \n",
    "            # Get distribution parameters for entropy calculation\n",
    "            t_n = time_grid[n]\n",
    "            t_batch = t_n.repeat(batch_size)\n",
    "            means, covariances = self.policy_network.get_action_distribution(t_batch, x_n)\n",
    "            \n",
    "            # Compute costs for each batch element\n",
    "            for i in range(batch_size):\n",
    "                # State cost: x^T C x\n",
    "                state_cost = x_n[i] @ self.soft_lqr.C @ x_n[i]\n",
    "                \n",
    "                # Control cost: a^T D a\n",
    "                control_cost = a_n[i] @ self.soft_lqr.D @ a_n[i]\n",
    "                \n",
    "                # Entropy term\n",
    "                diff = a_n[i] - means[i]\n",
    "                inv_cov = torch.inverse(covariances[i])\n",
    "                quad_term = diff @ inv_cov @ diff\n",
    "                log_det_term = torch.log(torch.det(2 * np.pi * covariances[i]))\n",
    "                log_prob = -0.5 * (quad_term + log_det_term)\n",
    "                entropy_term = self.soft_lqr.tau * (-log_prob)\n",
    "                \n",
    "                # Add costs for this time step\n",
    "                targets[i, n+1:] += (state_cost + control_cost + entropy_term) * dt\n",
    "        \n",
    "        # Add terminal costs\n",
    "        x_T = states[:, -1, :]\n",
    "        for i in range(batch_size):\n",
    "            terminal_cost = x_T[i] @ self.soft_lqr.R @ x_T[i]\n",
    "            targets[i, -1] += terminal_cost\n",
    "        \n",
    "        return targets\n",
    "    \n",
    "    def compute_actor_loss(self, states: torch.Tensor, actions: torch.Tensor, \n",
    "                          time_grid: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the loss for policy optimization using the critic network.\n",
    "        \n",
    "        Args:\n",
    "            states: States tensor (batch x time_steps+1 x state_dim)\n",
    "            actions: Actions tensor (batch x time_steps x control_dim)\n",
    "            time_grid: Time grid tensor (time_steps+1)\n",
    "            \n",
    "        Returns:\n",
    "            Policy loss\n",
    "        \"\"\"\n",
    "        batch_size = states.shape[0]\n",
    "        num_steps = actions.shape[1]\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Initialize loss\n",
    "        total_loss = torch.tensor(0.0, device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Compute loss at each time step\n",
    "        for n in range(num_steps):\n",
    "            # Current time, states, and actions\n",
    "            t_n = time_grid[n]\n",
    "            x_n = states[:, n, :]\n",
    "            a_n = actions[:, n, :]\n",
    "            \n",
    "            # Create batch of times\n",
    "            t_batch = t_n.repeat(batch_size)\n",
    "            \n",
    "            # 1. Compute state cost: x^T C x\n",
    "            state_costs = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "            for i in range(batch_size):\n",
    "                state_costs[i] = x_n[i] @ self.soft_lqr.C @ x_n[i]\n",
    "            \n",
    "            # 2. Compute control cost: a^T D a\n",
    "            control_costs = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "            for i in range(batch_size):\n",
    "                control_costs[i] = a_n[i] @ self.soft_lqr.D @ a_n[i]\n",
    "            \n",
    "            # 3. Compute value function gradient using critic network\n",
    "            value_gradients = self.compute_value_gradient(t_batch, x_n)\n",
    "            \n",
    "            # 4. Compute policy distribution parameters\n",
    "            means, covariances = self.policy_network.get_action_distribution(t_batch, x_n)\n",
    "            \n",
    "            # 5. Compute log probability of the actions\n",
    "            log_probs = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "            for i in range(batch_size):\n",
    "                # Log probability of multivariate normal\n",
    "                diff = a_n[i] - means[i]\n",
    "                inv_cov = torch.inverse(covariances[i])\n",
    "                \n",
    "                # Compute quadratic term: (a - μ)^T Σ^(-1) (a - μ)\n",
    "                quad_term = diff @ inv_cov @ diff\n",
    "                \n",
    "                # Compute log determinant term: log(det(2π Σ))\n",
    "                log_det_term = torch.log(torch.det(2 * np.pi * covariances[i]))\n",
    "                \n",
    "                # Log probability\n",
    "                log_probs[i] = -0.5 * (quad_term + log_det_term)\n",
    "            \n",
    "            # 6. Compute entropy regularization term\n",
    "            entropy_terms = self.soft_lqr.tau * (-log_probs)\n",
    "            \n",
    "            # 7. Compute value function drift term for each state-action pair\n",
    "            drift_terms = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "            for i in range(batch_size):\n",
    "                # Compute drift: Hx + Ma\n",
    "                drift = self.soft_lqr.H @ x_n[i] + self.soft_lqr.M @ a_n[i]\n",
    "                \n",
    "                # Compute drift term: ∇_x v(t,x)^T (Hx + Ma)\n",
    "                drift_terms[i] = value_gradients[i] @ drift\n",
    "            \n",
    "            # 8. Compute total loss for this time step\n",
    "            step_loss = (state_costs + control_costs + entropy_terms + drift_terms).mean()\n",
    "            \n",
    "            # 9. Accumulate loss with time step\n",
    "            total_loss = total_loss + step_loss * dt\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def compute_critic_loss(self, states: torch.Tensor, targets: torch.Tensor, \n",
    "                           time_grid: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the loss for value function optimization.\n",
    "        \n",
    "        Args:\n",
    "            states: States tensor (batch x time_steps+1 x state_dim)\n",
    "            targets: Target values (batch x time_steps+1)\n",
    "            time_grid: Time grid tensor (time_steps+1)\n",
    "            \n",
    "        Returns:\n",
    "            Critic loss\n",
    "        \"\"\"\n",
    "        batch_size = states.shape[0]\n",
    "        num_steps = states.shape[1] - 1\n",
    "        \n",
    "        # Initialize loss\n",
    "        total_loss = torch.tensor(0.0, device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Compute loss at each time step\n",
    "        for n in range(num_steps + 1):\n",
    "            # Current time and states\n",
    "            t_n = time_grid[n]\n",
    "            x_n = states[:, n, :]\n",
    "            \n",
    "            # Create batch of times\n",
    "            t_batch = t_n.repeat(batch_size)\n",
    "            \n",
    "            # Predict values using critic network\n",
    "            predicted = self.value_network.value_function(t_batch, x_n)\n",
    "            \n",
    "            # Target values at this time step\n",
    "            target = targets[:, n]\n",
    "            \n",
    "            # Compute MSE loss\n",
    "            step_loss = ((predicted - target) ** 2).mean()\n",
    "            \n",
    "            # Accumulate loss\n",
    "            total_loss = total_loss + step_loss\n",
    "        \n",
    "        # Average loss over all time steps\n",
    "        total_loss = total_loss / (num_steps + 1)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def train_step(self, initial_states_dist, num_states: int, num_steps: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Perform one training step of the actor-critic algorithm.\n",
    "        \n",
    "        Args:\n",
    "            initial_states_dist: Distribution to sample initial states from\n",
    "            num_states: Number of initial states to sample\n",
    "            num_steps: Number of time steps for simulation\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of loss values for this step\n",
    "        \"\"\"\n",
    "        # Sample initial states\n",
    "        initial_states = initial_states_dist.sample((num_states,)).to(self.device)\n",
    "        \n",
    "        # Simulate trajectory using current policy\n",
    "        states, actions, time_grid = self.simulate_trajectory(initial_states, num_steps)\n",
    "        \n",
    "        # Compute critic targets\n",
    "        targets = self.compute_critic_targets(states, actions, time_grid)\n",
    "        \n",
    "        # Update critic\n",
    "        critic_loss = self.compute_critic_loss(states, targets, time_grid)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Simulate new trajectory with updated critic\n",
    "        states, actions, time_grid = self.simulate_trajectory(initial_states, num_steps)\n",
    "        \n",
    "        # Update actor\n",
    "        actor_loss = self.compute_actor_loss(states, actions, time_grid)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            'critic_loss': critic_loss.item(),\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'total_loss': critic_loss.item() + actor_loss.item()\n",
    "        }\n",
    "    \n",
    "    def train(self, epochs: int, batch_size: int, num_steps: int, \n",
    "             initial_states_dist, eval_freq: int = 10, verbose: bool = True,\n",
    "             evaluation_states: Optional[torch.Tensor] = None) -> Dict[str, List[float]]:\n",
    "        \"\"\"\n",
    "        Train the actor-critic algorithm for multiple epochs.\n",
    "        \n",
    "        Args:\n",
    "            epochs: Number of training epochs\n",
    "            batch_size: Number of initial states per batch\n",
    "            num_steps: Number of time steps for simulation\n",
    "            initial_states_dist: Distribution to sample initial states from\n",
    "            eval_freq: Frequency of evaluation\n",
    "            verbose: Whether to print progress\n",
    "            evaluation_states: Optional tensor of states for periodic evaluation\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of lists of loss values during training\n",
    "        \"\"\"\n",
    "        losses = {\n",
    "            'critic_loss': [],\n",
    "            'actor_loss': [],\n",
    "            'total_loss': []\n",
    "        }\n",
    "        eval_errors = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Perform one training step\n",
    "            step_losses = self.train_step(initial_states_dist, batch_size, num_steps)\n",
    "            \n",
    "            # Record losses\n",
    "            for key in losses.keys():\n",
    "                losses[key].append(step_losses[key])\n",
    "            \n",
    "            # Evaluate periodically\n",
    "            if (epoch+1) % eval_freq == 0 and evaluation_states is not None:\n",
    "                with torch.no_grad():\n",
    "                    if hasattr(self.soft_lqr, 'optimal_control_distribution'):\n",
    "                        # Compute error between learned and optimal policy\n",
    "                        t0 = torch.zeros(evaluation_states.shape[0], device=self.device, dtype=torch.float64)\n",
    "                        learned_means, _ = self.policy_network.get_action_distribution(t0, evaluation_states)\n",
    "                        optimal_means, _ = self.soft_lqr.optimal_control_distribution(t0, evaluation_states)\n",
    "                        \n",
    "                        # Mean error in control means\n",
    "                        mean_error = torch.norm(learned_means - optimal_means, dim=1).mean().item()\n",
    "                        eval_errors.append(mean_error)\n",
    "                        \n",
    "                        if verbose:\n",
    "                            print(f\"Epoch {epoch+1}/{epochs}, Critic Loss: {step_losses['critic_loss']:.6f}, \"\n",
    "                                 f\"Actor Loss: {step_losses['actor_loss']:.6f}, Mean Error: {mean_error:.6f}\")\n",
    "                    else:\n",
    "                        if verbose:\n",
    "                            print(f\"Epoch {epoch+1}/{epochs}, Critic Loss: {step_losses['critic_loss']:.6f}, \"\n",
    "                                 f\"Actor Loss: {step_losses['actor_loss']:.6f}\")\n",
    "            elif verbose and (epoch+1) % eval_freq == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Critic Loss: {step_losses['critic_loss']:.6f}, \"\n",
    "                     f\"Actor Loss: {step_losses['actor_loss']:.6f}\")\n",
    "        \n",
    "        # Plot training curves\n",
    "        if verbose:\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.semilogy(losses['critic_loss'], label='Critic Loss')\n",
    "            plt.semilogy(losses['actor_loss'], label='Actor Loss')\n",
    "            plt.title('Loss During Training')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss (log scale)')\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            \n",
    "            if len(eval_errors) > 0:\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.semilogy(range(eval_freq-1, epochs, eval_freq), eval_errors)\n",
    "                plt.title('Policy Mean Error During Training')\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Mean Error (log scale)')\n",
    "                plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def evaluate(self, test_states: torch.Tensor, plot: bool = True) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate the trained actor-critic algorithm.\n",
    "        \n",
    "        Args:\n",
    "            test_states: Test states tensor (batch x state_dim)\n",
    "            plot: Whether to plot the results\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        # Set evaluation mode\n",
    "        self.policy_network.eval()\n",
    "        self.value_network.eval()\n",
    "        \n",
    "        metrics = {}\n",
    "        batch_size = test_states.shape[0]\n",
    "        t0 = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get value function predictions\n",
    "            predicted_values = self.value_network.value_function(t0, test_states)\n",
    "            \n",
    "            # Get policy distribution parameters\n",
    "            learned_means, learned_covs = self.policy_network.get_action_distribution(t0, test_states)\n",
    "            \n",
    "            # If we have the optimal policy and value function, compare\n",
    "            if hasattr(self.soft_lqr, 'value_function') and hasattr(self.soft_lqr, 'optimal_control_distribution'):\n",
    "                # Get optimal value function\n",
    "                true_values = self.soft_lqr.value_function(t0, test_states)\n",
    "                \n",
    "                # Get optimal policy parameters\n",
    "                optimal_means, optimal_cov = self.soft_lqr.optimal_control_distribution(t0, test_states)\n",
    "                \n",
    "                # Compute value function error\n",
    "                value_errors = torch.abs(predicted_values - true_values)\n",
    "                metrics['mean_value_error'] = value_errors.mean().item()\n",
    "                metrics['max_value_error'] = value_errors.max().item()\n",
    "                \n",
    "                # Compute policy mean error\n",
    "                mean_errors = torch.norm(learned_means - optimal_means, dim=1)\n",
    "                metrics['mean_policy_error'] = mean_errors.mean().item()\n",
    "                metrics['max_policy_error'] = mean_errors.max().item()\n",
    "                \n",
    "                print(f\"Evaluation results:\")\n",
    "                print(f\"  Value function - Mean error: {metrics['mean_value_error']:.6f}, Max error: {metrics['max_value_error']:.6f}\")\n",
    "                print(f\"  Policy means - Mean error: {metrics['mean_policy_error']:.6f}, Max error: {metrics['max_policy_error']:.6f}\")\n",
    "                \n",
    "                if plot:\n",
    "                    # Convert to numpy for plotting\n",
    "                    test_states_np = test_states.cpu().numpy()\n",
    "                    true_values_np = true_values.cpu().numpy()\n",
    "                    predicted_values_np = predicted_values.cpu().numpy()\n",
    "                    value_errors_np = value_errors.cpu().numpy()\n",
    "                    \n",
    "                    optimal_means_np = optimal_means.cpu().numpy()\n",
    "                    learned_means_np = learned_means.cpu().numpy()\n",
    "                    mean_errors_np = mean_errors.cpu().numpy()\n",
    "                    \n",
    "                    # Create plots\n",
    "                    plt.figure(figsize=(15, 10))\n",
    "                    \n",
    "                    # Value function plots\n",
    "                    plt.subplot(2, 2, 1)\n",
    "                    plt.scatter(true_values_np, predicted_values_np, alpha=0.7)\n",
    "                    min_val = min(true_values_np.min(), predicted_values_np.min())\n",
    "                    max_val = max(true_values_np.max(), predicted_values_np.max())\n",
    "                    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "                    plt.title('True vs Predicted Values')\n",
    "                    plt.xlabel('True Value')\n",
    "                    plt.ylabel('Predicted Value')\n",
    "                    plt.grid(True)\n",
    "                    \n",
    "                    plt.subplot(2, 2, 2)\n",
    "                    plt.scatter(true_values_np, value_errors_np, alpha=0.7)\n",
    "                    plt.title('Value Function Errors')\n",
    "                    plt.xlabel('True Value')\n",
    "                    plt.ylabel('Absolute Error')\n",
    "                    plt.grid(True)\n",
    "                    \n",
    "                    # Policy plots\n",
    "                    plt.subplot(2, 2, 3)\n",
    "                    plt.scatter(optimal_means_np[:, 0], learned_means_np[:, 0], alpha=0.7)\n",
    "                    min_val = min(optimal_means_np[:, 0].min(), learned_means_np[:, 0].min())\n",
    "                    max_val = max(optimal_means_np[:, 0].max(), learned_means_np[:, 0].max())\n",
    "                    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "                    plt.title('Optimal vs Learned Policy Means (First Dimension)')\n",
    "                    plt.xlabel('Optimal Mean')\n",
    "                    plt.ylabel('Learned Mean')\n",
    "                    plt.grid(True)\n",
    "                    \n",
    "                    plt.subplot(2, 2, 4)\n",
    "                    plt.scatter(np.linalg.norm(test_states_np, axis=1), mean_errors_np, alpha=0.7)\n",
    "                    plt.title('Policy Mean Errors vs State Norm')\n",
    "                    plt.xlabel('State Norm')\n",
    "                    plt.ylabel('Policy Mean Error')\n",
    "                    plt.grid(True)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "            else:\n",
    "                print(\"No optimal solution available for comparison. Skipping error metrics.\")\n",
    "        \n",
    "        # Switch back to training mode\n",
    "        self.policy_network.train()\n",
    "        self.value_network.train()\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUActorCriticAlgorithm(ActorCriticAlgorithm):\n",
    "    def __init__(self, soft_lqr: SoftLQR, policy_network: PolicyNetwork, value_network: ValueNetwork,\n",
    "                 actor_lr: float = 1e-4, critic_lr: float = 1e-3, device: str = \"cuda\"):\n",
    "        \"\"\"\n",
    "        GPU-optimized actor-critic algorithm for the soft LQR problem.\n",
    "        \n",
    "        Args:\n",
    "            soft_lqr: Soft LQR instance (used for system dynamics and cost parameters)\n",
    "            policy_network: Neural network to approximate the policy (actor)\n",
    "            value_network: Neural network to approximate the value function (critic)\n",
    "            actor_lr: Learning rate for the actor optimizer\n",
    "            critic_lr: Learning rate for the critic optimizer\n",
    "            device: Device to run the algorithm on (default: \"cuda\")\n",
    "        \"\"\"\n",
    "        super().__init__(soft_lqr, policy_network, value_network, actor_lr, critic_lr, device)\n",
    "        \n",
    "        # Create gradient scalers for mixed precision training\n",
    "        self.actor_scaler = torch.cuda.amp.GradScaler()\n",
    "        self.critic_scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "        # Enhanced optimizers with weight decay and momentum\n",
    "        self.actor_optimizer = optim.AdamW(\n",
    "            self.policy_network.parameters(), \n",
    "            lr=actor_lr,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "        \n",
    "        self.critic_optimizer = optim.AdamW(\n",
    "            self.value_network.parameters(), \n",
    "            lr=critic_lr,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "        \n",
    "        # Create learning rate schedulers\n",
    "        self.actor_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.actor_optimizer, 'min', factor=0.5, patience=20, verbose=True\n",
    "        )\n",
    "        \n",
    "        self.critic_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.critic_optimizer, 'min', factor=0.5, patience=20, verbose=True\n",
    "        )\n",
    "    \n",
    "    def compute_value_gradient(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        GPU-optimized computation of value function gradient.\n",
    "        Uses vectorized autograd for efficiency.\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            x: State tensor (batch x state_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Gradient of value function (batch x state_dim)\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Use autodiff for gradient calculation - more GPU friendly\n",
    "        # Create clone with gradient tracking for each batch element\n",
    "        gradients = torch.zeros_like(x)\n",
    "        \n",
    "        # Process in smaller batches to avoid memory issues\n",
    "        max_batch = 64  # Maximum batch size for gradient computation\n",
    "        \n",
    "        for i in range(0, batch_size, max_batch):\n",
    "            end_idx = min(i + max_batch, batch_size)\n",
    "            sub_batch_size = end_idx - i\n",
    "            \n",
    "            # Get sub-batch\n",
    "            t_sub = t[i:end_idx]\n",
    "            x_sub = x[i:end_idx].detach().clone().requires_grad_(True)\n",
    "            \n",
    "            # Compute value function\n",
    "            values = self.value_network.value_function(t_sub, x_sub)\n",
    "            \n",
    "            # Compute gradients for all elements in sub-batch at once\n",
    "            grad_outputs = torch.ones_like(values)\n",
    "            all_grads = torch.autograd.grad(\n",
    "                outputs=values,\n",
    "                inputs=x_sub,\n",
    "                grad_outputs=grad_outputs,\n",
    "                create_graph=False,\n",
    "                retain_graph=False,\n",
    "                only_inputs=True\n",
    "            )[0]\n",
    "            \n",
    "            # Store gradients\n",
    "            gradients[i:end_idx] = all_grads\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def simulate_trajectory(self, initial_states: torch.Tensor, \n",
    "                           num_steps: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        GPU-optimized trajectory simulation using vectorized operations.\n",
    "        \n",
    "        Args:\n",
    "            initial_states: Initial states tensor (batch x state_dim)\n",
    "            num_steps: Number of time steps for simulation\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (states, actions, time_points)\n",
    "        \"\"\"\n",
    "        batch_size = initial_states.shape[0]\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Generate time grid\n",
    "        time_grid = torch.linspace(0, self.soft_lqr.T, num_steps + 1, \n",
    "                                  device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Initialize states and actions\n",
    "        states = torch.zeros((batch_size, num_steps + 1, self.policy_network.state_dim), \n",
    "                           device=self.device, dtype=torch.float64)\n",
    "        states[:, 0, :] = initial_states\n",
    "        \n",
    "        actions = torch.zeros((batch_size, num_steps, self.policy_network.control_dim), \n",
    "                            device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Generate all Brownian increments at once\n",
    "        dW = torch.randn((batch_size, num_steps, self.soft_lqr.sigma.shape[1]), \n",
    "                        device=self.device, dtype=torch.float64) * np.sqrt(dt)\n",
    "        \n",
    "        # Pre-expand matrices for batched operations\n",
    "        H_expanded = self.soft_lqr.H.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        M_expanded = self.soft_lqr.M.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        sigma_expanded = self.soft_lqr.sigma.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Simulate trajectory using explicit scheme with vectorized operations\n",
    "        for n in range(num_steps):\n",
    "            # Current time and states\n",
    "            t_n = time_grid[n]\n",
    "            x_n = states[:, n, :]\n",
    "            \n",
    "            # Create batch of times\n",
    "            t_batch = torch.full((batch_size,), t_n, device=self.device, dtype=torch.float64)\n",
    "            \n",
    "            # Get policy distribution parameters\n",
    "            means, covariances = self.policy_network.get_action_distribution(t_batch, x_n)\n",
    "            \n",
    "            # Sample actions efficiently\n",
    "            # Generate standard normal samples\n",
    "            z = torch.randn((batch_size, self.policy_network.control_dim), \n",
    "                            device=self.device, dtype=torch.float64)\n",
    "            \n",
    "            # Transform to multivariate normal for each batch element\n",
    "            for i in range(batch_size):\n",
    "                # Compute Cholesky decomposition\n",
    "                L = torch.linalg.cholesky(covariances[i])\n",
    "                \n",
    "                # Apply transformation\n",
    "                actions[i, n, :] = means[i] + torch.mv(L, z[i])\n",
    "            \n",
    "            # Compute all drifts at once using batched matrix multiplication\n",
    "            # Reshape for bmm: [batch_size, state_dim, 1]\n",
    "            x_n_expanded = x_n.unsqueeze(2)\n",
    "            a_n_expanded = actions[:, n, :].unsqueeze(2)\n",
    "            \n",
    "            # Compute Hx: [batch_size, state_dim, 1]\n",
    "            Hx = torch.bmm(H_expanded, x_n_expanded)\n",
    "            \n",
    "            # Compute Ma: [batch_size, state_dim, 1]\n",
    "            Ma = torch.bmm(M_expanded, a_n_expanded)\n",
    "            \n",
    "            # Compute drift: [batch_size, state_dim, 1]\n",
    "            drift = Hx + Ma\n",
    "            \n",
    "            # Apply diffusion term: [batch_size, state_dim, noise_dim] x [batch_size, noise_dim, 1]\n",
    "            diffusion = torch.bmm(sigma_expanded, dW[:, n, :].unsqueeze(2))\n",
    "            \n",
    "            # Update states: [batch_size, state_dim]\n",
    "            states[:, n+1, :] = x_n + drift.squeeze(2) * dt + diffusion.squeeze(2)\n",
    "        \n",
    "        return states, actions, time_grid\n",
    "    \n",
    "    def compute_critic_targets(self, states: torch.Tensor, actions: torch.Tensor, \n",
    "                              time_grid: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        GPU-optimized computation of critic targets using vectorized operations.\n",
    "        \n",
    "        Args:\n",
    "            states: States tensor (batch x time_steps+1 x state_dim)\n",
    "            actions: Actions tensor (batch x time_steps x control_dim)\n",
    "            time_grid: Time grid tensor (time_steps+1)\n",
    "            \n",
    "        Returns:\n",
    "            Target values for each state and time step (batch x time_steps+1)\n",
    "        \"\"\"\n",
    "        batch_size = states.shape[0]\n",
    "        num_steps = time_grid.shape[0] - 1\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Initialize accumulated costs (will be transformed to targets)\n",
    "        accumulated_costs = torch.zeros((batch_size, num_steps + 1), \n",
    "                                       device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Pre-expand cost matrices for batched operations\n",
    "        C_expanded = self.soft_lqr.C.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        D_expanded = self.soft_lqr.D.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        R_expanded = self.soft_lqr.R.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Process in batches of time steps to avoid memory issues\n",
    "        step_batch_size = min(num_steps, 20)  # Process 20 steps at a time\n",
    "        \n",
    "        for step_start in range(0, num_steps, step_batch_size):\n",
    "            step_end = min(step_start + step_batch_size, num_steps)\n",
    "            \n",
    "            # Get states and actions for this batch of steps\n",
    "            batch_states = states[:, step_start:step_end, :]\n",
    "            batch_actions = actions[:, step_start:step_end, :]\n",
    "            batch_times = time_grid[step_start:step_end]\n",
    "            \n",
    "            # Compute state costs for all steps in batch\n",
    "            # Reshape for batched matrix multiplication: [batch_size, steps, state_dim, 1]\n",
    "            x_expanded = batch_states.unsqueeze(3)\n",
    "            \n",
    "            # Compute x^T C x for all states: [batch_size, steps]\n",
    "            Cx = torch.matmul(C_expanded.unsqueeze(1), x_expanded)\n",
    "            state_costs = torch.matmul(x_expanded.transpose(2, 3), Cx).squeeze()\n",
    "            \n",
    "            # Compute control costs for all steps in batch\n",
    "            # Reshape for batched matrix multiplication: [batch_size, steps, control_dim, 1]\n",
    "            a_expanded = batch_actions.unsqueeze(3)\n",
    "            \n",
    "            # Compute a^T D a for all actions: [batch_size, steps]\n",
    "            Da = torch.matmul(D_expanded.unsqueeze(1), a_expanded)\n",
    "            control_costs = torch.matmul(a_expanded.transpose(2, 3), Da).squeeze()\n",
    "            \n",
    "            # Compute entropy terms for all steps in batch\n",
    "            entropy_terms = torch.zeros((batch_size, step_end - step_start), \n",
    "                                      device=self.device, dtype=torch.float64)\n",
    "            \n",
    "            for t_idx, t in enumerate(batch_times):\n",
    "                # Get policy distribution parameters\n",
    "                t_batch = torch.full((batch_size,), t, device=self.device, dtype=torch.float64)\n",
    "                means, covariances = self.policy_network.get_action_distribution(t_batch, batch_states[:, t_idx, :])\n",
    "                \n",
    "                # Compute log probabilities\n",
    "                for i in range(batch_size):\n",
    "                    # Compute log probability of the action\n",
    "                    diff = batch_actions[i, t_idx, :] - means[i]\n",
    "                    inv_cov = torch.inverse(covariances[i])\n",
    "                    quad_term = torch.dot(diff, torch.mv(inv_cov, diff))\n",
    "                    log_det_term = torch.log(torch.det(2 * np.pi * covariances[i]))\n",
    "                    log_prob = -0.5 * (quad_term + log_det_term)\n",
    "                    \n",
    "                    # Entropy term is -τ * log_prob\n",
    "                    entropy_terms[i, t_idx] = self.soft_lqr.tau * (-log_prob)\n",
    "            \n",
    "            # Compute total costs for this batch of steps\n",
    "            step_costs = state_costs + control_costs + entropy_terms\n",
    "            \n",
    "            # Accumulate costs into running tally\n",
    "            for i in range(batch_size):\n",
    "                for j in range(step_start, step_end):\n",
    "                    accumulated_costs[i, j:] += step_costs[i, j-step_start] * dt\n",
    "        \n",
    "        # Add terminal costs\n",
    "        x_T = states[:, -1, :]\n",
    "        x_T_expanded = x_T.unsqueeze(2)  # [batch_size, state_dim, 1]\n",
    "        R_x_T = torch.bmm(R_expanded, x_T_expanded)  # [batch_size, state_dim, 1]\n",
    "        terminal_costs = torch.bmm(x_T_expanded.transpose(1, 2), R_x_T).squeeze()  # [batch_size]\n",
    "        accumulated_costs[:, -1] += terminal_costs\n",
    "        \n",
    "        return accumulated_costs\n",
    "    \n",
    "    def compute_actor_loss(self, states: torch.Tensor, actions: torch.Tensor, \n",
    "                          time_grid: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        GPU-optimized computation of actor loss using vectorized operations.\n",
    "        \n",
    "        Args:\n",
    "            states: States tensor (batch x time_steps+1 x state_dim)\n",
    "            actions: Actions tensor (batch x time_steps x control_dim)\n",
    "            time_grid: Time grid tensor (time_steps+1)\n",
    "            \n",
    "        Returns:\n",
    "            Policy loss\n",
    "        \"\"\"\n",
    "        batch_size = states.shape[0]\n",
    "        num_steps = actions.shape[1]\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Initialize loss\n",
    "        total_loss = torch.tensor(0.0, device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Pre-expand matrices for vectorized operations\n",
    "        C_expanded = self.soft_lqr.C.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        D_expanded = self.soft_lqr.D.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        H_expanded = self.soft_lqr.H.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        M_expanded = self.soft_lqr.M.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Process in batches of time steps to avoid memory issues\n",
    "        step_batch_size = min(num_steps, 10)  # Process 10 steps at a time\n",
    "        \n",
    "        for step_start in range(0, num_steps, step_batch_size):\n",
    "            step_end = min(step_start + step_batch_size, num_steps)\n",
    "            step_count = step_end - step_start\n",
    "            \n",
    "            # Get states and actions for this batch of steps\n",
    "            batch_states = states[:, step_start:step_end, :]  # [batch_size, step_count, state_dim]\n",
    "            batch_actions = actions[:, step_start:step_end, :]  # [batch_size, step_count, control_dim]\n",
    "            batch_times = time_grid[step_start:step_end]  # [step_count]\n",
    "            \n",
    "            # Reshape for vectorized operations\n",
    "            # [batch_size * step_count, state_dim]\n",
    "            flat_states = batch_states.reshape(-1, self.policy_network.state_dim)\n",
    "            # [batch_size * step_count, control_dim]\n",
    "            flat_actions = batch_actions.reshape(-1, self.policy_network.control_dim)\n",
    "            # [batch_size * step_count]\n",
    "            flat_times = torch.repeat_interleave(batch_times, batch_size)\n",
    "            \n",
    "            # Compute state costs: x^T C x\n",
    "            # Reshape for batched matrix multiplication: [batch_size*step_count, state_dim, 1]\n",
    "            x_expanded = flat_states.unsqueeze(2)\n",
    "            # Expand C: [batch_size*step_count, state_dim, state_dim]\n",
    "            C_flat = C_expanded.repeat_interleave(step_count, dim=0)\n",
    "            # Compute Cx: [batch_size*step_count, state_dim, 1]\n",
    "            Cx = torch.bmm(C_flat, x_expanded)\n",
    "            # Compute x^T C x: [batch_size*step_count]\n",
    "            state_costs = torch.bmm(x_expanded.transpose(1, 2), Cx).squeeze()\n",
    "            \n",
    "            # Compute control costs: a^T D a\n",
    "            # Reshape for batched matrix multiplication: [batch_size*step_count, control_dim, 1]\n",
    "            a_expanded = flat_actions.unsqueeze(2)\n",
    "            # Expand D: [batch_size*step_count, control_dim, control_dim]\n",
    "            D_flat = D_expanded.repeat_interleave(step_count, dim=0)\n",
    "            # Compute Da: [batch_size*step_count, control_dim, 1]\n",
    "            Da = torch.bmm(D_flat, a_expanded)\n",
    "            # Compute a^T D a: [batch_size*step_count]\n",
    "            control_costs = torch.bmm(a_expanded.transpose(1, 2), Da).squeeze()\n",
    "            \n",
    "            # Compute value function gradients for all states at once\n",
    "            value_gradients = self.compute_value_gradient(flat_times, flat_states)\n",
    "            \n",
    "            # Get policy distribution parameters for all states at once\n",
    "            means, covariances = self.policy_network.get_action_distribution(flat_times, flat_states)\n",
    "            \n",
    "            # Compute log probabilities and entropy terms\n",
    "            log_probs = torch.zeros(batch_size * step_count, device=self.device, dtype=torch.float64)\n",
    "            \n",
    "            for i in range(batch_size * step_count):\n",
    "                # Compute log probability of the action\n",
    "                diff = flat_actions[i] - means[i]\n",
    "                inv_cov = torch.inverse(covariances[i])\n",
    "                quad_term = torch.dot(diff, torch.mv(inv_cov, diff))\n",
    "                log_det_term = torch.log(torch.det(2 * np.pi * covariances[i]))\n",
    "                log_probs[i] = -0.5 * (quad_term + log_det_term)\n",
    "            \n",
    "            # Entropy terms: -τ * log_prob\n",
    "            entropy_terms = self.soft_lqr.tau * (-log_probs)\n",
    "            \n",
    "            # Compute drift terms\n",
    "            # Expand H and M matrices: [batch_size*step_count, state_dim, state_dim/control_dim]\n",
    "            H_flat = H_expanded.repeat_interleave(step_count, dim=0)\n",
    "            M_flat = M_expanded.repeat_interleave(step_count, dim=0)\n",
    "            \n",
    "            # Compute Hx: [batch_size*step_count, state_dim]\n",
    "            Hx = torch.bmm(H_flat, x_expanded).squeeze()\n",
    "            \n",
    "            # Compute Ma: [batch_size*step_count, state_dim]\n",
    "            Ma = torch.bmm(M_flat, a_expanded).squeeze()\n",
    "            \n",
    "            # Compute total drift: [batch_size*step_count, state_dim]\n",
    "            drift = Hx + Ma\n",
    "            \n",
    "            # Compute drift term: ∇_x v(t,x)^T (Hx + Ma)\n",
    "            # [batch_size*step_count]\n",
    "            drift_terms = torch.sum(value_gradients * drift, dim=1)\n",
    "            \n",
    "            # Total costs for all states and time steps: [batch_size*step_count]\n",
    "            total_costs = state_costs + control_costs + entropy_terms + drift_terms\n",
    "            \n",
    "            # Reshape back to [batch_size, step_count]\n",
    "            reshaped_costs = total_costs.view(batch_size, step_count)\n",
    "            \n",
    "            # Average over batch and accumulate with time step weight\n",
    "            step_loss = reshaped_costs.mean() * dt * step_count\n",
    "            total_loss = total_loss + step_loss\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def compute_critic_loss(self, states: torch.Tensor, targets: torch.Tensor, \n",
    "                           time_grid: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        GPU-optimized computation of critic loss using vectorized operations.\n",
    "        \n",
    "        Args:\n",
    "            states: States tensor (batch x time_steps+1 x state_dim)\n",
    "            targets: Target values (batch x time_steps+1)\n",
    "            time_grid: Time grid tensor (time_steps+1)\n",
    "            \n",
    "        Returns:\n",
    "            Critic loss\n",
    "        \"\"\"\n",
    "        batch_size = states.shape[0]\n",
    "        num_steps = states.shape[1] - 1\n",
    "        \n",
    "        # Initialize total loss\n",
    "        total_loss = torch.tensor(0.0, device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Process in batches of time steps to avoid memory issues\n",
    "        step_batch_size = min(num_steps + 1, 20)  # Process 20 steps at a time\n",
    "        \n",
    "        for step_start in range(0, num_steps + 1, step_batch_size):\n",
    "            step_end = min(step_start + step_batch_size, num_steps + 1)\n",
    "            step_count = step_end - step_start\n",
    "            \n",
    "            # Get states and targets for this batch of steps\n",
    "            batch_states = states[:, step_start:step_end, :]  # [batch_size, step_count, state_dim]\n",
    "            batch_targets = targets[:, step_start:step_end]  # [batch_size, step_count]\n",
    "            batch_times = time_grid[step_start:step_end]  # [step_count]\n",
    "            \n",
    "            # Reshape for vectorized operations\n",
    "            # [batch_size * step_count, state_dim]\n",
    "            flat_states = batch_states.reshape(-1, self.policy_network.state_dim)\n",
    "            # [batch_size * step_count]\n",
    "            flat_targets = batch_targets.reshape(-1)\n",
    "            # [batch_size * step_count]\n",
    "            flat_times = torch.repeat_interleave(batch_times, batch_size)\n",
    "            \n",
    "            # Predict values for all states at once\n",
    "            flat_predicted = self.value_network.value_function(flat_times, flat_states)\n",
    "            \n",
    "            # Compute MSE loss for all states at once\n",
    "            step_loss = ((flat_predicted - flat_targets) ** 2).mean()\n",
    "            \n",
    "            # Add to total loss\n",
    "            total_loss = total_loss + step_loss * step_count\n",
    "        \n",
    "        # Average loss over all time steps\n",
    "        total_loss = total_loss / (num_steps + 1)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def train_step(self, initial_states_dist, num_states: int, num_steps: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        GPU-optimized training step with mixed precision.\n",
    "        \n",
    "        Args:\n",
    "            initial_states_dist: Distribution to sample initial states from\n",
    "            num_states: Number of initial states to sample\n",
    "            num_steps: Number of time steps for simulation\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of loss values for this step\n",
    "        \"\"\"\n",
    "        # Sample initial states\n",
    "        initial_states = initial_states_dist.sample((num_states,)).to(self.device)\n",
    "        \n",
    "        # Simulate trajectory using current policy (in float64 for accuracy)\n",
    "        states, actions, time_grid = self.simulate_trajectory(initial_states, num_steps)\n",
    "        \n",
    "        # Compute critic targets (in float64 for accuracy)\n",
    "        targets = self.compute_critic_targets(states, actions, time_grid)\n",
    "        \n",
    "        # Update critic with mixed precision\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Cast to float32 for network computation\n",
    "            states_fp32 = states.to(torch.float32)\n",
    "            targets_fp32 = targets.to(torch.float32)\n",
    "            time_grid_fp32 = time_grid.to(torch.float32)\n",
    "            \n",
    "            # Compute critic loss\n",
    "            critic_loss = self.compute_critic_loss(states_fp32, targets_fp32, time_grid_fp32)\n",
    "        \n",
    "        # Backpropagation with gradient scaling for critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        self.critic_scaler.scale(critic_loss).backward()\n",
    "        \n",
    "        # Clip gradients to avoid exploding gradients\n",
    "        self.critic_scaler.unscale_(self.critic_optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(self.value_network.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update critic weights and scaler\n",
    "        self.critic_scaler.step(self.critic_optimizer)\n",
    "        self.critic_scaler.update()\n",
    "        \n",
    "        # After updating the critic, simulate new trajectory\n",
    "        states, actions, time_grid = self.simulate_trajectory(initial_states, num_steps)\n",
    "        \n",
    "        # Update actor with mixed precision\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Cast to float32 for network computation\n",
    "            states_fp32 = states.to(torch.float32)\n",
    "            actions_fp32 = actions.to(torch.float32)\n",
    "            time_grid_fp32 = time_grid.to(torch.float32)\n",
    "            \n",
    "            # Compute actor loss\n",
    "            actor_loss = self.compute_actor_loss(states_fp32, actions_fp32, time_grid_fp32)\n",
    "        \n",
    "        # Backpropagation with gradient scaling for actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.actor_scaler.scale(actor_loss).backward()\n",
    "        \n",
    "        # Clip gradients to avoid exploding gradients\n",
    "        self.actor_scaler.unscale_(self.actor_optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_network.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update actor weights and scaler\n",
    "        self.actor_scaler.step(self.actor_optimizer)\n",
    "        self.actor_scaler.update()\n",
    "        \n",
    "        return {\n",
    "            'critic_loss': critic_loss.item(),\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'total_loss': critic_loss.item() + actor_loss.item()\n",
    "        }\n",
    "    \n",
    "    def train(self, epochs: int, batch_size: int, num_steps: int, \n",
    "             initial_states_dist, eval_freq: int = 10, verbose: bool = True,\n",
    "             evaluation_states: Optional[torch.Tensor] = None) -> Dict[str, List[float]]:\n",
    "        \"\"\"\n",
    "        GPU-optimized training with timing and learning rate scheduling.\n",
    "        \n",
    "        Args:\n",
    "            epochs: Number of training epochs\n",
    "            batch_size: Number of initial states per batch\n",
    "            num_steps: Number of time steps for simulation\n",
    "            initial_states_dist: Distribution to sample initial states from\n",
    "            eval_freq: Frequency of evaluation\n",
    "            verbose: Whether to print progress\n",
    "            evaluation_states: Optional tensor of states for periodic evaluation\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of lists of loss values during training\n",
    "        \"\"\"\n",
    "        losses = {\n",
    "            'critic_loss': [],\n",
    "            'actor_loss': [],\n",
    "            'total_loss': []\n",
    "        }\n",
    "        eval_errors = []\n",
    "        \n",
    "        # Create events for timing on GPU\n",
    "        if torch.cuda.is_available():\n",
    "            start_event = torch.cuda.Event(enable_timing=True)\n",
    "            end_event = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Start timing for this epoch\n",
    "            if torch.cuda.is_available():\n",
    "                start_event.record()\n",
    "            \n",
    "            # Perform one training step\n",
    "            step_losses = self.train_step(initial_states_dist, batch_size, num_steps)\n",
    "            \n",
    "            # Record losses\n",
    "            for key in losses.keys():\n",
    "                losses[key].append(step_losses[key])\n",
    "            \n",
    "            # Update learning rate schedulers\n",
    "            self.actor_scheduler.step(step_losses['actor_loss'])\n",
    "            self.critic_scheduler.step(step_losses['critic_loss'])\n",
    "            \n",
    "            # End timing for this epoch\n",
    "            if torch.cuda.is_available():\n",
    "                end_event.record()\n",
    "                torch.cuda.synchronize()\n",
    "                epoch_time = start_event.elapsed_time(end_event) / 1000.0  # in seconds\n",
    "            else:\n",
    "                epoch_time = 0.0\n",
    "            \n",
    "            # Evaluate periodically\n",
    "            if (epoch+1) % eval_freq == 0:\n",
    "                if evaluation_states is not None and hasattr(self.soft_lqr, 'optimal_control_distribution'):\n",
    "                    with torch.no_grad():\n",
    "                        # Compute error between learned and optimal policy\n",
    "                        t0 = torch.zeros(evaluation_states.shape[0], device=self.device, dtype=torch.float64)\n",
    "                        \n",
    "                        # Use pure FP64 for accurate evaluation\n",
    "                        learned_means, _ = self.policy_network.get_action_distribution(t0, evaluation_states)\n",
    "                        optimal_means, _ = self.soft_lqr.optimal_control_distribution(t0, evaluation_states)\n",
    "                        \n",
    "                        # Mean error in control means\n",
    "                        mean_error = torch.norm(learned_means - optimal_means, dim=1).mean().item()\n",
    "                        eval_errors.append(mean_error)\n",
    "                        \n",
    "                        if verbose:\n",
    "                            current_actor_lr = self.actor_optimizer.param_groups[0]['lr']\n",
    "                            current_critic_lr = self.critic_optimizer.param_groups[0]['lr']\n",
    "                            print(f\"Epoch {epoch+1}/{epochs}, Critic Loss: {step_losses['critic_loss']:.6f}, \"\n",
    "                                 f\"Actor Loss: {step_losses['actor_loss']:.6f}, Mean Error: {mean_error:.6f}, \"\n",
    "                                 f\"Actor LR: {current_actor_lr:.6f}, Critic LR: {current_critic_lr:.6f}, \"\n",
    "                                 f\"Time: {epoch_time:.3f}s\")\n",
    "                elif verbose:\n",
    "                    current_actor_lr = self.actor_optimizer.param_groups[0]['lr']\n",
    "                    current_critic_lr = self.critic_optimizer.param_groups[0]['lr']\n",
    "                    print(f\"Epoch {epoch+1}/{epochs}, Critic Loss: {step_losses['critic_loss']:.6f}, \"\n",
    "                         f\"Actor Loss: {step_losses['actor_loss']:.6f}, \"\n",
    "                         f\"Actor LR: {current_actor_lr:.6f}, Critic LR: {current_critic_lr:.6f}, \"\n",
    "                         f\"Time: {epoch_time:.3f}s\")\n",
    "        \n",
    "        # Plot training curves\n",
    "        if verbose:\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.semilogy(losses['critic_loss'], label='Critic Loss')\n",
    "            plt.semilogy(losses['actor_loss'], label='Actor Loss')\n",
    "            plt.title('Loss During Training (GPU Accelerated)')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss (log scale)')\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            \n",
    "            if len(eval_errors) > 0:\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.semilogy(range(eval_freq-1, epochs, eval_freq), eval_errors)\n",
    "                plt.title('Policy Mean Error During Training')\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Mean Error (log scale)')\n",
    "                plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_actor_critic():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Problem setup as specified in the exercise\n",
    "    H = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64) * 0.5\n",
    "    M = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64)\n",
    "    sigma = torch.eye(2, dtype=torch.float64) * 0.5\n",
    "    C = torch.tensor([[1.0, 0.1], [0.1, 1.0]], dtype=torch.float64) * 1.0\n",
    "    D = torch.eye(2, dtype=torch.float64)  # D just identity, as specified\n",
    "    R = torch.tensor([[1.0, 0.3], [0.3, 1.0]], dtype=torch.float64) * 10.0\n",
    "    \n",
    "    # Set terminal time and time grid\n",
    "    T = 0.5\n",
    "    grid_size = 1000\n",
    "    time_grid = torch.linspace(0, T, grid_size, dtype=torch.float64)\n",
    "    \n",
    "    # Set entropy regularization parameters\n",
    "    tau = 0.5  # τ = 1/2 as specified\n",
    "    gamma = 1.0  # γ = 1 as specified\n",
    "    \n",
    "    # Create soft LQR instance for reference solution\n",
    "    soft_lqr = SoftLQR(H, M, sigma, C, D, R, T, time_grid, tau, gamma)\n",
    "    \n",
    "    # Solve Ricatti ODE for reference solution\n",
    "    print(\"Computing reference solution...\")\n",
    "    soft_lqr.solve_ricatti()\n",
    "    \n",
    "    # Create policy network (actor)\n",
    "    actor_hidden_size = 256\n",
    "    policy_network = PolicyNetwork(\n",
    "        hidden_size=actor_hidden_size, \n",
    "        state_dim=2, \n",
    "        control_dim=2, \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create value network (critic)\n",
    "    critic_hidden_size = 512\n",
    "    value_network = ValueNetwork(\n",
    "        hidden_size=critic_hidden_size,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Set learning rates\n",
    "    actor_lr = 1e-4\n",
    "    critic_lr = 1e-3\n",
    "    \n",
    "    # Create actor-critic algorithm based on device\n",
    "    if device.type == \"cuda\":\n",
    "        print(\"Using GPU-optimized actor-critic algorithm...\")\n",
    "        actor_critic = GPUActorCriticAlgorithm(\n",
    "            soft_lqr, policy_network, value_network, \n",
    "            actor_lr, critic_lr, device\n",
    "        )\n",
    "    else:\n",
    "        print(\"Using CPU-based actor-critic algorithm...\")\n",
    "        actor_critic = ActorCriticAlgorithm(\n",
    "            soft_lqr, policy_network, value_network, \n",
    "            actor_lr, critic_lr, device\n",
    "        )\n",
    "    \n",
    "    # Define initial state distribution\n",
    "    # Uniform distribution in [-3, 3] x [-3, 3]\n",
    "    class UniformInitialState:\n",
    "        def __init__(self, low=-3.0, high=3.0, dim=2, device=device):\n",
    "            self.low = low\n",
    "            self.high = high\n",
    "            self.dim = dim\n",
    "            self.device = device\n",
    "        \n",
    "        def sample(self, shape):\n",
    "            return torch.empty((*shape, self.dim), dtype=torch.float64, device=self.device).uniform_(self.low, self.high)\n",
    "    \n",
    "    initial_states_dist = UniformInitialState()\n",
    "    \n",
    "    # Create evaluation grid for periodic assessment\n",
    "    x1 = torch.linspace(-3, 3, 11, device=device, dtype=torch.float64)\n",
    "    x2 = torch.linspace(-3, 3, 11, device=device, dtype=torch.float64)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    eval_states = torch.stack([X1.flatten(), X2.flatten()], dim=1)\n",
    "    \n",
    "    # Training parameters\n",
    "    if device.type == \"cuda\":\n",
    "        epochs = 500\n",
    "        batch_size = 128  # Larger for GPU\n",
    "        num_steps = 100  # N = 100 as specified\n",
    "        eval_freq = 25\n",
    "    else:\n",
    "        epochs = 300  # Fewer epochs for CPU\n",
    "        batch_size = 64  # Smaller for CPU\n",
    "        num_steps = 100  # N = 100 as specified\n",
    "        eval_freq = 25\n",
    "    \n",
    "    # Train the actor-critic algorithm\n",
    "    print(\"Training actor-critic algorithm...\")\n",
    "    losses = actor_critic.train(\n",
    "        epochs, batch_size, num_steps, initial_states_dist, \n",
    "        eval_freq, verbose=True, evaluation_states=eval_states\n",
    "    )\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"Final evaluation of actor-critic algorithm...\")\n",
    "    actor_critic.evaluate(eval_states, plot=True)\n",
    "    \n",
    "    # Visualize final policies\n",
    "    # Create denser grid for smoother visualization\n",
    "    x1_dense = torch.linspace(-3, 3, 21, device=device, dtype=torch.float64)\n",
    "    x2_dense = torch.linspace(-3, 3, 21, device=device, dtype=torch.float64)\n",
    "    X1_dense, X2_dense = torch.meshgrid(x1_dense, x2_dense, indexing='ij')\n",
    "    dense_states = torch.stack([X1_dense.flatten(), X2_dense.flatten()], dim=1)\n",
    "    \n",
    "    # Set time to zero\n",
    "    t0 = torch.zeros(dense_states.shape[0], device=device, dtype=torch.float64)\n",
    "    \n",
    "    # Get learned and optimal policy means\n",
    "    with torch.no_grad():\n",
    "        learned_means, learned_covs = policy_network.get_action_distribution(t0, dense_states)\n",
    "        optimal_means, optimal_cov = soft_lqr.optimal_control_distribution(t0, dense_states)\n",
    "    \n",
    "    # Convert to numpy for plotting\n",
    "    X1_np = X1_dense.cpu().numpy()\n",
    "    X2_np = X2_dense.cpu().numpy()\n",
    "    \n",
    "    learned_means_np = learned_means.cpu().numpy()\n",
    "    optimal_means_np = optimal_means.cpu().numpy()\n",
    "    \n",
    "    # Reshape for quiver plots\n",
    "    learned_u = learned_means_np[:, 0].reshape(X1_np.shape)\n",
    "    learned_v = learned_means_np[:, 1].reshape(X1_np.shape)\n",
    "    \n",
    "    optimal_u = optimal_means_np[:, 0].reshape(X1_np.shape)\n",
    "    optimal_v = optimal_means_np[:, 1].reshape(X1_np.shape)\n",
    "    \n",
    "    # Create control policy quiver plots\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.quiver(X1_np, X2_np, optimal_u, optimal_v)\n",
    "    plt.title('Optimal Policy (Soft LQR Solution)')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.quiver(X1_np, X2_np, learned_u, learned_v)\n",
    "    plt.title('Learned Policy (Actor-Critic)')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize value function\n",
    "    with torch.no_grad():\n",
    "        learned_values = value_network.value_function(t0, dense_states)\n",
    "        optimal_values = soft_lqr.value_function(t0, dense_states)\n",
    "    \n",
    "    # Convert to numpy for plotting\n",
    "    learned_values_np = learned_values.cpu().numpy().reshape(X1_np.shape)\n",
    "    optimal_values_np = optimal_values.cpu().numpy().reshape(X1_np.shape)\n",
    "    \n",
    "    # Create value function contour plots\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    contour1 = plt.contourf(X1_np, X2_np, optimal_values_np, 20, cmap='viridis')\n",
    "    plt.colorbar(contour1)\n",
    "    plt.title('Optimal Value Function (Soft LQR Solution)')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    contour2 = plt.contourf(X1_np, X2_np, learned_values_np, 20, cmap='viridis')\n",
    "    plt.colorbar(contour2)\n",
    "    plt.title('Learned Value Function (Actor-Critic)')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Test policy on trajectory simulations\n",
    "    print(\"Simulating trajectories with actor-critic policy...\")\n",
    "    \n",
    "    # Sample test initial states\n",
    "    test_initial_states = torch.tensor([\n",
    "        [2.0, 2.0],\n",
    "        [2.0, -2.0],\n",
    "        [-2.0, -2.0],\n",
    "        [-2.0, 2.0]\n",
    "    ], device=device, dtype=torch.float64)\n",
    "    \n",
    "    # Simulate trajectories with both policies\n",
    "    num_steps_sim = 200\n",
    "    dt = T / num_steps_sim\n",
    "    \n",
    "    # Generate same noise for both simulations\n",
    "    dW = torch.randn((4, num_steps_sim, sigma.shape[1]), \n",
    "                    device=device, dtype=torch.float64) * np.sqrt(dt)\n",
    "    \n",
    "    # Time grid for simulation\n",
    "    t_sim = torch.linspace(0, T, num_steps_sim + 1, device=device, dtype=torch.float64)\n",
    "    \n",
    "    # Initialize trajectories\n",
    "    opt_traj = torch.zeros((4, num_steps_sim + 1, 2), device=device, dtype=torch.float64)\n",
    "    learned_traj = torch.zeros((4, num_steps_sim + 1, 2), device=device, dtype=torch.float64)\n",
    "    \n",
    "    # Set initial states\n",
    "    opt_traj[:, 0, :] = test_initial_states\n",
    "    learned_traj[:, 0, :] = test_initial_states\n",
    "    \n",
    "    # Simulate trajectories\n",
    "    for n in range(num_steps_sim):\n",
    "        # Current time and states\n",
    "        t_n = t_sim[n]\n",
    "        t_batch = torch.full((4,), t_n, device=device, dtype=torch.float64)\n",
    "        \n",
    "        # Get control from optimal policy\n",
    "        opt_means, opt_cov = soft_lqr.optimal_control_distribution(t_batch, opt_traj[:, n, :])\n",
    "        opt_L = torch.linalg.cholesky(opt_cov)\n",
    "        \n",
    "        # Get control from learned policy\n",
    "        learned_means, learned_covs = policy_network.get_action_distribution(t_batch, learned_traj[:, n, :])\n",
    "        \n",
    "        # Use same noise for both policies\n",
    "        z = torch.randn((4, 2), device=device, dtype=torch.float64)\n",
    "        \n",
    "        # Sample controls\n",
    "        opt_actions = torch.zeros((4, 2), device=device, dtype=torch.float64)\n",
    "        learned_actions = torch.zeros((4, 2), device=device, dtype=torch.float64)\n",
    "        \n",
    "        for i in range(4):\n",
    "            opt_actions[i] = opt_means[i] + torch.mv(opt_L, z[i])\n",
    "            \n",
    "            # For learned policy, need to compute Cholesky for each covariance\n",
    "            learned_L = torch.linalg.cholesky(learned_covs[i])\n",
    "            learned_actions[i] = learned_means[i] + torch.mv(learned_L, z[i])\n",
    "        \n",
    "        # Update states\n",
    "        for i in range(4):\n",
    "            # Optimal trajectory update\n",
    "            opt_drift = H @ opt_traj[i, n, :] + M @ opt_actions[i]\n",
    "            opt_traj[i, n+1, :] = opt_traj[i, n, :] + opt_drift * dt + sigma @ dW[i, n, :]\n",
    "            \n",
    "            # Learned trajectory update\n",
    "            learned_drift = H @ learned_traj[i, n, :] + M @ learned_actions[i]\n",
    "            learned_traj[i, n+1, :] = learned_traj[i, n, :] + learned_drift * dt + sigma @ dW[i, n, :]\n",
    "    \n",
    "    # Convert to numpy for plotting\n",
    "    opt_traj_np = opt_traj.cpu().numpy()\n",
    "    learned_traj_np = learned_traj.cpu().numpy()\n",
    "    \n",
    "    # Plot trajectories\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    for i in range(4):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.plot(opt_traj_np[i, :, 0], opt_traj_np[i, :, 1], 'b-', label='Optimal Policy')\n",
    "        plt.plot(learned_traj_np[i, :, 0], learned_traj_np[i, :, 1], 'r-', label='Actor-Critic Policy')\n",
    "        plt.scatter(test_initial_states[i, 0].cpu().numpy(), test_initial_states[i, 1].cpu().numpy(), \n",
    "                   c='g', s=100, marker='o', label='Initial State')\n",
    "        plt.title(f'Trajectory from Initial State {test_initial_states[i].cpu().numpy()}')\n",
    "        plt.xlabel('x1')\n",
    "        plt.ylabel('x2')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Actor-Critic training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_actor_critic()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
