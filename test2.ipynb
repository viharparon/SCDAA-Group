{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "class LQR:\n",
    "    def __init__(self, H: torch.Tensor, M: torch.Tensor, sigma: torch.Tensor, \n",
    "                 C: torch.Tensor, D: torch.Tensor, R: torch.Tensor, \n",
    "                 T: float, time_grid: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Initialize the LQR problem.\n",
    "        \n",
    "        Args:\n",
    "            H: System dynamics matrix (d x d)\n",
    "            M: Control input matrix (d x m)\n",
    "            sigma: Noise matrix (d x d')\n",
    "            C: State cost matrix (d x d)\n",
    "            D: Control cost matrix (m x m)\n",
    "            R: Terminal state cost matrix (d x d)\n",
    "            T: Terminal time\n",
    "            time_grid: Grid of time points\n",
    "        \"\"\"\n",
    "        self.H = H\n",
    "        self.M = M\n",
    "        self.sigma = sigma\n",
    "        self.C = C\n",
    "        self.D = D\n",
    "        self.R = R\n",
    "        self.T = T\n",
    "        self.time_grid = time_grid\n",
    "        \n",
    "        # Check dimensions\n",
    "        d, m = M.shape\n",
    "        assert H.shape == (d, d), \"H must be d x d\"\n",
    "        assert sigma.shape[0] == d, \"sigma must be d x d'\"\n",
    "        assert C.shape == (d, d), \"C must be d x d\"\n",
    "        assert D.shape == (m, m), \"D must be m x m\"\n",
    "        assert R.shape == (d, d), \"R must be d x d\"\n",
    "        \n",
    "        # Store dimensions\n",
    "        self.d = d\n",
    "        self.m = m\n",
    "        \n",
    "        # Compute inverse of D once for efficiency\n",
    "        self.D_inv = torch.inverse(D)\n",
    "        \n",
    "        # Initialize solution placeholders\n",
    "        self.S_grid = None  # Will be populated when solve_ricatti is called\n",
    "        self.int_term_grid = None  # Will store the integral term\n",
    "        \n",
    "    def ricatti_rhs(self, t: float, S_flat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Right-hand side of the Ricatti ODE: S'(t) = S(t)MD^(-1)M^TS(t) - H^TS(t) - S(t)H - C\n",
    "        \n",
    "        Args:\n",
    "            t: Time\n",
    "            S_flat: Flattened S matrix\n",
    "            \n",
    "        Returns:\n",
    "            Flattened derivative of S\n",
    "        \"\"\"\n",
    "        # Reshape S from flattened form\n",
    "        S = torch.tensor(S_flat.reshape(self.d, self.d), dtype=torch.float64)\n",
    "        \n",
    "        # Compute right-hand side\n",
    "        term1 = S @ self.M @ self.D_inv @ self.M.T @ S\n",
    "        term2 = self.H.T @ S\n",
    "        term3 = S @ self.H\n",
    "        term4 = self.C\n",
    "        \n",
    "        # Compute derivative\n",
    "        dS = term1 - term2 - term3 - term4\n",
    "        \n",
    "        # Return flattened result\n",
    "        return dS.flatten().numpy()\n",
    "    \n",
    "    def solve_ricatti(self) -> None:\n",
    "        \"\"\"\n",
    "        Solve the Ricatti ODE using scipy's solve_ivp for high accuracy.\n",
    "        \"\"\"\n",
    "        # Convert matrices to double precision if they aren't already\n",
    "        self.H = self.H.to(torch.float64)\n",
    "        self.M = self.M.to(torch.float64)\n",
    "        self.sigma = self.sigma.to(torch.float64)\n",
    "        self.C = self.C.to(torch.float64)\n",
    "        self.D = self.D.to(torch.float64)\n",
    "        self.R = self.R.to(torch.float64)\n",
    "        self.D_inv = self.D_inv.to(torch.float64)\n",
    "        \n",
    "        # Terminal condition: S(T) = R\n",
    "        S_T_flat = self.R.flatten().numpy()\n",
    "        \n",
    "        # Time points for ODE solver (reversed for backward integration)\n",
    "        t_points = self.time_grid.numpy()\n",
    "        t_reversed = self.T - t_points[::-1]\n",
    "        \n",
    "        # Solve the ODE backward in time (from T to 0)\n",
    "        solution = solve_ivp(\n",
    "            lambda t, y: -self.ricatti_rhs(self.T - t, y),  # Negative for backward integration\n",
    "            [0, self.T],\n",
    "            S_T_flat,\n",
    "            t_eval=t_reversed,\n",
    "            method='RK45',\n",
    "            rtol=1e-12,\n",
    "            atol=1e-12\n",
    "        )\n",
    "        \n",
    "        # Convert solution back to PyTorch tensors and reshape\n",
    "        S_values = solution.y.T\n",
    "        S_matrices = [S.reshape(self.d, self.d) for S in S_values]\n",
    "        S_matrices.reverse()  # Reverse back to forward time\n",
    "        \n",
    "        self.S_grid = torch.tensor(S_matrices, dtype=torch.float64)\n",
    "        \n",
    "        # Compute integral term for value function\n",
    "        self.compute_integral_term()\n",
    "    \n",
    "    def compute_integral_term(self) -> None:\n",
    "        \"\"\"\n",
    "        Compute the integral term for the value function: int_t^T tr(sigma sigma^T S(r)) dr\n",
    "        \"\"\"\n",
    "        if self.S_grid is None:\n",
    "            self.solve_ricatti()\n",
    "        \n",
    "        # Compute trace term at each time point\n",
    "        trace_terms = torch.zeros(len(self.time_grid))\n",
    "        sigma_sigma_T = self.sigma @ self.sigma.T\n",
    "        \n",
    "        for i in range(len(self.time_grid)):\n",
    "            trace_terms[i] = torch.trace(sigma_sigma_T @ self.S_grid[i])\n",
    "        \n",
    "        # Compute integral using trapezoidal rule (backward from T)\n",
    "        integral_term = torch.zeros(len(self.time_grid))\n",
    "        \n",
    "        for i in range(len(self.time_grid) - 1, 0, -1):\n",
    "            dt = self.time_grid[i] - self.time_grid[i-1]\n",
    "            integral_term[i-1] = integral_term[i] + 0.5 * (trace_terms[i] + trace_terms[i-1]) * dt\n",
    "        \n",
    "        self.int_term_grid = integral_term\n",
    "    \n",
    "    def get_S_at_time(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get the S matrix at a given time by finding the nearest time point in the grid.\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            \n",
    "        Returns:\n",
    "            S matrices at the specified times (batch x d x d)\n",
    "        \"\"\"\n",
    "        if self.S_grid is None:\n",
    "            self.solve_ricatti()\n",
    "            \n",
    "        # Find indices of nearest time points for each t\n",
    "        indices = torch.argmin(torch.abs(t.unsqueeze(1) - self.time_grid.unsqueeze(0)), dim=1)\n",
    "        \n",
    "        # Get the corresponding S matrices\n",
    "        return self.S_grid[indices]\n",
    "    \n",
    "    def get_integral_term_at_time(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get the integral term at a given time by finding the nearest time point in the grid.\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            \n",
    "        Returns:\n",
    "            Integral terms at the specified times (batch)\n",
    "        \"\"\"\n",
    "        if self.int_term_grid is None:\n",
    "            self.compute_integral_term()\n",
    "            \n",
    "        # Find indices of nearest time points for each t\n",
    "        indices = torch.argmin(torch.abs(t.unsqueeze(1) - self.time_grid.unsqueeze(0)), dim=1)\n",
    "        \n",
    "        # Get the corresponding integral terms\n",
    "        return self.int_term_grid[indices]\n",
    "    \n",
    "    def value_function(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the value function v(t, x) = x^T S(t) x + int_t^T tr(sigma sigma^T S(r)) dr\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            x: State tensor (batch x d)\n",
    "            \n",
    "        Returns:\n",
    "            Value function at (t, x) (batch)\n",
    "        \"\"\"\n",
    "        # Get S matrices at the specified times\n",
    "        S_matrices = self.get_S_at_time(t)\n",
    "        \n",
    "        # Compute quadratic term x^T S(t) x\n",
    "        batch_size = x.shape[0]\n",
    "        values = torch.zeros(batch_size, device=x.device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            values[i] = x[i] @ S_matrices[i] @ x[i]\n",
    "        \n",
    "        # Add integral term\n",
    "        values = values + self.get_integral_term_at_time(t)\n",
    "        \n",
    "        return values\n",
    "    \n",
    "    def optimal_control(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the optimal control a(t, x) = -D^(-1)M^TS(t)x\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            x: State tensor (batch x d)\n",
    "            \n",
    "        Returns:\n",
    "            Optimal control at (t, x) (batch x m)\n",
    "        \"\"\"\n",
    "        # Get S matrices at the specified times\n",
    "        S_matrices = self.get_S_at_time(t)\n",
    "        \n",
    "        # Compute optimal control for each (t, x) pair\n",
    "        batch_size = x.shape[0]\n",
    "        controls = torch.zeros((batch_size, self.m), device=x.device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            controls[i] = -self.D_inv @ self.M.T @ S_matrices[i] @ x[i]\n",
    "        \n",
    "        return controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_sde_explicit(lqr: LQR, x0: torch.Tensor, num_steps: int, num_samples: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Simulate the controlled SDE using the explicit scheme.\n",
    "    \n",
    "    Args:\n",
    "        lqr: LQR instance\n",
    "        x0: Initial state (batch x d)\n",
    "        num_steps: Number of time steps\n",
    "        num_samples: Number of Monte Carlo samples\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (trajectory, cost)\n",
    "    \"\"\"\n",
    "    dt = lqr.T / num_steps\n",
    "    t_grid = torch.linspace(0, lqr.T, num_steps + 1, dtype=torch.float64)\n",
    "    \n",
    "    batch_size = x0.shape[0]\n",
    "    d = lqr.d\n",
    "    \n",
    "    # Ensure all tensors are double precision\n",
    "    H = lqr.H.to(torch.float64)\n",
    "    M = lqr.M.to(torch.float64)\n",
    "    sigma = lqr.sigma.to(torch.float64)\n",
    "    C = lqr.C.to(torch.float64)\n",
    "    D = lqr.D.to(torch.float64)\n",
    "    \n",
    "    # Initialize trajectories and costs\n",
    "    X = torch.zeros((num_samples, batch_size, num_steps + 1, d), dtype=torch.float64)\n",
    "    X[:, :, 0, :] = x0.unsqueeze(0).repeat(num_samples, 1, 1)\n",
    "    \n",
    "    costs = torch.zeros((num_samples, batch_size), dtype=torch.float64)\n",
    "    \n",
    "    # Generate Brownian increments\n",
    "    dW = torch.randn((num_samples, batch_size, num_steps, sigma.shape[1]), dtype=torch.float64) * np.sqrt(dt)\n",
    "    \n",
    "    # Simulate trajectories\n",
    "    for n in range(num_steps):\n",
    "        t_n = t_grid[n]\n",
    "        X_n = X[:, :, n, :]\n",
    "        \n",
    "        # Reshape for batch processing\n",
    "        X_n_flat = X_n.reshape(-1, d)\n",
    "        t_flat = t_n.repeat(num_samples * batch_size)\n",
    "        \n",
    "        # Compute optimal control\n",
    "        control_flat = lqr.optimal_control(t_flat, X_n_flat).to(torch.float64)\n",
    "        control = control_flat.reshape(num_samples, batch_size, lqr.m)\n",
    "        \n",
    "        # Compute drift and apply update for each sample\n",
    "        for i in range(num_samples):\n",
    "            for j in range(batch_size):\n",
    "                # Compute drift term: HX + Ma\n",
    "                drift = H @ X_n[i, j] + M @ control[i, j]\n",
    "                \n",
    "                # Update state using explicit scheme\n",
    "                X[i, j, n+1] = X_n[i, j] + drift * dt + sigma @ dW[i, j, n]\n",
    "                \n",
    "                # Accumulate running cost\n",
    "                state_cost = X_n[i, j] @ C @ X_n[i, j]\n",
    "                control_cost = control[i, j] @ D @ control[i, j]\n",
    "                costs[i, j] += (state_cost + control_cost) * dt\n",
    "    \n",
    "    # Add terminal cost\n",
    "    X_T = X[:, :, -1, :]\n",
    "    for i in range(num_samples):\n",
    "        for j in range(batch_size):\n",
    "            terminal_cost = X_T[i, j] @ lqr.R @ X_T[i, j]\n",
    "            costs[i, j] += terminal_cost\n",
    "    \n",
    "    return X, costs\n",
    "\n",
    "def simulate_sde_implicit(lqr: LQR, x0: torch.Tensor, num_steps: int, num_samples: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Simulate the controlled SDE using the implicit scheme.\n",
    "    \n",
    "    Args:\n",
    "        lqr: LQR instance\n",
    "        x0: Initial state (batch x d)\n",
    "        num_steps: Number of time steps\n",
    "        num_samples: Number of Monte Carlo samples\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (trajectory, cost)\n",
    "    \"\"\"\n",
    "    dt = lqr.T / num_steps\n",
    "    t_grid = torch.linspace(0, lqr.T, num_steps + 1, dtype=torch.float64)\n",
    "    \n",
    "    batch_size = x0.shape[0]\n",
    "    d = lqr.d\n",
    "    \n",
    "    # Ensure all tensors are double precision\n",
    "    H = lqr.H.to(torch.float64)\n",
    "    M = lqr.M.to(torch.float64)\n",
    "    sigma = lqr.sigma.to(torch.float64)\n",
    "    C = lqr.C.to(torch.float64)\n",
    "    D = lqr.D.to(torch.float64)\n",
    "    D_inv = lqr.D_inv.to(torch.float64)\n",
    "    \n",
    "    # Initialize trajectories and costs\n",
    "    X = torch.zeros((num_samples, batch_size, num_steps + 1, d), dtype=torch.float64)\n",
    "    X[:, :, 0, :] = x0.unsqueeze(0).repeat(num_samples, 1, 1)\n",
    "    \n",
    "    costs = torch.zeros((num_samples, batch_size), dtype=torch.float64)\n",
    "    \n",
    "    # Generate Brownian increments\n",
    "    dW = torch.randn((num_samples, batch_size, num_steps, sigma.shape[1]), dtype=torch.float64) * np.sqrt(dt)\n",
    "    \n",
    "    # Identity matrix for linear system\n",
    "    I = torch.eye(d, dtype=torch.float64)\n",
    "    \n",
    "    # Simulate trajectories\n",
    "    for n in range(num_steps):\n",
    "        t_n = t_grid[n]\n",
    "        t_np1 = t_grid[n+1] \n",
    "        X_n = X[:, :, n, :]\n",
    "        \n",
    "        # Reshape for batch processing for cost calculation\n",
    "        X_n_flat = X_n.reshape(-1, d)\n",
    "        t_flat = t_n.repeat(num_samples * batch_size)\n",
    "        \n",
    "        # Compute optimal control for cost calculation\n",
    "        control_flat = lqr.optimal_control(t_flat, X_n_flat).to(torch.float64)\n",
    "        control = control_flat.reshape(num_samples, batch_size, lqr.m)\n",
    "        \n",
    "        # For implicit scheme, we need to solve a linear system for each sample\n",
    "        S_np1 = lqr.get_S_at_time(torch.tensor([t_np1], dtype=torch.float64))[0].to(torch.float64)\n",
    "        \n",
    "        # Construct system matrix: (I - dt*H + dt*M*D^(-1)*M^T*S(t_{n+1}))\n",
    "        MD_inv_MT = M @ D_inv @ M.T\n",
    "        A = I - dt * H + dt * MD_inv_MT @ S_np1\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            for j in range(batch_size):\n",
    "                # Compute right-hand side: X_n + sigma * dW_n\n",
    "                b = X_n[i, j] + sigma @ dW[i, j, n]\n",
    "                \n",
    "                # Solve the linear system: A * X_{n+1} = b\n",
    "                X[i, j, n+1] = torch.linalg.solve(A, b)\n",
    "                \n",
    "                # Accumulate running cost\n",
    "                state_cost = X_n[i, j] @ C @ X_n[i, j]\n",
    "                control_cost = control[i, j] @ D @ control[i, j]\n",
    "                costs[i, j] += (state_cost + control_cost) * dt\n",
    "    \n",
    "    # Add terminal cost\n",
    "    X_T = X[:, :, -1, :]\n",
    "    for i in range(num_samples):\n",
    "        for j in range(batch_size):\n",
    "            terminal_cost = X_T[i, j] @ lqr.R @ X_T[i, j]\n",
    "            costs[i, j] += terminal_cost\n",
    "    \n",
    "    return X, costs\n",
    "\n",
    "def run_monte_carlo_tests(lqr: LQR, x0: torch.Tensor, scheme: str = 'explicit') -> None:\n",
    "    \"\"\"\n",
    "    Run Monte Carlo tests for the LQR problem.\n",
    "    \n",
    "    Args:\n",
    "        lqr: LQR instance\n",
    "        x0: Initial states to test\n",
    "        scheme: 'explicit' or 'implicit'\n",
    "    \"\"\"\n",
    "    # 1. Test varying time steps with fixed number of samples\n",
    "    num_samples = 2500 # should be 10k\n",
    "    time_steps_list = [2**i for i in range(1, 10)] # should be up to 12\n",
    "    time_step_errors = []\n",
    "    \n",
    "    # Compute true value function at t=0, x=x0\n",
    "    t0 = torch.zeros(x0.shape[0], dtype=torch.float64)\n",
    "    true_values = lqr.value_function(t0, x0)\n",
    "    \n",
    "    print(f\"\\n--- Testing convergence for {scheme} scheme ---\")\n",
    "    print(\"Varying time steps...\")\n",
    "    \n",
    "    for num_steps in time_steps_list:\n",
    "        print(f\"Running with {num_steps} time steps...\")\n",
    "        \n",
    "        # Run simulation with current parameters\n",
    "        if scheme == 'explicit':\n",
    "            _, costs = simulate_sde_explicit(lqr, x0, num_steps, num_samples)\n",
    "        else:\n",
    "            _, costs = simulate_sde_implicit(lqr, x0, num_steps, num_samples)\n",
    "        \n",
    "        # Compute mean cost\n",
    "        mean_costs = costs.mean(dim=0)\n",
    "        \n",
    "        # Compute error\n",
    "        error = torch.abs(mean_costs - true_values).mean().item()\n",
    "        time_step_errors.append(error)\n",
    "        \n",
    "        print(f\"  Error: {error:.6f}\")\n",
    "    \n",
    "    # Plot time step convergence\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.loglog(time_steps_list, time_step_errors, 'o-', label=f'{scheme.capitalize()} Scheme')\n",
    "    \n",
    "    # Add trend line \n",
    "    if scheme == 'explicit':\n",
    "        ref_line = [time_step_errors[0] * (time_steps_list[0] / n) for n in time_steps_list]\n",
    "        plt.loglog(time_steps_list, ref_line, '--', label='O(1/N)')\n",
    "    else:\n",
    "        ref_line = [time_step_errors[0] * (time_steps_list[0] / n)**2 for n in time_steps_list]\n",
    "        plt.loglog(time_steps_list, ref_line, '--', label='O(1/N²)')\n",
    "    \n",
    "    plt.xlabel('Number of Time Steps (N)')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.title(f'Convergence with Varying Time Steps ({scheme.capitalize()} Scheme)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Test varying number of samples with fixed number of time steps\n",
    "    num_steps = 5000  # Large number for accuracy should be 10k\n",
    "    sample_counts = [2 * 4**i for i in range(5)] # should be range(6)\n",
    "    sample_errors = []\n",
    "    \n",
    "    print(\"\\nVarying sample counts...\")\n",
    "    \n",
    "    for num_samples in sample_counts:\n",
    "        print(f\"Running with {num_samples} samples...\")\n",
    "        \n",
    "        # Run simulation with current parameters\n",
    "        if scheme == 'explicit':\n",
    "            _, costs = simulate_sde_explicit(lqr, x0, num_steps, num_samples)\n",
    "        else:\n",
    "            _, costs = simulate_sde_implicit(lqr, x0, num_steps, num_samples)\n",
    "        \n",
    "        # Compute mean cost\n",
    "        mean_costs = costs.mean(dim=0)\n",
    "        \n",
    "        # Compute error\n",
    "        error = torch.abs(mean_costs - true_values).mean().item()\n",
    "        sample_errors.append(error)\n",
    "        \n",
    "        print(f\"  Error: {error:.6f}\")\n",
    "    \n",
    "    # Plot sample count convergence\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.loglog(sample_counts, sample_errors, 'o-', label=f'{scheme.capitalize()} Scheme')\n",
    "    \n",
    "    # Add trend line (should be O(1/sqrt(M)) for Monte Carlo)\n",
    "    ref_line = [sample_errors[0] * np.sqrt(sample_counts[0] / n) for n in sample_counts]\n",
    "    plt.loglog(sample_counts, ref_line, '--', label='O(1/√M)')\n",
    "    \n",
    "    plt.xlabel('Number of Monte Carlo Samples (M)')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.title(f'Convergence with Varying Sample Counts ({scheme.capitalize()} Scheme)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def run_monte_carlo_comparison(lqr: LQR, x0: torch.Tensor) -> None:\n",
    "    \"\"\"\n",
    "    Run Monte Carlo tests comparing explicit and implicit schemes on the same plots.\n",
    "    \n",
    "    Args:\n",
    "        lqr: LQR instance\n",
    "        x0: Initial states to test\n",
    "    \"\"\"\n",
    "    # 1. Test varying time steps with fixed number of samples\n",
    "    num_samples = 2500 # Should be 10k\n",
    "    time_steps_list = [2**i for i in range(1, 10)] # Should be 12\n",
    "    \n",
    "    # Compute true value function at t=0, x=x0\n",
    "    t0 = torch.zeros(x0.shape[0], dtype=torch.float64)\n",
    "    true_values = lqr.value_function(t0, x0)\n",
    "    \n",
    "    # Arrays to store results for both schemes\n",
    "    explicit_time_errors = []\n",
    "    implicit_time_errors = []\n",
    "    \n",
    "    print(\"\\n--- Testing convergence for both schemes with varying time steps ---\")\n",
    "    \n",
    "    for num_steps in time_steps_list:\n",
    "        print(f\"Running with {num_steps} time steps...\")\n",
    "        \n",
    "        # Run simulation with explicit scheme\n",
    "        _, costs_explicit = simulate_sde_explicit(lqr, x0, num_steps, num_samples)\n",
    "        mean_costs_explicit = costs_explicit.mean(dim=0)\n",
    "        error_explicit = torch.abs(mean_costs_explicit - true_values).mean().item()\n",
    "        explicit_time_errors.append(error_explicit)\n",
    "        \n",
    "        # Run simulation with implicit scheme\n",
    "        _, costs_implicit = simulate_sde_implicit(lqr, x0, num_steps, num_samples)\n",
    "        mean_costs_implicit = costs_implicit.mean(dim=0)\n",
    "        error_implicit = torch.abs(mean_costs_implicit - true_values).mean().item()\n",
    "        implicit_time_errors.append(error_implicit)\n",
    "        \n",
    "        print(f\"  Explicit scheme error: {error_explicit:.6f}\")\n",
    "        print(f\"  Implicit scheme error: {error_implicit:.6f}\")\n",
    "    \n",
    "    # Plot time step convergence for both schemes on the same graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.loglog(time_steps_list, explicit_time_errors, 'o-', color='blue', label='Explicit Scheme')\n",
    "    plt.loglog(time_steps_list, implicit_time_errors, 's-', color='red', label='Implicit Scheme')\n",
    "    \n",
    "    # Add reference lines for O(1/N) and O(1/N²) convergence\n",
    "    ref_line_order1 = [explicit_time_errors[0] * (time_steps_list[0] / n) for n in time_steps_list]\n",
    "    ref_line_order2 = [explicit_time_errors[0] * (time_steps_list[0] / n)**2 for n in time_steps_list]\n",
    "    \n",
    "    plt.loglog(time_steps_list, ref_line_order1, '--', color='blue', alpha=0.5, label='O(1/N)')\n",
    "    plt.loglog(time_steps_list, ref_line_order2, '--', color='red', alpha=0.5, label='O(1/N²)')\n",
    "    \n",
    "    plt.xlabel('Number of Time Steps (N)')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.title('Convergence with Varying Time Steps - Comparison of Schemes')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Test varying number of samples with fixed number of time steps\n",
    "    num_steps = 5000  # Large number for accuracy should be 10k\n",
    "    sample_counts = [2 * 4**i for i in range(5)] # Should be 6\n",
    "    \n",
    "    # Arrays to store results for both schemes\n",
    "    explicit_sample_errors = []\n",
    "    implicit_sample_errors = []\n",
    "    \n",
    "    print(\"\\n--- Testing convergence for both schemes with varying sample counts ---\")\n",
    "    \n",
    "    for num_samples in sample_counts:\n",
    "        print(f\"Running with {num_samples} samples...\")\n",
    "        \n",
    "        # Run simulation with explicit scheme\n",
    "        _, costs_explicit = simulate_sde_explicit(lqr, x0, num_steps, num_samples)\n",
    "        mean_costs_explicit = costs_explicit.mean(dim=0)\n",
    "        error_explicit = torch.abs(mean_costs_explicit - true_values).mean().item()\n",
    "        explicit_sample_errors.append(error_explicit)\n",
    "        \n",
    "        # Run simulation with implicit scheme\n",
    "        _, costs_implicit = simulate_sde_implicit(lqr, x0, num_steps, num_samples)\n",
    "        mean_costs_implicit = costs_implicit.mean(dim=0)\n",
    "        error_implicit = torch.abs(mean_costs_implicit - true_values).mean().item()\n",
    "        implicit_sample_errors.append(error_implicit)\n",
    "        \n",
    "        print(f\"  Explicit scheme error: {error_explicit:.6f}\")\n",
    "        print(f\"  Implicit scheme error: {error_implicit:.6f}\")\n",
    "    \n",
    "    # Plot sample count convergence for both schemes on the same graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.loglog(sample_counts, explicit_sample_errors, 'o-', color='blue', label='Explicit Scheme')\n",
    "    plt.loglog(sample_counts, implicit_sample_errors, 's-', color='red', label='Implicit Scheme')\n",
    "    \n",
    "    # Add reference line for O(1/sqrt(M)) convergence (Monte Carlo rate)\n",
    "    ref_line_mc = [explicit_sample_errors[0] * np.sqrt(sample_counts[0] / n) for n in sample_counts]\n",
    "    plt.loglog(sample_counts, ref_line_mc, '--', color='green', label='O(1/√M)')\n",
    "    \n",
    "    plt.xlabel('Number of Monte Carlo Samples (M)')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.title('Convergence with Varying Sample Counts - Comparison of Schemes')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S(0):\n",
      " tensor([[ 0.3698, -0.1901],\n",
      "        [-0.1901,  0.5432]], dtype=torch.float64)\n",
      "S(T/2):\n",
      " tensor([[ 0.4916, -0.3262],\n",
      "        [-0.3262,  0.7793]], dtype=torch.float64)\n",
      "S(T):\n",
      " tensor([[10.,  3.],\n",
      "        [ 3., 10.]], dtype=torch.float64)\n",
      "\n",
      "Value function at t=0:\n",
      "v(0, [1.0, 1.0]) = 0.782136\n",
      "v(0, [2.0, 2.0]) = 2.380319\n",
      "\n",
      "Optimal control at t=0:\n",
      "u(0, [1.0, 1.0]) = [-1.2770086526870728, -5.199576377868652]\n",
      "u(0, [2.0, 2.0]) = [-2.5540173053741455, -10.399152755737305]\n",
      "\n",
      "--- Testing convergence for explicit scheme ---\n",
      "Varying time steps...\n",
      "Running with 2 time steps...\n",
      "  Error: 5.045467\n",
      "Running with 4 time steps...\n",
      "  Error: 1.133618\n",
      "Running with 8 time steps...\n",
      "  Error: 0.491513\n",
      "Running with 16 time steps...\n",
      "  Error: 0.224379\n",
      "Running with 32 time steps...\n",
      "  Error: 0.103346\n",
      "Running with 64 time steps...\n",
      "  Error: 0.053396\n",
      "Running with 128 time steps...\n",
      "  Error: 0.024888\n",
      "Running with 256 time steps...\n",
      "  Error: 0.012114\n",
      "Running with 512 time steps...\n",
      "  Error: 0.004284\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Varying sample counts...\n",
      "Running with 2 samples...\n",
      "  Error: 0.251542\n",
      "Running with 8 samples...\n",
      "  Error: 0.129240\n",
      "Running with 32 samples...\n",
      "  Error: 0.038412\n",
      "Running with 128 samples...\n",
      "  Error: 0.019944\n",
      "Running with 512 samples...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mu(0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx0[i].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mu0[i].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Run Monte Carlo tests for explicit scheme\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mrun_monte_carlo_tests\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlqr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheme\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mexplicit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Run Monte Carlo tests for implicit scheme\u001b[39;00m\n\u001b[32m     48\u001b[39m run_monte_carlo_tests(lqr, x0, scheme=\u001b[33m'\u001b[39m\u001b[33mimplicit\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 224\u001b[39m, in \u001b[36mrun_monte_carlo_tests\u001b[39m\u001b[34m(lqr, x0, scheme)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# Run simulation with current parameters\u001b[39;00m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scheme == \u001b[33m'\u001b[39m\u001b[33mexplicit\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     _, costs = \u001b[43msimulate_sde_explicit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlqr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    226\u001b[39m     _, costs = simulate_sde_implicit(lqr, x0, num_steps, num_samples)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36msimulate_sde_explicit\u001b[39m\u001b[34m(lqr, x0, num_steps, num_samples)\u001b[39m\n\u001b[32m     59\u001b[39m             state_cost = X_n[i, j] @ C @ X_n[i, j]\n\u001b[32m     60\u001b[39m             control_cost = control[i, j] @ D @ control[i, j]\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m             costs[i, j] += (state_cost + control_cost) * dt\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Add terminal cost\u001b[39;00m\n\u001b[32m     64\u001b[39m X_T = X[:, :, -\u001b[32m1\u001b[39m, :]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def main_init():\n",
    "    # Set the problem matrices as specified in Figure 1\n",
    "    H = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64) * 0.5\n",
    "    M = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64)\n",
    "    sigma = torch.eye(2, dtype=torch.float64) * 0.5\n",
    "    C = torch.tensor([[1.0, 0.1], [0.1, 1.0]], dtype=torch.float64) * 1.0\n",
    "    D = torch.tensor([[1.0, 0.1], [0.1, 1.0]], dtype=torch.float64) * 0.1\n",
    "    R = torch.tensor([[1.0, 0.3], [0.3, 1.0]], dtype=torch.float64) * 10.0\n",
    "\n",
    "    # Set the terminal time and time grid\n",
    "    T = 0.5\n",
    "    grid_size = 1000\n",
    "    time_grid = torch.linspace(0, T, grid_size, dtype=torch.float64)\n",
    "\n",
    "    # Create LQR instance\n",
    "    lqr = LQR(H, M, sigma, C, D, R, T, time_grid)\n",
    "\n",
    "    # Solve Ricatti ODE\n",
    "    lqr.solve_ricatti()\n",
    "\n",
    "    # Print S matrices at key time points\n",
    "    print(\"S(0):\\n\", lqr.S_grid[0])\n",
    "    print(\"S(T/2):\\n\", lqr.S_grid[grid_size//2])\n",
    "    print(\"S(T):\\n\", lqr.S_grid[-1])\n",
    "\n",
    "    # Test points\n",
    "    x0 = torch.tensor([\n",
    "        [1.0, 1.0],\n",
    "        [2.0, 2.0]\n",
    "    ], dtype=torch.float64)\n",
    "\n",
    "    # Compute value function at test points\n",
    "    t0 = torch.zeros(x0.shape[0], dtype=torch.float64)\n",
    "    v0 = lqr.value_function(t0, x0)\n",
    "    print(\"\\nValue function at t=0:\")\n",
    "    for i in range(x0.shape[0]):\n",
    "        print(f\"v(0, {x0[i].tolist()}) = {v0[i].item():.6f}\")\n",
    "\n",
    "    # Get the optimal control for the test points\n",
    "    u0 = lqr.optimal_control(t0, x0)\n",
    "    print(\"\\nOptimal control at t=0:\")\n",
    "    for i in range(x0.shape[0]):\n",
    "        print(f\"u(0, {x0[i].tolist()}) = {u0[i].tolist()}\")\n",
    "\n",
    "    # Run Monte Carlo tests for explicit scheme\n",
    "    run_monte_carlo_tests(lqr, x0, scheme='explicit')\n",
    "\n",
    "    # Run Monte Carlo tests for implicit scheme\n",
    "    run_monte_carlo_tests(lqr, x0, scheme='implicit')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S(0):\n",
      " tensor([[ 0.3698, -0.1901],\n",
      "        [-0.1901,  0.5432]], dtype=torch.float64)\n",
      "S(T/2):\n",
      " tensor([[ 0.4916, -0.3262],\n",
      "        [-0.3262,  0.7793]], dtype=torch.float64)\n",
      "S(T):\n",
      " tensor([[10.,  3.],\n",
      "        [ 3., 10.]], dtype=torch.float64)\n",
      "\n",
      "Value function at t=0:\n",
      "v(0, [1.0, 1.0]) = 0.782136\n",
      "v(0, [2.0, 2.0]) = 2.380319\n",
      "\n",
      "Optimal control at t=0:\n",
      "u(0, [1.0, 1.0]) = [-1.2770086526870728, -5.199576377868652]\n",
      "u(0, [2.0, 2.0]) = [-2.5540173053741455, -10.399152755737305]\n",
      "\n",
      "--- Testing convergence for both schemes with varying time steps ---\n",
      "Running with 2 time steps...\n",
      "  Explicit scheme error: 4.944351\n",
      "  Implicit scheme error: 2.352849\n",
      "Running with 4 time steps...\n",
      "  Explicit scheme error: 1.136594\n",
      "  Implicit scheme error: 1.085881\n",
      "Running with 8 time steps...\n",
      "  Explicit scheme error: 0.489024\n",
      "  Implicit scheme error: 0.482751\n",
      "Running with 16 time steps...\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Set the problem matrices as specified in Figure 1\n",
    "    H = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64) * 0.5\n",
    "    M = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64)\n",
    "    sigma = torch.eye(2, dtype=torch.float64) * 0.5\n",
    "    C = torch.tensor([[1.0, 0.1], [0.1, 1.0]], dtype=torch.float64) * 1.0\n",
    "    D = torch.tensor([[1.0, 0.1], [0.1, 1.0]], dtype=torch.float64) * 0.1\n",
    "    R = torch.tensor([[1.0, 0.3], [0.3, 1.0]], dtype=torch.float64) * 10.0\n",
    "    \n",
    "    # Set the terminal time and time grid\n",
    "    T = 0.5\n",
    "    grid_size = 1000\n",
    "    time_grid = torch.linspace(0, T, grid_size, dtype=torch.float64)\n",
    "    \n",
    "    # Create LQR instance\n",
    "    lqr = LQR(H, M, sigma, C, D, R, T, time_grid)\n",
    "    \n",
    "    # Solve Ricatti ODE\n",
    "    lqr.solve_ricatti()\n",
    "    \n",
    "    # Print S matrices at key time points\n",
    "    print(\"S(0):\\n\", lqr.S_grid[0])\n",
    "    print(\"S(T/2):\\n\", lqr.S_grid[grid_size//2])\n",
    "    print(\"S(T):\\n\", lqr.S_grid[-1])\n",
    "    \n",
    "    # Test points\n",
    "    x0 = torch.tensor([\n",
    "        [1.0, 1.0],\n",
    "        [2.0, 2.0]\n",
    "    ], dtype=torch.float64)\n",
    "    \n",
    "    # Compute value function at test points\n",
    "    t0 = torch.zeros(x0.shape[0], dtype=torch.float64)\n",
    "    v0 = lqr.value_function(t0, x0)\n",
    "    print(\"\\nValue function at t=0:\")\n",
    "    for i in range(x0.shape[0]):\n",
    "        print(f\"v(0, {x0[i].tolist()}) = {v0[i].item():.6f}\")\n",
    "    \n",
    "    # Get the optimal control for the test points\n",
    "    u0 = lqr.optimal_control(t0, x0)\n",
    "    print(\"\\nOptimal control at t=0:\")\n",
    "    for i in range(x0.shape[0]):\n",
    "        print(f\"u(0, {x0[i].tolist()}) = {u0[i].tolist()}\")\n",
    "    \n",
    "    # Run Monte Carlo comparison for both schemes\n",
    "    run_monte_carlo_comparison(lqr, x0)\n",
    "    \n",
    "    # Additionally, compare trajectories from both schemes\n",
    "    compare_scheme_trajectories(lqr, x0)\n",
    "\n",
    "def compare_scheme_trajectories(lqr: LQR, x0: torch.Tensor) -> None:\n",
    "    \"\"\"\n",
    "    Compare and plot trajectories from explicit and implicit schemes.\n",
    "    \n",
    "    Args:\n",
    "        lqr: LQR instance\n",
    "        x0: Initial states to test\n",
    "    \"\"\"\n",
    "    # Set simulation parameters\n",
    "    num_steps = 1000\n",
    "    num_samples = 1  # Just one sample for visualization\n",
    "    \n",
    "    # Simulate trajectories using both schemes with the same noise\n",
    "    # Generate Brownian motion for consistency between schemes\n",
    "    dt = lqr.T / num_steps\n",
    "    dW = torch.randn((num_samples, x0.shape[0], num_steps, lqr.sigma.shape[1]), \n",
    "                    dtype=torch.float64) * np.sqrt(dt)\n",
    "    \n",
    "    # Run simulations with shared noise\n",
    "    X_explicit, _ = simulate_sde_explicit(lqr, x0, num_steps, num_samples, fixed_noise=dW)\n",
    "    X_implicit, _ = simulate_sde_implicit(lqr, x0, num_steps, num_samples, fixed_noise=dW)\n",
    "    \n",
    "    # Plot trajectories for each initial state\n",
    "    fig, axes = plt.subplots(1, x0.shape[0], figsize=(15, 5))\n",
    "    \n",
    "    for i in range(x0.shape[0]):\n",
    "        ax = axes[i] if x0.shape[0] > 1 else axes\n",
    "        \n",
    "        # Extract trajectories for this initial state\n",
    "        traj_explicit = X_explicit[0, i].cpu().numpy()  # First sample, ith initial state\n",
    "        traj_implicit = X_implicit[0, i].cpu().numpy()\n",
    "        \n",
    "        # Plot trajectories\n",
    "        ax.plot(traj_explicit[:, 0], traj_explicit[:, 1], 'b-', label='Explicit Scheme')\n",
    "        ax.plot(traj_implicit[:, 0], traj_implicit[:, 1], 'r-', label='Implicit Scheme')\n",
    "        ax.scatter([x0[i, 0]], [x0[i, 1]], c='g', s=100, marker='o', label='Initial State')\n",
    "        \n",
    "        ax.set_title(f'Trajectories from Initial State {x0[i].tolist()}')\n",
    "        ax.set_xlabel('X1')\n",
    "        ax.set_ylabel('X2')\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Update the simulation functions to accept fixed noise\n",
    "def simulate_sde_explicit(lqr: LQR, x0: torch.Tensor, num_steps: int, num_samples: int, \n",
    "                         fixed_noise: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Simulate the controlled SDE using the explicit scheme with optional fixed noise.\n",
    "    \n",
    "    Args:\n",
    "        lqr: LQR instance\n",
    "        x0: Initial state (batch x d)\n",
    "        num_steps: Number of time steps\n",
    "        num_samples: Number of Monte Carlo samples\n",
    "        fixed_noise: Optional fixed Brownian increments for consistent comparisons\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (trajectory, cost)\n",
    "    \"\"\"\n",
    "    dt = lqr.T / num_steps\n",
    "    t_grid = torch.linspace(0, lqr.T, num_steps + 1, dtype=torch.float64)\n",
    "    \n",
    "    batch_size = x0.shape[0]\n",
    "    d = lqr.d\n",
    "    \n",
    "    # Ensure all tensors are double precision\n",
    "    H = lqr.H.to(torch.float64)\n",
    "    M = lqr.M.to(torch.float64)\n",
    "    sigma = lqr.sigma.to(torch.float64)\n",
    "    C = lqr.C.to(torch.float64)\n",
    "    D = lqr.D.to(torch.float64)\n",
    "    \n",
    "    # Initialize trajectories and costs\n",
    "    X = torch.zeros((num_samples, batch_size, num_steps + 1, d), dtype=torch.float64)\n",
    "    X[:, :, 0, :] = x0.unsqueeze(0).repeat(num_samples, 1, 1)\n",
    "    \n",
    "    costs = torch.zeros((num_samples, batch_size), dtype=torch.float64)\n",
    "    \n",
    "    # Generate Brownian increments or use fixed ones\n",
    "    if fixed_noise is None:\n",
    "        dW = torch.randn((num_samples, batch_size, num_steps, sigma.shape[1]), \n",
    "                        dtype=torch.float64) * np.sqrt(dt)\n",
    "    else:\n",
    "        dW = fixed_noise\n",
    "    \n",
    "    # Simulate trajectories\n",
    "    for n in range(num_steps):\n",
    "        t_n = t_grid[n]\n",
    "        X_n = X[:, :, n, :]\n",
    "        \n",
    "        # Reshape for batch processing\n",
    "        X_n_flat = X_n.reshape(-1, d)\n",
    "        t_flat = t_n.repeat(num_samples * batch_size)\n",
    "        \n",
    "        # Compute optimal control\n",
    "        control_flat = lqr.optimal_control(t_flat, X_n_flat).to(torch.float64)\n",
    "        control = control_flat.reshape(num_samples, batch_size, lqr.m)\n",
    "        \n",
    "        # Compute drift and apply update for each sample\n",
    "        for i in range(num_samples):\n",
    "            for j in range(batch_size):\n",
    "                # Compute drift term: HX + Ma\n",
    "                drift = H @ X_n[i, j] + M @ control[i, j]\n",
    "                \n",
    "                # Update state using explicit scheme\n",
    "                X[i, j, n+1] = X_n[i, j] + drift * dt + sigma @ dW[i, j, n]\n",
    "                \n",
    "                # Accumulate running cost\n",
    "                state_cost = X_n[i, j] @ C @ X_n[i, j]\n",
    "                control_cost = control[i, j] @ D @ control[i, j]\n",
    "                costs[i, j] += (state_cost + control_cost) * dt\n",
    "    \n",
    "    # Add terminal cost\n",
    "    X_T = X[:, :, -1, :]\n",
    "    for i in range(num_samples):\n",
    "        for j in range(batch_size):\n",
    "            terminal_cost = X_T[i, j] @ lqr.R @ X_T[i, j]\n",
    "            costs[i, j] += terminal_cost\n",
    "    \n",
    "    return X, costs\n",
    "\n",
    "def simulate_sde_implicit(lqr: LQR, x0: torch.Tensor, num_steps: int, num_samples: int,\n",
    "                         fixed_noise: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Simulate the controlled SDE using the implicit scheme with optional fixed noise.\n",
    "    \n",
    "    Args:\n",
    "        lqr: LQR instance\n",
    "        x0: Initial state (batch x d)\n",
    "        num_steps: Number of time steps\n",
    "        num_samples: Number of Monte Carlo samples\n",
    "        fixed_noise: Optional fixed Brownian increments for consistent comparisons\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (trajectory, cost)\n",
    "    \"\"\"\n",
    "    dt = lqr.T / num_steps\n",
    "    t_grid = torch.linspace(0, lqr.T, num_steps + 1, dtype=torch.float64)\n",
    "    \n",
    "    batch_size = x0.shape[0]\n",
    "    d = lqr.d\n",
    "    \n",
    "    # Ensure all tensors are double precision\n",
    "    H = lqr.H.to(torch.float64)\n",
    "    M = lqr.M.to(torch.float64)\n",
    "    sigma = lqr.sigma.to(torch.float64)\n",
    "    C = lqr.C.to(torch.float64)\n",
    "    D = lqr.D.to(torch.float64)\n",
    "    D_inv = lqr.D_inv.to(torch.float64)\n",
    "    \n",
    "    # Initialize trajectories and costs\n",
    "    X = torch.zeros((num_samples, batch_size, num_steps + 1, d), dtype=torch.float64)\n",
    "    X[:, :, 0, :] = x0.unsqueeze(0).repeat(num_samples, 1, 1)\n",
    "    \n",
    "    costs = torch.zeros((num_samples, batch_size), dtype=torch.float64)\n",
    "    \n",
    "    # Generate Brownian increments or use fixed ones\n",
    "    if fixed_noise is None:\n",
    "        dW = torch.randn((num_samples, batch_size, num_steps, sigma.shape[1]), \n",
    "                        dtype=torch.float64) * np.sqrt(dt)\n",
    "    else:\n",
    "        dW = fixed_noise\n",
    "    \n",
    "    # Identity matrix for linear system\n",
    "    I = torch.eye(d, dtype=torch.float64)\n",
    "    \n",
    "    # Simulate trajectories\n",
    "    for n in range(num_steps):\n",
    "        t_n = t_grid[n]\n",
    "        t_np1 = t_grid[n+1] \n",
    "        X_n = X[:, :, n, :]\n",
    "        \n",
    "        # Reshape for batch processing for cost calculation\n",
    "        X_n_flat = X_n.reshape(-1, d)\n",
    "        t_flat = t_n.repeat(num_samples * batch_size)\n",
    "        \n",
    "        # Compute optimal control for cost calculation\n",
    "        control_flat = lqr.optimal_control(t_flat, X_n_flat).to(torch.float64)\n",
    "        control = control_flat.reshape(num_samples, batch_size, lqr.m)\n",
    "        \n",
    "        # For implicit scheme, we need to solve a linear system for each sample\n",
    "        S_np1 = lqr.get_S_at_time(torch.tensor([t_np1], dtype=torch.float64))[0].to(torch.float64)\n",
    "        \n",
    "        # Construct system matrix: (I - dt*H + dt*M*D^(-1)*M^T*S(t_{n+1}))\n",
    "        MD_inv_MT = M @ D_inv @ M.T\n",
    "        A = I - dt * H + dt * MD_inv_MT @ S_np1\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            for j in range(batch_size):\n",
    "                # Compute right-hand side: X_n + sigma * dW_n\n",
    "                b = X_n[i, j] + sigma @ dW[i, j, n]\n",
    "                \n",
    "                # Solve the linear system: A * X_{n+1} = b\n",
    "                X[i, j, n+1] = torch.linalg.solve(A, b)\n",
    "                \n",
    "                # Accumulate running cost\n",
    "                state_cost = X_n[i, j] @ C @ X_n[i, j]\n",
    "                control_cost = control[i, j] @ D @ control[i, j]\n",
    "                costs[i, j] += (state_cost + control_cost) * dt\n",
    "    \n",
    "    # Add terminal cost\n",
    "    X_T = X[:, :, -1, :]\n",
    "    for i in range(num_samples):\n",
    "        for j in range(batch_size):\n",
    "            terminal_cost = X_T[i, j] @ lqr.R @ X_T[i, j]\n",
    "            costs[i, j] += terminal_cost\n",
    "    \n",
    "    return X, costs\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "from typing import Tuple, Optional\n",
    "import math\n",
    "\n",
    "class SoftLQR(LQR):\n",
    "    def __init__(self, H: torch.Tensor, M: torch.Tensor, sigma: torch.Tensor, \n",
    "                 C: torch.Tensor, D: torch.Tensor, R: torch.Tensor, \n",
    "                 T: float, time_grid: torch.Tensor,\n",
    "                 tau: float, gamma: float):\n",
    "        \"\"\"\n",
    "        Initialize the soft LQR problem with entropy regularization.\n",
    "        \n",
    "        Args:\n",
    "            H: System dynamics matrix (d x d)\n",
    "            M: Control input matrix (d x m)\n",
    "            sigma: Noise matrix (d x d')\n",
    "            C: State cost matrix (d x d)\n",
    "            D: Control cost matrix (m x m)\n",
    "            R: Terminal state cost matrix (d x d)\n",
    "            T: Terminal time\n",
    "            time_grid: Grid of time points\n",
    "            tau: Entropy regularization strength\n",
    "            gamma: Variance of prior normal density\n",
    "        \"\"\"\n",
    "        super().__init__(H, M, sigma, C, D, R, T, time_grid)\n",
    "        \n",
    "        # Store additional parameters\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Compute modified inverse term for the soft LQR\n",
    "        # Σ⁻¹ = D + τ/(2γ²)I\n",
    "        self.sigma_inv = self.D + (tau / (2 * gamma**2)) * torch.eye(self.m, dtype=self.D.dtype, device=self.D.device)\n",
    "        self.sigma_term = torch.inverse(self.sigma_inv)\n",
    "        \n",
    "        # Compute determinant term for value function\n",
    "        # C_D,τ,γ = -τ ln(τ^(m/2)/(γ^m * det(Σ)^(1/2)))\n",
    "        self.CD_tau_gamma = -tau * math.log((tau**(self.m/2)) / (gamma**self.m * torch.sqrt(torch.det(self.sigma_term)).item()))\n",
    "\n",
    "    def ricatti_rhs(self, t: float, S_flat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Right-hand side of the modified Ricatti ODE for soft LQR:\n",
    "        S'(t) = S(t)M(D + τ/(2γ²)I)^(-1)M^TS(t) - H^TS(t) - S(t)H - C\n",
    "        \n",
    "        Args:\n",
    "            t: Time\n",
    "            S_flat: Flattened S matrix\n",
    "            \n",
    "        Returns:\n",
    "            Flattened derivative of S\n",
    "        \"\"\"\n",
    "        # Reshape S from flattened form\n",
    "        S = torch.tensor(S_flat.reshape(self.d, self.d), dtype=torch.float64)\n",
    "        \n",
    "        # Compute right-hand side with modified term\n",
    "        term1 = S @ self.M @ self.sigma_term @ self.M.T @ S\n",
    "        term2 = self.H.T @ S\n",
    "        term3 = S @ self.H\n",
    "        term4 = self.C\n",
    "        \n",
    "        # Compute derivative\n",
    "        dS = term1 - term2 - term3 - term4\n",
    "        \n",
    "        # Return flattened result\n",
    "        return dS.flatten().numpy()\n",
    "    \n",
    "    def compute_integral_term(self) -> None:\n",
    "        \"\"\"\n",
    "        Compute the integral term for the value function: \n",
    "        int_t^T tr(sigma sigma^T S(r)) dr + (T-t) * C_D,τ,γ\n",
    "        \"\"\"\n",
    "        if self.S_grid is None:\n",
    "            self.solve_ricatti()\n",
    "        \n",
    "        # Compute trace term at each time point\n",
    "        trace_terms = torch.zeros(len(self.time_grid))\n",
    "        sigma_sigma_T = self.sigma @ self.sigma.T\n",
    "        \n",
    "        for i in range(len(self.time_grid)):\n",
    "            trace_terms[i] = torch.trace(sigma_sigma_T @ self.S_grid[i])\n",
    "        \n",
    "        # Compute integral using trapezoidal rule (backward from T)\n",
    "        integral_term = torch.zeros(len(self.time_grid))\n",
    "        \n",
    "        for i in range(len(self.time_grid) - 1, 0, -1):\n",
    "            dt = self.time_grid[i] - self.time_grid[i-1]\n",
    "            integral_term[i-1] = integral_term[i] + 0.5 * (trace_terms[i] + trace_terms[i-1]) * dt\n",
    "        \n",
    "        # Add the constant term proportional to (T-t)\n",
    "        for i in range(len(self.time_grid)):\n",
    "            integral_term[i] += (self.T - self.time_grid[i]) * self.CD_tau_gamma\n",
    "        \n",
    "        self.int_term_grid = integral_term\n",
    "    \n",
    "    def optimal_control_distribution(self, t: torch.Tensor, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute the parameters of the optimal control distribution:\n",
    "        π*(·|t, x) = N(-(D + τ/(2γ²)I)^(-1)M^TS(t)x, τ(D + τ/(2γ²)I))\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            x: State tensor (batch x d)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (mean, covariance) of optimal control distribution\n",
    "        \"\"\"\n",
    "        # Get S matrices at the specified times\n",
    "        S_matrices = self.get_S_at_time(t)\n",
    "        \n",
    "        # Compute mean for each (t, x) pair\n",
    "        batch_size = x.shape[0]\n",
    "        means = torch.zeros((batch_size, self.m), dtype=torch.float64, device=x.device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            means[i] = -self.sigma_term @ self.M.T @ S_matrices[i] @ x[i]\n",
    "        \n",
    "        # The covariance is constant for all (t, x)\n",
    "        covariance = self.tau * self.sigma_term\n",
    "        \n",
    "        return means, covariance\n",
    "    \n",
    "    def optimal_control(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample from the optimal control distribution.\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            x: State tensor (batch x d)\n",
    "            \n",
    "        Returns:\n",
    "            Sampled control actions (batch x m)\n",
    "        \"\"\"\n",
    "        means, covariance = self.optimal_control_distribution(t, x)\n",
    "        \n",
    "        # Create multivariate normal distribution\n",
    "        batch_size = means.shape[0]\n",
    "        samples = torch.zeros_like(means)\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        L = torch.linalg.cholesky(covariance)  # Lower triangular Cholesky factor\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Generate standard normal samples\n",
    "            z = torch.randn(self.m, dtype=torch.float64, device=means.device)\n",
    "            # Transform to multivariate normal with the required covariance\n",
    "            samples[i] = means[i] + L @ z\n",
    "        \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would require installing torchdiffeq: pip install torchdiffeq\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "class GPUSoftLQR(SoftLQR):\n",
    "    def solve_ricatti(self) -> None:\n",
    "        \"\"\"\n",
    "        Solve the Ricatti ODE using torchdiffeq for GPU acceleration.\n",
    "        \"\"\"\n",
    "        # Convert matrices to double precision and move to the proper device\n",
    "        self.H = self.H.to(torch.float64, device=self.device)\n",
    "        self.M = self.M.to(torch.float64, device=self.device)\n",
    "        self.sigma = self.sigma.to(torch.float64, device=self.device)\n",
    "        self.C = self.C.to(torch.float64, device=self.device)\n",
    "        self.D = self.D.to(torch.float64, device=self.device)\n",
    "        self.R = self.R.to(torch.float64, device=self.device)\n",
    "        self.D_inv = self.D_inv.to(torch.float64, device=self.device)\n",
    "        self.sigma_inv = self.sigma_inv.to(torch.float64, device=self.device)\n",
    "        self.sigma_term = self.sigma_term.to(torch.float64, device=self.device)\n",
    "        \n",
    "        # Define RiccatiODE as a torch.nn.Module for torchdiffeq\n",
    "        class RiccatiODE(torch.nn.Module):\n",
    "            def __init__(self, H, M, sigma_term, C, d):\n",
    "                super().__init__()\n",
    "                self.H = H\n",
    "                self.M = M\n",
    "                self.sigma_term = sigma_term\n",
    "                self.C = C\n",
    "                self.d = d\n",
    "            \n",
    "            def forward(self, t, S_flat):\n",
    "                # Reshape S from flattened form\n",
    "                S = S_flat.reshape(self.d, self.d)\n",
    "                \n",
    "                # Compute right-hand side\n",
    "                term1 = S @ self.M @ self.sigma_term @ self.M.T @ S\n",
    "                term2 = self.H.T @ S\n",
    "                term3 = S @ self.H\n",
    "                term4 = self.C\n",
    "                \n",
    "                # Compute derivative\n",
    "                dS = term1 - term2 - term3 - term4\n",
    "                \n",
    "                # Return flattened result\n",
    "                return dS.flatten()\n",
    "        \n",
    "        # Create the ODE module\n",
    "        ode_fn = RiccatiODE(\n",
    "            self.H, self.M, self.sigma_term, self.C, self.d\n",
    "        )\n",
    "        \n",
    "        # Terminal condition: S(T) = R\n",
    "        S_T_flat = self.R.flatten()\n",
    "        \n",
    "        # Time points for ODE solver (reversed for backward integration)\n",
    "        t_points = self.time_grid\n",
    "        t_reversed = self.T - t_points.flip(0)\n",
    "        \n",
    "        # Solve the ODE backward in time (from T to 0) using torchdiffeq\n",
    "        with torch.no_grad():\n",
    "            S_solution = odeint(\n",
    "                ode_fn, \n",
    "                S_T_flat, \n",
    "                t_reversed, \n",
    "                method='dopri5',\n",
    "                rtol=1e-10, \n",
    "                atol=1e-10\n",
    "            )\n",
    "        \n",
    "        # Reshape solution and reverse back to forward time\n",
    "        S_matrices = S_solution.reshape(-1, self.d, self.d).flip(0)\n",
    "        \n",
    "        self.S_grid = S_matrices\n",
    "        \n",
    "        # Compute integral term for value function\n",
    "        self.compute_integral_term()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_comparison(lqr: LQR, soft_lqr: SoftLQR, x0_list: list, num_steps: int, scheme: str = 'explicit') -> None:\n",
    "    \"\"\"\n",
    "    Simulate and compare the standard LQR and soft LQR trajectories.\n",
    "    \n",
    "    Args:\n",
    "        lqr: Standard LQR instance\n",
    "        soft_lqr: Soft LQR instance\n",
    "        x0_list: List of initial states to test\n",
    "        num_steps: Number of time steps\n",
    "        scheme: 'explicit' or 'implicit'\n",
    "    \"\"\"\n",
    "    dt = lqr.T / num_steps\n",
    "    t_grid = torch.linspace(0, lqr.T, num_steps + 1, dtype=torch.float64)\n",
    "    \n",
    "    # Initialize plot\n",
    "    fig, axes = plt.subplots(len(x0_list), 2, figsize=(16, 4*len(x0_list)))\n",
    "    \n",
    "    for i, x0 in enumerate(x0_list):\n",
    "        x0_tensor = torch.tensor([x0], dtype=torch.float64)\n",
    "        \n",
    "        # Ensure all matrices are double precision\n",
    "        H = lqr.H.to(torch.float64)\n",
    "        M = lqr.M.to(torch.float64)\n",
    "        sigma = lqr.sigma.to(torch.float64)\n",
    "        C = lqr.C.to(torch.float64)\n",
    "        D = lqr.D.to(torch.float64)\n",
    "        \n",
    "        # Initialize trajectories and costs\n",
    "        X_lqr = torch.zeros((num_steps + 1, 2), dtype=torch.float64)\n",
    "        X_soft = torch.zeros((num_steps + 1, 2), dtype=torch.float64)\n",
    "        X_lqr[0] = x0_tensor[0]\n",
    "        X_soft[0] = x0_tensor[0]\n",
    "        \n",
    "        costs_lqr = torch.zeros(1, dtype=torch.float64)\n",
    "        costs_soft = torch.zeros(1, dtype=torch.float64)\n",
    "        \n",
    "        # Generate same Brownian increments for both simulations\n",
    "        dW = torch.randn((num_steps, sigma.shape[1]), dtype=torch.float64) * np.sqrt(dt)\n",
    "        \n",
    "        # Simulate trajectories\n",
    "        if scheme == 'explicit':\n",
    "            # Explicit scheme\n",
    "            for n in range(num_steps):\n",
    "                t_n = t_grid[n]\n",
    "                t_tensor = torch.tensor([t_n], dtype=torch.float64)\n",
    "                \n",
    "                # Standard LQR step\n",
    "                control_lqr = lqr.optimal_control(t_tensor, X_lqr[n:n+1]).to(torch.float64)[0]\n",
    "                drift_lqr = H @ X_lqr[n] + M @ control_lqr\n",
    "                X_lqr[n+1] = X_lqr[n] + drift_lqr * dt + sigma @ dW[n]\n",
    "                \n",
    "                # Compute running cost for LQR\n",
    "                state_cost_lqr = X_lqr[n] @ C @ X_lqr[n]\n",
    "                control_cost_lqr = control_lqr @ D @ control_lqr\n",
    "                costs_lqr += (state_cost_lqr + control_cost_lqr) * dt\n",
    "                \n",
    "                # Soft LQR step\n",
    "                control_soft = soft_lqr.optimal_control(t_tensor, X_soft[n:n+1]).to(torch.float64)[0]\n",
    "                drift_soft = H @ X_soft[n] + M @ control_soft\n",
    "                X_soft[n+1] = X_soft[n] + drift_soft * dt + sigma @ dW[n]\n",
    "                \n",
    "                # Compute running cost for soft LQR (includes entropy term)\n",
    "                state_cost_soft = X_soft[n] @ C @ X_soft[n]\n",
    "                control_cost_soft = control_soft @ D @ control_soft\n",
    "                \n",
    "                # Add entropy regularization term\n",
    "                means, covariance = soft_lqr.optimal_control_distribution(t_tensor, X_soft[n:n+1])\n",
    "                entropy_term = soft_lqr.tau * torch.log(torch.det(2 * math.pi * math.e * covariance)).item() / 2\n",
    "                \n",
    "                costs_soft += (state_cost_soft + control_cost_soft + entropy_term) * dt\n",
    "        else:\n",
    "            # Implicit scheme\n",
    "            # Identity matrix for linear system\n",
    "            I = torch.eye(2, dtype=torch.float64)\n",
    "            \n",
    "            for n in range(num_steps):\n",
    "                t_n = t_grid[n]\n",
    "                t_np1 = t_grid[n+1]\n",
    "                t_tensor = torch.tensor([t_n], dtype=torch.float64)\n",
    "                t_np1_tensor = torch.tensor([t_np1], dtype=torch.float64)\n",
    "                \n",
    "                # Standard LQR\n",
    "                control_lqr = lqr.optimal_control(t_tensor, X_lqr[n:n+1]).to(torch.float64)[0]\n",
    "                S_np1_lqr = lqr.get_S_at_time(t_np1_tensor)[0].to(torch.float64)\n",
    "                A_lqr = I - dt * H + dt * M @ lqr.D_inv @ M.T @ S_np1_lqr\n",
    "                b_lqr = X_lqr[n] + sigma @ dW[n]\n",
    "                X_lqr[n+1] = torch.linalg.solve(A_lqr, b_lqr)\n",
    "                \n",
    "                # Compute running cost for LQR\n",
    "                state_cost_lqr = X_lqr[n] @ C @ X_lqr[n]\n",
    "                control_cost_lqr = control_lqr @ D @ control_lqr\n",
    "                costs_lqr += (state_cost_lqr + control_cost_lqr) * dt\n",
    "                \n",
    "                # Soft LQR\n",
    "                control_soft = soft_lqr.optimal_control(t_tensor, X_soft[n:n+1]).to(torch.float64)[0]\n",
    "                S_np1_soft = soft_lqr.get_S_at_time(t_np1_tensor)[0].to(torch.float64)\n",
    "                A_soft = I - dt * H + dt * M @ soft_lqr.sigma_term @ M.T @ S_np1_soft\n",
    "                b_soft = X_soft[n] + sigma @ dW[n]\n",
    "                X_soft[n+1] = torch.linalg.solve(A_soft, b_soft)\n",
    "                \n",
    "                # Compute running cost for soft LQR (includes entropy term)\n",
    "                state_cost_soft = X_soft[n] @ C @ X_soft[n]\n",
    "                control_cost_soft = control_soft @ D @ control_soft\n",
    "                \n",
    "                # Add entropy regularization term\n",
    "                means, covariance = soft_lqr.optimal_control_distribution(t_tensor, X_soft[n:n+1])\n",
    "                entropy_term = soft_lqr.tau * torch.log(torch.det(2 * math.pi * math.e * covariance)).item() / 2\n",
    "                \n",
    "                costs_soft += (state_cost_soft + control_cost_soft + entropy_term) * dt\n",
    "        \n",
    "        # Add terminal costs\n",
    "        terminal_cost_lqr = X_lqr[-1] @ lqr.R @ X_lqr[-1]\n",
    "        terminal_cost_soft = X_soft[-1] @ soft_lqr.R @ X_soft[-1]\n",
    "        costs_lqr += terminal_cost_lqr\n",
    "        costs_soft += terminal_cost_soft\n",
    "        \n",
    "        # Plot trajectories\n",
    "        ax1 = axes[i, 0]\n",
    "        ax1.plot(X_lqr[:, 0].numpy(), X_lqr[:, 1].numpy(), 'b-', label='Standard LQR')\n",
    "        ax1.plot(X_soft[:, 0].numpy(), X_soft[:, 1].numpy(), 'r-', label='Soft LQR')\n",
    "        ax1.scatter([x0[0]], [x0[1]], color='g', s=100, marker='o', label='Initial State')\n",
    "        ax1.set_title(f'Trajectories from Initial State {x0}')\n",
    "        ax1.set_xlabel('X1')\n",
    "        ax1.set_ylabel('X2')\n",
    "        ax1.grid(True)\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Plot running costs\n",
    "        running_costs_lqr = torch.zeros(num_steps + 1)\n",
    "        running_costs_soft = torch.zeros(num_steps + 1)\n",
    "        \n",
    "        # Recompute running costs for plotting\n",
    "        for n in range(num_steps):\n",
    "            t_n = t_grid[n]\n",
    "            t_tensor = torch.tensor([t_n], dtype=torch.float64)\n",
    "            \n",
    "            # LQR costs\n",
    "            control_lqr = lqr.optimal_control(t_tensor, X_lqr[n:n+1])[0]\n",
    "            state_cost_lqr = X_lqr[n] @ C @ X_lqr[n]\n",
    "            control_cost_lqr = control_lqr @ D @ control_lqr\n",
    "            running_costs_lqr[n+1] = running_costs_lqr[n] + (state_cost_lqr + control_cost_lqr) * dt\n",
    "            \n",
    "            # Soft LQR costs\n",
    "            control_soft = soft_lqr.optimal_control(t_tensor, X_soft[n:n+1])[0]\n",
    "            state_cost_soft = X_soft[n] @ C @ X_soft[n]\n",
    "            control_cost_soft = control_soft @ D @ control_soft\n",
    "            means, covariance = soft_lqr.optimal_control_distribution(t_tensor, X_soft[n:n+1])\n",
    "            entropy_term = soft_lqr.tau * torch.log(torch.det(2 * math.pi * math.e * covariance)).item() / 2\n",
    "            running_costs_soft[n+1] = running_costs_soft[n] + (state_cost_soft + control_cost_soft + entropy_term) * dt\n",
    "        \n",
    "        # Add terminal costs\n",
    "        running_costs_lqr[-1] += terminal_cost_lqr\n",
    "        running_costs_soft[-1] += terminal_cost_soft\n",
    "        \n",
    "        ax2 = axes[i, 1]\n",
    "        ax2.plot(t_grid.numpy(), running_costs_lqr.numpy(), 'b-', label='Standard LQR Cost')\n",
    "        ax2.plot(t_grid.numpy(), running_costs_soft.numpy(), 'r-', label='Soft LQR Cost')\n",
    "        ax2.set_title(f'Cost Over Time from Initial State {x0}')\n",
    "        ax2.set_xlabel('Time')\n",
    "        ax2.set_ylabel('Cost')\n",
    "        ax2.grid(True)\n",
    "        ax2.legend()\n",
    "        \n",
    "        print(f\"Initial state {x0}:\")\n",
    "        print(f\"  Standard LQR final cost: {costs_lqr.item():.2f}\")\n",
    "        print(f\"  Soft LQR final cost: {costs_soft.item():.2f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_soft_lqr():\n",
    "    # Set the problem matrices as specified in Figure 1\n",
    "    H = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64) * 0.5\n",
    "    M = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64)\n",
    "    sigma = torch.eye(2, dtype=torch.float64) * 0.5\n",
    "    C = torch.tensor([[1.0, 0.1], [0.1, 1.0]], dtype=torch.float64) * 1.0\n",
    "    D = torch.tensor([[1.0, 0.1], [0.1, 1.0]], dtype=torch.float64) * 0.1\n",
    "    R = torch.tensor([[1.0, 0.3], [0.3, 1.0]], dtype=torch.float64) * 10.0\n",
    "    \n",
    "    # Set the terminal time and time grid\n",
    "    T = 0.5\n",
    "    grid_size = 1000\n",
    "    time_grid = torch.linspace(0, T, grid_size, dtype=torch.float64)\n",
    "    \n",
    "    # Set entropy regularization parameters\n",
    "    tau = 0.1\n",
    "    gamma = 10.0\n",
    "    \n",
    "    # Create standard LQR instance\n",
    "    lqr = LQR(H, M, sigma, C, D, R, T, time_grid)\n",
    "    \n",
    "    # Create soft LQR instance\n",
    "    soft_lqr = SoftLQR(H, M, sigma, C, D, R, T, time_grid, tau, gamma)\n",
    "    \n",
    "    # Solve Ricatti ODEs\n",
    "    lqr.solve_ricatti()\n",
    "    soft_lqr.solve_ricatti()\n",
    "    \n",
    "    # Print S matrices at key time points\n",
    "    print(\"Standard LQR S(0):\\n\", lqr.S_grid[0])\n",
    "    print(\"Soft LQR S(0):\\n\", soft_lqr.S_grid[0])\n",
    "    \n",
    "    # Test points for trajectory comparison\n",
    "    initial_states = [\n",
    "        [2.0, 2.0],\n",
    "        [2.0, -2.0],\n",
    "        [-2.0, -2.0],\n",
    "        [-2.0, 2.0]\n",
    "    ]\n",
    "    \n",
    "    # Compare trajectories\n",
    "    simulate_comparison(lqr, soft_lqr, initial_states, num_steps=200, scheme='explicit')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_soft_lqr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size=512, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Neural network to learn the value function for the soft LQR problem.\n",
    "        \n",
    "        Args:\n",
    "            hidden_size: Size of the hidden layers\n",
    "            device: Device to run the network on\n",
    "        \"\"\"\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        \n",
    "        # Set device\n",
    "        self.device = device\n",
    "        \n",
    "        # Define layers\n",
    "        self.hidden_layer1 = nn.Linear(1, hidden_size).to(device)\n",
    "        self.hidden_layer2 = nn.Linear(hidden_size, hidden_size).to(device)\n",
    "        \n",
    "        # Output for the matrix (2x2 symmetric)\n",
    "        self.matrix_output = nn.Linear(hidden_size, 2*2).to(device)\n",
    "        \n",
    "        # Output for the scalar offset\n",
    "        self.offset_output = nn.Linear(hidden_size, 1).to(device)\n",
    "    \n",
    "    def forward(self, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (matrix, offset) defining the value function\n",
    "        \"\"\"\n",
    "        # Reshape time to (batch, 1)\n",
    "        t = t.view(-1, 1).to(self.device)\n",
    "        \n",
    "        # Forward pass through the hidden layers\n",
    "        hidden = torch.relu(self.hidden_layer1(t))\n",
    "        hidden = torch.relu(self.hidden_layer2(hidden))\n",
    "        \n",
    "        # Compute the matrix output\n",
    "        matrix_elements = self.matrix_output(hidden)\n",
    "        matrix = matrix_elements.view(-1, 2, 2)\n",
    "        \n",
    "        # Make the matrix positive semi-definite by product with transpose\n",
    "        # A^T A is always positive semi-definite\n",
    "        # Adding small constant to diagonal for stability\n",
    "        eye = torch.eye(2, device=self.device).unsqueeze(0).repeat(matrix.shape[0], 1, 1)\n",
    "        matrix = torch.bmm(matrix, matrix.transpose(1, 2)) + 1e-3 * eye\n",
    "        \n",
    "        # Compute the offset\n",
    "        offset = self.offset_output(hidden)\n",
    "        \n",
    "        return matrix, offset\n",
    "    \n",
    "    def value_function(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the value function v(t, x) = x^T S(t) x + b(t)\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            x: State tensor (batch x 2)\n",
    "            \n",
    "        Returns:\n",
    "            Value function at (t, x) (batch)\n",
    "        \"\"\"\n",
    "        # Get the matrix and offset from the network\n",
    "        S, b = self.forward(t)\n",
    "        \n",
    "        # Compute the quadratic term x^T S x\n",
    "        batch_size = x.shape[0]\n",
    "        values = torch.zeros(batch_size, device=self.device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            values[i] = torch.matmul(torch.matmul(x[i], S[i]), x[i])\n",
    "        \n",
    "        # Add the offset term\n",
    "        values = values + b.view(-1)\n",
    "        \n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticAlgorithm:\n",
    "    def __init__(self, soft_lqr: SoftLQR, value_network: ValueNetwork, \n",
    "                 learning_rate: float = 1e-3, device: str = \"cpu\"):\n",
    "        \"\"\"\n",
    "        Critic algorithm to learn the value function for a fixed policy.\n",
    "        \n",
    "        Args:\n",
    "            soft_lqr: Soft LQR instance with the fixed policy\n",
    "            value_network: Neural network to approximate the value function\n",
    "            learning_rate: Learning rate for the optimizer\n",
    "            device: Device to run the algorithm on\n",
    "        \"\"\"\n",
    "        self.soft_lqr = soft_lqr\n",
    "        self.value_network = value_network\n",
    "        self.device = device\n",
    "        \n",
    "        # Move soft_lqr matrices to the device\n",
    "        self.soft_lqr.H = self.soft_lqr.H.to(device)\n",
    "        self.soft_lqr.M = self.soft_lqr.M.to(device)\n",
    "        self.soft_lqr.sigma = self.soft_lqr.sigma.to(device)\n",
    "        self.soft_lqr.C = self.soft_lqr.C.to(device)\n",
    "        self.soft_lqr.D = self.soft_lqr.D.to(device)\n",
    "        self.soft_lqr.R = self.soft_lqr.R.to(device)\n",
    "        self.soft_lqr.D_inv = self.soft_lqr.D_inv.to(device)\n",
    "        self.soft_lqr.sigma_inv = self.soft_lqr.sigma_inv.to(device)\n",
    "        self.soft_lqr.sigma_term = self.soft_lqr.sigma_term.to(device)\n",
    "        if hasattr(self.soft_lqr, 'S_grid') and self.soft_lqr.S_grid is not None:\n",
    "            self.soft_lqr.S_grid = self.soft_lqr.S_grid.to(device)\n",
    "        if hasattr(self.soft_lqr, 'int_term_grid') and self.soft_lqr.int_term_grid is not None:\n",
    "            self.soft_lqr.int_term_grid = self.soft_lqr.int_term_grid.to(device)\n",
    "        \n",
    "        # Setup optimizer\n",
    "        self.optimizer = optim.Adam(self.value_network.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def compute_episode_values(self, initial_states: torch.Tensor, num_steps: int, \n",
    "                              num_episodes: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute the true value function and Monte Carlo estimates for a batch of initial states.\n",
    "        \n",
    "        Args:\n",
    "            initial_states: Initial states tensor (batch x 2)\n",
    "            num_steps: Number of time steps for simulation\n",
    "            num_episodes: Number of episodes for Monte Carlo estimation\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (true_values, mc_estimates, states_trajectory)\n",
    "        \"\"\"\n",
    "        batch_size = initial_states.shape[0]\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Compute true value function at t=0, x=initial_states\n",
    "        t0 = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "        true_values = self.soft_lqr.value_function(t0, initial_states)\n",
    "        \n",
    "        # Initialize trajectories and accumulated costs\n",
    "        states = torch.zeros((num_episodes, batch_size, num_steps + 1, 2), \n",
    "                             device=self.device, dtype=torch.float64)\n",
    "        states[:, :, 0, :] = initial_states.unsqueeze(0).repeat(num_episodes, 1, 1)\n",
    "        \n",
    "        accumulated_costs = torch.zeros((num_episodes, batch_size), \n",
    "                                       device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Generate time grid\n",
    "        t_grid = torch.linspace(0, self.soft_lqr.T, num_steps + 1, \n",
    "                               device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Generate Brownian increments\n",
    "        dW = torch.randn((num_episodes, batch_size, num_steps, self.soft_lqr.sigma.shape[1]), \n",
    "                        device=self.device, dtype=torch.float64) * np.sqrt(dt)\n",
    "        \n",
    "        # Simulate trajectories using explicit scheme\n",
    "        for n in range(num_steps):\n",
    "            t_n = t_grid[n]\n",
    "            t_flat = t_n.repeat(num_episodes * batch_size)\n",
    "            \n",
    "            # Reshape states for batch processing\n",
    "            states_flat = states[:, :, n, :].reshape(-1, 2)\n",
    "            \n",
    "            # Get control means and covariance\n",
    "            means, covariance = self.soft_lqr.optimal_control_distribution(t_flat, states_flat)\n",
    "            means = means.reshape(num_episodes, batch_size, -1)\n",
    "            \n",
    "            # Sample controls\n",
    "            controls = torch.zeros_like(means)\n",
    "            L = torch.linalg.cholesky(covariance)\n",
    "            \n",
    "            for i in range(num_episodes):\n",
    "                for j in range(batch_size):\n",
    "                    # Generate standard normal samples\n",
    "                    z = torch.randn(self.soft_lqr.m, device=self.device, dtype=torch.float64)\n",
    "                    # Transform to multivariate normal\n",
    "                    controls[i, j] = means[i, j] + torch.matmul(L, z)\n",
    "            \n",
    "            # Update states\n",
    "            for i in range(num_episodes):\n",
    "                for j in range(batch_size):\n",
    "                    # Compute drift\n",
    "                    drift = self.soft_lqr.H @ states[i, j, n] + self.soft_lqr.M @ controls[i, j]\n",
    "                    \n",
    "                    # Update state using explicit scheme\n",
    "                    states[i, j, n+1] = states[i, j, n] + drift * dt + self.soft_lqr.sigma @ dW[i, j, n]\n",
    "                    \n",
    "                    # Compute running cost (state cost + control cost + entropy regularization)\n",
    "                    state_cost = states[i, j, n] @ self.soft_lqr.C @ states[i, j, n]\n",
    "                    control_cost = controls[i, j] @ self.soft_lqr.D @ controls[i, j]\n",
    "                    \n",
    "                    # Log probability density of the control under the policy\n",
    "                    log_prob = -0.5 * (\n",
    "                        (controls[i, j] - means[i, j]).T @ torch.inverse(covariance) @ (controls[i, j] - means[i, j])\n",
    "                        + torch.log(torch.det(2 * np.pi * covariance))\n",
    "                    )\n",
    "                    \n",
    "                    # Entropy regularization term\n",
    "                    entropy_term = self.soft_lqr.tau * (-log_prob)\n",
    "                    \n",
    "                    # Accumulate costs\n",
    "                    accumulated_costs[i, j] += (state_cost + control_cost + entropy_term) * dt\n",
    "        \n",
    "        # Add terminal costs\n",
    "        for i in range(num_episodes):\n",
    "            for j in range(batch_size):\n",
    "                terminal_cost = states[i, j, -1] @ self.soft_lqr.R @ states[i, j, -1]\n",
    "                accumulated_costs[i, j] += terminal_cost\n",
    "        \n",
    "        # Compute Monte Carlo estimates (mean over episodes)\n",
    "        mc_estimates = accumulated_costs.mean(dim=0)\n",
    "        \n",
    "        return true_values, mc_estimates, states\n",
    "    \n",
    "    def train_step(self, initial_states_dist, num_states: int, num_steps: int, \n",
    "                  num_episodes: int) -> float:\n",
    "        \"\"\"\n",
    "        Perform one training step of the critic algorithm.\n",
    "        \n",
    "        Args:\n",
    "            initial_states_dist: Distribution to sample initial states from\n",
    "            num_states: Number of initial states to sample\n",
    "            num_steps: Number of time steps for simulation\n",
    "            num_episodes: Number of episodes for Monte Carlo estimation\n",
    "            \n",
    "        Returns:\n",
    "            Loss value for this step\n",
    "        \"\"\"\n",
    "        # Sample initial states\n",
    "        initial_states = initial_states_dist.sample((num_states,)).to(self.device)\n",
    "        \n",
    "        # Compute true values and Monte Carlo estimates\n",
    "        _, mc_estimates, states = self.compute_episode_values(\n",
    "            initial_states, num_steps, num_episodes)\n",
    "        \n",
    "        # Initialize loss\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        # Compute loss for each time step\n",
    "        for n in range(num_steps + 1):\n",
    "            # Get time and states at this step\n",
    "            t_n = torch.ones(num_states, device=self.device, dtype=torch.float64) * (n * self.soft_lqr.T / num_steps)\n",
    "            x_n = states[0, :, n, :]  # Use first episode for states\n",
    "            \n",
    "            # Compute value function prediction\n",
    "            predicted_values = self.value_network.value_function(t_n, x_n)\n",
    "            \n",
    "            # Compute target values at this time step\n",
    "            if n == num_steps:\n",
    "                # At terminal time, target is just the terminal cost\n",
    "                target_values = torch.zeros(num_states, device=self.device, dtype=torch.float64)\n",
    "                for j in range(num_states):\n",
    "                    target_values[j] = x_n[j] @ self.soft_lqr.R @ x_n[j]\n",
    "            else:\n",
    "                # For non-terminal times, target is the Monte Carlo estimate\n",
    "                remaining_costs = torch.zeros((num_episodes, num_states), \n",
    "                                            device=self.device, dtype=torch.float64)\n",
    "                \n",
    "                # Recompute remaining costs for this time step\n",
    "                for i in range(num_episodes):\n",
    "                    for j in range(num_states):\n",
    "                        for k in range(n, num_steps):\n",
    "                            # Time at step k\n",
    "                            t_k = k * self.soft_lqr.T / num_steps\n",
    "                            t_tensor = torch.tensor([t_k], device=self.device, dtype=torch.float64)\n",
    "                            \n",
    "                            # State and control at step k\n",
    "                            state_k = states[i, j, k]\n",
    "                            state_tensor = state_k.unsqueeze(0)\n",
    "                            \n",
    "                            # Get control distribution\n",
    "                            means, covariance = self.soft_lqr.optimal_control_distribution(\n",
    "                                t_tensor, state_tensor)\n",
    "                            \n",
    "                            # Sample control\n",
    "                            L = torch.linalg.cholesky(covariance)\n",
    "                            z = torch.randn(self.soft_lqr.m, device=self.device, dtype=torch.float64)\n",
    "                            control = means[0] + torch.matmul(L, z)\n",
    "                            \n",
    "                            # Compute costs\n",
    "                            state_cost = state_k @ self.soft_lqr.C @ state_k\n",
    "                            control_cost = control @ self.soft_lqr.D @ control\n",
    "                            \n",
    "                            # Log probability\n",
    "                            log_prob = -0.5 * (\n",
    "                                (control - means[0]).T @ torch.inverse(covariance) @ (control - means[0])\n",
    "                                + torch.log(torch.det(2 * np.pi * covariance))\n",
    "                            )\n",
    "                            \n",
    "                            # Entropy term\n",
    "                            entropy_term = self.soft_lqr.tau * (-log_prob)\n",
    "                            \n",
    "                            # Add to remaining costs\n",
    "                            remaining_costs[i, j] += (state_cost + control_cost + entropy_term) * (self.soft_lqr.T / num_steps)\n",
    "                        \n",
    "                        # Add terminal cost\n",
    "                        terminal_state = states[i, j, -1]\n",
    "                        terminal_cost = terminal_state @ self.soft_lqr.R @ terminal_state\n",
    "                        remaining_costs[i, j] += terminal_cost\n",
    "                \n",
    "                # Average over episodes\n",
    "                target_values = remaining_costs.mean(dim=0)\n",
    "            \n",
    "            # Compute MSE loss for this time step\n",
    "            step_loss = ((predicted_values - target_values) ** 2).mean()\n",
    "            total_loss += step_loss\n",
    "        \n",
    "        # Average loss over time steps\n",
    "        avg_loss = total_loss / (num_steps + 1)\n",
    "        \n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        avg_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return avg_loss.item()\n",
    "    \n",
    "    def train(self, epochs: int, batch_size: int, num_steps: int, num_episodes: int, \n",
    "             initial_states_dist, eval_freq: int = 10, verbose: bool = True) -> List[float]:\n",
    "        \"\"\"\n",
    "        Train the critic algorithm for multiple epochs.\n",
    "        \n",
    "        Args:\n",
    "            epochs: Number of training epochs\n",
    "            batch_size: Batch size (number of initial states) per epoch\n",
    "            num_steps: Number of time steps for simulation\n",
    "            num_episodes: Number of episodes for Monte Carlo estimation\n",
    "            initial_states_dist: Distribution to sample initial states from\n",
    "            eval_freq: Frequency of evaluation\n",
    "            verbose: Whether to print progress\n",
    "            \n",
    "        Returns:\n",
    "            List of loss values during training\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Perform one training step\n",
    "            loss = self.train_step(initial_states_dist, batch_size, num_steps, num_episodes)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch+1) % eval_freq == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4e}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def evaluate(self, test_states: torch.Tensor, plot: bool = True) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Evaluate the trained value network against the true value function.\n",
    "        \n",
    "        Args:\n",
    "            test_states: Test states tensor (batch x 2)\n",
    "            plot: Whether to plot the results\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of errors\n",
    "        \"\"\"\n",
    "        # Compute true value function at t=0, x=test_states\n",
    "        batch_size = test_states.shape[0]\n",
    "        t0 = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "        true_values = self.soft_lqr.value_function(t0, test_states)\n",
    "        \n",
    "        # Compute value function prediction\n",
    "        predicted_values = self.value_network.value_function(t0, test_states)\n",
    "        \n",
    "        # Compute errors\n",
    "        errors = torch.abs(predicted_values - true_values)\n",
    "        \n",
    "        # Print statistics\n",
    "        max_error = errors.max().item()\n",
    "        mean_error = errors.mean().item()\n",
    "        print(f\"Evaluation results:\")\n",
    "        print(f\"  Max error: {max_error:.4e}\")\n",
    "        print(f\"  Mean error: {mean_error:.4e}\")\n",
    "        \n",
    "        # Plot results if requested\n",
    "        if plot:\n",
    "            # Convert to numpy for plotting\n",
    "            test_states_np = test_states.cpu().numpy()\n",
    "            true_values_np = true_values.cpu().numpy()\n",
    "            predicted_values_np = predicted_values.cpu().numpy()\n",
    "            errors_np = errors.cpu().numpy()\n",
    "            \n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            # Plot true vs predicted values\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.scatter(true_values_np, predicted_values_np, alpha=0.7)\n",
    "            min_val = min(true_values_np.min(), predicted_values_np.min())\n",
    "            max_val = max(true_values_np.max(), predicted_values_np.max())\n",
    "            plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "            plt.title('True vs Predicted Values')\n",
    "            plt.xlabel('True Value')\n",
    "            plt.ylabel('Predicted Value')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            # Plot errors vs true values\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.scatter(true_values_np, errors_np, alpha=0.7)\n",
    "            plt.title('Errors vs True Values')\n",
    "            plt.xlabel('True Value')\n",
    "            plt.ylabel('Absolute Error')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_critic():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Problem setup as specified in the exercise\n",
    "    H = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64) * 0.5\n",
    "    M = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64)\n",
    "    sigma = torch.eye(2, dtype=torch.float64) * 0.5\n",
    "    C = torch.tensor([[1.0, 0.1], [0.1, 1.0]], dtype=torch.float64) * 1.0\n",
    "    D = torch.eye(2, dtype=torch.float64)  # D just identity, as specified\n",
    "    R = torch.tensor([[1.0, 0.3], [0.3, 1.0]], dtype=torch.float64) * 10.0\n",
    "    \n",
    "    # Set terminal time and time grid\n",
    "    T = 0.5\n",
    "    grid_size = 1000\n",
    "    time_grid = torch.linspace(0, T, grid_size, dtype=torch.float64)\n",
    "    \n",
    "    # Set entropy regularization parameters\n",
    "    tau = 0.5  # τ = 1/2 as specified\n",
    "    gamma = 1.0  # γ = 1 as specified\n",
    "    \n",
    "    # Create soft LQR instance\n",
    "    soft_lqr = SoftLQR(H, M, sigma, C, D, R, T, time_grid, tau, gamma)\n",
    "    \n",
    "    # Solve Ricatti ODE\n",
    "    print(\"Solving Ricatti ODE...\")\n",
    "    soft_lqr.solve_ricatti()\n",
    "    \n",
    "    # Create value network\n",
    "    hidden_size = 512  # As specified\n",
    "    value_network = ValueNetwork(hidden_size=hidden_size, device=device)\n",
    "    \n",
    "    # Create critic algorithm\n",
    "    learning_rate = 1e-3  # As specified\n",
    "    critic = CriticAlgorithm(soft_lqr, value_network, learning_rate, device)\n",
    "    \n",
    "    # Define initial state distribution\n",
    "    # Uniform distribution in [-3, 3] x [-3, 3]\n",
    "    class UniformInitialState:\n",
    "        def __init__(self, low=-3.0, high=3.0, dim=2):\n",
    "            self.low = low\n",
    "            self.high = high\n",
    "            self.dim = dim\n",
    "        \n",
    "        def sample(self, shape):\n",
    "            return torch.empty((*shape, self.dim), dtype=torch.float64).uniform_(self.low, self.high)\n",
    "    \n",
    "    initial_states_dist = UniformInitialState()\n",
    "    \n",
    "    # Training parameters\n",
    "    epochs = 500\n",
    "    batch_size = 64\n",
    "    num_steps = 100  # N = 100 as specified\n",
    "    num_episodes = 16  # Number of episodes for Monte Carlo estimation\n",
    "    eval_freq = 25\n",
    "    \n",
    "    # Train the critic algorithm\n",
    "    print(\"Training critic algorithm...\")\n",
    "    losses = critic.train(epochs, batch_size, num_steps, num_episodes, \n",
    "                         initial_states_dist, eval_freq, verbose=True)\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.semilogy(losses)\n",
    "    plt.title('Critic Loss During Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate on test states\n",
    "    print(\"Evaluating on test states...\")\n",
    "    \n",
    "    # Generate grid of test states in [-3, 3] x [-3, 3]\n",
    "    x1 = torch.linspace(-3, 3, 11, dtype=torch.float64)\n",
    "    x2 = torch.linspace(-3, 3, 11, dtype=torch.float64)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    test_states = torch.stack([X1.flatten(), X2.flatten()], dim=1).to(device)\n",
    "    \n",
    "    # Evaluate the critic\n",
    "    errors = critic.evaluate(test_states, plot=True)\n",
    "    \n",
    "    # Plot value function\n",
    "    t0 = torch.zeros(test_states.shape[0], device=device, dtype=torch.float64)\n",
    "    true_values = soft_lqr.value_function(t0, test_states)\n",
    "    predicted_values = value_network.value_function(t0, test_states)\n",
    "    \n",
    "    # Convert to numpy for plotting\n",
    "    X1_np = X1.cpu().numpy()\n",
    "    X2_np = X2.cpu().numpy()\n",
    "    true_values_np = true_values.cpu().numpy().reshape(X1_np.shape)\n",
    "    predicted_values_np = predicted_values.cpu().numpy().reshape(X1_np.shape)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot true value function\n",
    "    plt.subplot(1, 2, 1)\n",
    "    contour = plt.contourf(X1_np, X2_np, true_values_np, 20, cmap='viridis')\n",
    "    plt.colorbar(contour)\n",
    "    plt.title('True Value Function')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    # Plot predicted value function\n",
    "    plt.subplot(1, 2, 2)\n",
    "    contour = plt.contourf(X1_np, X2_np, predicted_values_np, 20, cmap='viridis')\n",
    "    plt.colorbar(contour)\n",
    "    plt.title('Predicted Value Function')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_critic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUOptimizedCritic(CriticAlgorithm):\n",
    "    def compute_episode_values(self, initial_states: torch.Tensor, num_steps: int, \n",
    "                              num_episodes: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        GPU-optimized version that vectorizes operations across episodes and states.\n",
    "        \"\"\"\n",
    "        batch_size = initial_states.shape[0]\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Compute true value function at t=0, x=initial_states\n",
    "        t0 = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "        true_values = self.soft_lqr.value_function(t0, initial_states)\n",
    "        \n",
    "        # Initialize trajectories and accumulated costs\n",
    "        # Shape: [num_episodes, batch_size, num_steps + 1, 2]\n",
    "        states = torch.zeros((num_episodes, batch_size, num_steps + 1, 2), \n",
    "                             device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Expand initial states for all episodes\n",
    "        states[:, :, 0, :] = initial_states.unsqueeze(0).repeat(num_episodes, 1, 1)\n",
    "        \n",
    "        accumulated_costs = torch.zeros((num_episodes, batch_size), \n",
    "                                       device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Generate time grid\n",
    "        t_grid = torch.linspace(0, self.soft_lqr.T, num_steps + 1, \n",
    "                              device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Generate all Brownian increments at once\n",
    "        dW = torch.randn((num_episodes, batch_size, num_steps, self.soft_lqr.sigma.shape[1]), \n",
    "                        device=self.device, dtype=torch.float64) * np.sqrt(dt)\n",
    "        \n",
    "        # Pre-compute Cholesky decomposition of covariance if it's constant\n",
    "        covariance = self.soft_lqr.tau * self.soft_lqr.sigma_term\n",
    "        L_chol = torch.linalg.cholesky(covariance)\n",
    "        \n",
    "        # Simulate trajectories using explicit scheme - vectorized across all episodes\n",
    "        for n in range(num_steps):\n",
    "            # Current time\n",
    "            t_n = t_grid[n]\n",
    "            \n",
    "            # Get states at current time step\n",
    "            # Shape: [num_episodes, batch_size, 2]\n",
    "            current_states = states[:, :, n, :]\n",
    "            \n",
    "            # Reshape for batched processing\n",
    "            # Shape: [num_episodes*batch_size, 2]\n",
    "            flat_states = current_states.reshape(-1, 2)\n",
    "            flat_times = t_n.repeat(num_episodes * batch_size)\n",
    "            \n",
    "            # Get control means for all states\n",
    "            # Shape: [num_episodes*batch_size, control_dim]\n",
    "            means, _ = self.soft_lqr.optimal_control_distribution(flat_times, flat_states)\n",
    "            means = means.reshape(num_episodes, batch_size, -1)\n",
    "            \n",
    "            # Generate standard normal samples for all episodes/states at once\n",
    "            # Shape: [num_episodes, batch_size, control_dim]\n",
    "            z = torch.randn((num_episodes, batch_size, self.soft_lqr.m), \n",
    "                           device=self.device, dtype=torch.float64)\n",
    "            \n",
    "            # Transform to multivariate normal - vectorized\n",
    "            # Shape: [num_episodes, batch_size, control_dim]\n",
    "            controls = means + torch.matmul(z.view(-1, self.soft_lqr.m), L_chol.T).view(num_episodes, batch_size, -1)\n",
    "            \n",
    "            # Compute all drifts at once using broadcasting\n",
    "            # Shape: [num_episodes, batch_size, 2]\n",
    "            drifts = torch.matmul(current_states, self.soft_lqr.H.T) + torch.matmul(controls, self.soft_lqr.M.T)\n",
    "            \n",
    "            # Update all states at once\n",
    "            # Shape: [num_episodes, batch_size, 2]\n",
    "            states[:, :, n+1, :] = current_states + drifts * dt + torch.matmul(dW[:, :, n, :], self.soft_lqr.sigma.T)\n",
    "            \n",
    "            # Compute all costs at once\n",
    "            \n",
    "            # State costs: x^T C x\n",
    "            # Shape: [num_episodes, batch_size]\n",
    "            state_costs = torch.sum(current_states * torch.matmul(current_states, self.soft_lqr.C.T), dim=-1)\n",
    "            \n",
    "            # Control costs: u^T D u\n",
    "            # Shape: [num_episodes, batch_size]\n",
    "            control_costs = torch.sum(controls * torch.matmul(controls, self.soft_lqr.D.T), dim=-1)\n",
    "            \n",
    "            # Entropy regularization term - computed efficiently using matrix operations\n",
    "            # Shape: [num_episodes, batch_size]\n",
    "            control_diff = (controls - means).reshape(-1, self.soft_lqr.m)\n",
    "            inv_cov = torch.inverse(covariance)\n",
    "            quadratic_term = torch.sum(control_diff * torch.matmul(control_diff, inv_cov.T), dim=-1)\n",
    "            log_det_term = torch.log(torch.det(2 * np.pi * covariance))\n",
    "            log_probs = -0.5 * (quadratic_term + log_det_term)\n",
    "            entropy_terms = self.soft_lqr.tau * (-log_probs).reshape(num_episodes, batch_size)\n",
    "            \n",
    "            # Accumulate all costs\n",
    "            accumulated_costs += (state_costs + control_costs + entropy_terms) * dt\n",
    "        \n",
    "        # Compute terminal costs for all episodes at once\n",
    "        # Shape: [num_episodes, batch_size]\n",
    "        terminal_states = states[:, :, -1, :]\n",
    "        terminal_costs = torch.sum(terminal_states * torch.matmul(terminal_states, self.soft_lqr.R.T), dim=-1)\n",
    "        accumulated_costs += terminal_costs\n",
    "        \n",
    "        # Compute Monte Carlo estimates (mean over episodes)\n",
    "        mc_estimates = accumulated_costs.mean(dim=0)\n",
    "        \n",
    "        return true_values, mc_estimates, states\n",
    "        \n",
    "    def train_batch(self, initial_states: torch.Tensor, num_steps: int, \n",
    "                   num_episodes: int) -> float:\n",
    "        \"\"\"\n",
    "        Train on a batch of initial states with parallelized computation.\n",
    "        \"\"\"\n",
    "        batch_size = initial_states.shape[0]\n",
    "        \n",
    "        # Get Monte Carlo estimates\n",
    "        _, mc_estimates, states = self.compute_episode_values(\n",
    "            initial_states, num_steps, num_episodes)\n",
    "        \n",
    "        # Generate the time grid\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        t_grid = torch.linspace(0, self.soft_lqr.T, num_steps + 1, \n",
    "                              device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Compute predictions and loss for all time steps at once\n",
    "        all_losses = []\n",
    "        \n",
    "        for n in range(num_steps + 1):\n",
    "            # Current time and states\n",
    "            t_n = torch.ones(batch_size, device=self.device, dtype=torch.float64) * t_grid[n]\n",
    "            x_n = states[0, :, n, :]  # Use first episode\n",
    "            \n",
    "            # Get value function predictions\n",
    "            predicted = self.value_network.value_function(t_n, x_n)\n",
    "            \n",
    "            # Terminal time - target is terminal cost\n",
    "            if n == num_steps:\n",
    "                target = torch.sum(x_n * torch.matmul(x_n, self.soft_lqr.R.T), dim=-1)\n",
    "            else:\n",
    "                # For non-terminal, we need to collect remaining costs\n",
    "                # This is still done with loops, but we could further optimize with custom CUDA kernels\n",
    "                remaining_costs = torch.zeros((num_episodes, batch_size), \n",
    "                                           device=self.device, dtype=torch.float64)\n",
    "                \n",
    "                for i in range(num_episodes):\n",
    "                    # Extract trajectory for this episode from n onwards\n",
    "                    traj = states[i, :, n:, :]\n",
    "                    \n",
    "                    # Time points from n onwards\n",
    "                    time_points = t_grid[n:]\n",
    "                    \n",
    "                    # Compute costs for all remaining steps\n",
    "                    for k in range(len(time_points)-1):\n",
    "                        t_k = time_points[k]\n",
    "                        state_k = traj[:, k, :]\n",
    "                        \n",
    "                        # Reshape for batched processing\n",
    "                        flat_t_k = t_k.repeat(batch_size)\n",
    "                        \n",
    "                        # Get control means\n",
    "                        means, covariance = self.soft_lqr.optimal_control_distribution(flat_t_k, state_k)\n",
    "                        \n",
    "                        # Sample controls using pre-computed Cholesky\n",
    "                        L = torch.linalg.cholesky(covariance)\n",
    "                        z = torch.randn((batch_size, self.soft_lqr.m), device=self.device, dtype=torch.float64)\n",
    "                        controls = means + torch.matmul(z, L.T)\n",
    "                        \n",
    "                        # State costs - vectorized\n",
    "                        state_costs = torch.sum(state_k * torch.matmul(state_k, self.soft_lqr.C.T), dim=-1)\n",
    "                        \n",
    "                        # Control costs - vectorized\n",
    "                        control_costs = torch.sum(controls * torch.matmul(controls, self.soft_lqr.D.T), dim=-1)\n",
    "                        \n",
    "                        # Entropy term - vectorized\n",
    "                        control_diff = controls - means\n",
    "                        inv_cov = torch.inverse(covariance)\n",
    "                        quadratic = torch.sum(control_diff * torch.matmul(control_diff, inv_cov.T), dim=-1)\n",
    "                        log_det = torch.log(torch.det(2 * np.pi * covariance))\n",
    "                        entropy = self.soft_lqr.tau * (0.5 * (quadratic + log_det))\n",
    "                        \n",
    "                        # Add costs\n",
    "                        remaining_costs[i, :] += (state_costs + control_costs + entropy) * dt\n",
    "                    \n",
    "                    # Add terminal costs\n",
    "                    terminal_states = traj[:, -1, :]\n",
    "                    terminal_costs = torch.sum(terminal_states * torch.matmul(terminal_states, self.soft_lqr.R.T), dim=-1)\n",
    "                    remaining_costs[i, :] += terminal_costs\n",
    "                \n",
    "                # Average over episodes\n",
    "                target = remaining_costs.mean(dim=0)\n",
    "            \n",
    "            # Compute MSE loss\n",
    "            loss = ((predicted - target) ** 2).mean()\n",
    "            all_losses.append(loss)\n",
    "        \n",
    "        # Sum losses over all time steps\n",
    "        total_loss = torch.stack(all_losses).sum()\n",
    "        \n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return total_loss.item() / (num_steps + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PyTorch's automatic mixed precision\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "class MixedPrecisionCritic(GPUOptimizedCritic):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Create a gradient scaler for mixed precision training\n",
    "        self.scaler = GradScaler()\n",
    "    \n",
    "    def train_batch(self, initial_states: torch.Tensor, num_steps: int, \n",
    "                   num_episodes: int) -> float:\n",
    "        \"\"\"\n",
    "        Train with mixed precision for better GPU utilization.\n",
    "        \"\"\"\n",
    "        batch_size = initial_states.shape[0]\n",
    "        \n",
    "        # The Monte Carlo simulation should still use fp64 for accuracy\n",
    "        # but we'll cast to fp32 for the neural network operations\n",
    "        initial_states_fp32 = initial_states.to(torch.float32)\n",
    "        \n",
    "        # Get Monte Carlo estimates\n",
    "        _, mc_estimates, states = self.compute_episode_values(\n",
    "            initial_states, num_steps, num_episodes)\n",
    "        \n",
    "        # Cast to fp32 for neural network training\n",
    "        mc_estimates_fp32 = mc_estimates.to(torch.float32)\n",
    "        states_fp32 = states.to(torch.float32)\n",
    "        \n",
    "        # Generate the time grid\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        t_grid = torch.linspace(0, self.soft_lqr.T, num_steps + 1, \n",
    "                              device=self.device, dtype=torch.float32)\n",
    "        \n",
    "        # Prepare for backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Total loss across all time steps\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        # Use automatic mixed precision\n",
    "        with autocast():\n",
    "            for n in range(num_steps + 1):\n",
    "                # Current time and states\n",
    "                t_n = torch.ones(batch_size, device=self.device, dtype=torch.float32) * t_grid[n]\n",
    "                x_n = states_fp32[0, :, n, :]  # Use first episode\n",
    "                \n",
    "                # Get value function predictions\n",
    "                predicted = self.value_network.value_function(t_n, x_n)\n",
    "                \n",
    "                # Terminal time - target is terminal cost\n",
    "                if n == num_steps:\n",
    "                    # Compute terminal cost directly\n",
    "                    R_fp32 = self.soft_lqr.R.to(torch.float32)\n",
    "                    target = torch.sum(x_n * torch.matmul(x_n, R_fp32.T), dim=-1)\n",
    "                else:\n",
    "                    # For non-terminal, we can use targets from our MC estimation\n",
    "                    # This can be further optimized, but we simplify for illustration\n",
    "                    target = torch.zeros_like(predicted)\n",
    "                    \n",
    "                    # Compute remaining cost from this point onward\n",
    "                    for i in range(batch_size):\n",
    "                        remaining_idx = range(n, num_steps + 1)\n",
    "                        target[i] = mc_estimates_fp32[i] * (len(remaining_idx) / (num_steps + 1))\n",
    "                \n",
    "                # Compute MSE loss\n",
    "                loss = ((predicted - target) ** 2).mean()\n",
    "                total_loss = total_loss + loss\n",
    "        \n",
    "        # Average loss\n",
    "        avg_loss = total_loss / (num_steps + 1)\n",
    "        \n",
    "        # Scale gradients and optimize with mixed precision\n",
    "        self.scaler.scale(avg_loss).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        \n",
    "        return avg_loss.item()\n",
    "        \n",
    "    def train(self, epochs: int, batch_size: int, num_steps: int, num_episodes: int, \n",
    "             initial_states_dist, eval_freq: int = 10, verbose: bool = True) -> List[float]:\n",
    "        \"\"\"\n",
    "        Train with mixed precision and learning rate scheduling.\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        # Add learning rate scheduler for better convergence\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, 'min', factor=0.5, patience=20, verbose=verbose\n",
    "        )\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Sample initial states\n",
    "            initial_states = initial_states_dist.sample((batch_size,)).to(self.device)\n",
    "            \n",
    "            # Perform one training step\n",
    "            loss = self.train_batch(initial_states, num_steps, num_episodes)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Update learning rate based on performance\n",
    "            scheduler.step(loss)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch+1) % eval_freq == 0:\n",
    "                current_lr = self.optimizer.param_groups[0]['lr']\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4e}, LR: {current_lr:.4e}\")\n",
    "        \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_critic_gpu():\n",
    "    # Ensure we use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    \n",
    "    # Problem setup as specified in the exercise\n",
    "    H = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64, device=device) * 0.5\n",
    "    M = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64, device=device)\n",
    "    sigma = torch.eye(2, dtype=torch.float64, device=device) * 0.5\n",
    "    C = torch.tensor([[1.0, 0.1], [0.1, 1.0]], dtype=torch.float64, device=device) * 1.0\n",
    "    D = torch.eye(2, dtype=torch.float64, device=device)  # D just identity, as specified\n",
    "    R = torch.tensor([[1.0, 0.3], [0.3, 1.0]], dtype=torch.float64, device=device) * 10.0\n",
    "    \n",
    "    # Set terminal time and time grid\n",
    "    T = 0.5\n",
    "    grid_size = 1000\n",
    "    time_grid = torch.linspace(0, T, grid_size, dtype=torch.float64, device=device)\n",
    "    \n",
    "    # Set entropy regularization parameters\n",
    "    tau = 0.5  # τ = 1/2 as specified\n",
    "    gamma = 1.0  # γ = 1 as specified\n",
    "    \n",
    "    # Create GPU-accelerated soft LQR instance\n",
    "    soft_lqr = GPUSoftLQR(\n",
    "        H, M, sigma, C, D, R, T, time_grid, tau, gamma, device=device\n",
    "    )\n",
    "    \n",
    "    # Solve Ricatti ODE using GPU\n",
    "    print(\"Solving Ricatti ODE on GPU...\")\n",
    "    with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "        soft_lqr.solve_ricatti()\n",
    "    \n",
    "    # Create value network with larger capacity for GPU\n",
    "    hidden_size = 1024  # Increased from 512 for GPU\n",
    "    value_network = ValueNetwork(hidden_size=hidden_size, device=device)\n",
    "    \n",
    "    # Create mixed precision critic algorithm\n",
    "    learning_rate = 2e-3  # Slightly higher for mixed precision\n",
    "    critic = MixedPrecisionCritic(soft_lqr, value_network, learning_rate, device)\n",
    "    \n",
    "    # Define initial state distribution\n",
    "    class UniformInitialState:\n",
    "        def __init__(self, low=-3.0, high=3.0, dim=2, device=device):\n",
    "            self.low = low\n",
    "            self.high = high\n",
    "            self.dim = dim\n",
    "            self.device = device\n",
    "        \n",
    "        def sample(self, shape):\n",
    "            return torch.empty(\n",
    "                (*shape, self.dim), \n",
    "                dtype=torch.float64, \n",
    "                device=self.device\n",
    "            ).uniform_(self.low, self.high)\n",
    "    \n",
    "    initial_states_dist = UniformInitialState(device=device)\n",
    "    \n",
    "    # Training parameters - larger batch and parallel processing for GPU\n",
    "    epochs = 500\n",
    "    batch_size = 256  # Increased from 64 for GPU\n",
    "    num_steps = 100  # N = 100 as specified\n",
    "    num_episodes = 64  # Increased from 16 for GPU\n",
    "    eval_freq = 25\n",
    "    \n",
    "    # Create a CUDA events for timing\n",
    "    if torch.cuda.is_available():\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "        start_event.record()\n",
    "    \n",
    "    # Train with mixed precision on GPU\n",
    "    print(\"Training critic algorithm with GPU acceleration...\")\n",
    "    losses = critic.train(\n",
    "        epochs, batch_size, num_steps, num_episodes, \n",
    "        initial_states_dist, eval_freq, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Record time\n",
    "    if torch.cuda.is_available():\n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "        training_time = start_event.elapsed_time(end_event) / 1000  # Convert to seconds\n",
    "        print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Plot loss with PyTorch-Lightning's rich logging\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.semilogy(losses)\n",
    "    plt.title('Critic Loss During Training (GPU Accelerated)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate on larger test grid for better visualization\n",
    "    print(\"Evaluating on test grid with GPU...\")\n",
    "    \n",
    "    # Generate dense grid of test states in [-3, 3] x [-3, 3]\n",
    "    x1 = torch.linspace(-3, 3, 21, dtype=torch.float64, device=device)\n",
    "    x2 = torch.linspace(-3, 3, 21, dtype=torch.float64, device=device)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    test_states = torch.stack([X1.flatten(), X2.flatten()], dim=1)\n",
    "    \n",
    "    # Evaluate the critic using GPU\n",
    "    with torch.cuda.amp.autocast('cuda', enabled=torch.cuda.is_available()):\n",
    "        errors = critic.evaluate(test_states, plot=True)\n",
    "    \n",
    "    # Create interactive 3D plot of value function\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    \n",
    "    t0 = torch.zeros(test_states.shape[0], device=device, dtype=torch.float64)\n",
    "    \n",
    "    # Get values in mixed precision for GPU efficiency\n",
    "    with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "        true_values = soft_lqr.value_function(t0, test_states)\n",
    "        predicted_values = value_network.value_function(t0.to(torch.float32), test_states.to(torch.float32))\n",
    "    \n",
    "    # Convert to numpy for plotting\n",
    "    X1_np = X1.cpu().numpy()\n",
    "    X2_np = X2.cpu().numpy()\n",
    "    true_values_np = true_values.cpu().numpy().reshape(X1_np.shape)\n",
    "    predicted_values_np = predicted_values.cpu().numpy().reshape(X1_np.shape)\n",
    "    \n",
    "    # Create 3D plots\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # True value function\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    surf1 = ax1.plot_surface(X1_np, X2_np, true_values_np, cmap='viridis', \n",
    "                            linewidth=0, antialiased=True)\n",
    "    ax1.set_title('True Value Function')\n",
    "    ax1.set_xlabel('x1')\n",
    "    ax1.set_ylabel('x2')\n",
    "    ax1.set_zlabel('Value')\n",
    "    fig.colorbar(surf1, ax=ax1, shrink=0.5, aspect=5)\n",
    "    \n",
    "    # Predicted value function\n",
    "    ax2 = fig.add_subplot(122, projection='3d')\n",
    "    surf2 = ax2.plot_surface(X1_np, X2_np, predicted_values_np, cmap='plasma', \n",
    "                            linewidth=0, antialiased=True)\n",
    "    ax2.set_title('Predicted Value Function')\n",
    "    ax2.set_xlabel('x1')\n",
    "    ax2.set_ylabel('x2')\n",
    "    ax2.set_zlabel('Value')\n",
    "    fig.colorbar(surf2, ax=ax2, shrink=0.5, aspect=5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"GPU-accelerated critic training completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_critic_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size=256, state_dim=2, control_dim=2, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Neural network to learn the policy function for the soft LQR problem.\n",
    "        \n",
    "        Args:\n",
    "            hidden_size: Size of the hidden layers\n",
    "            state_dim: Dimension of the state space\n",
    "            control_dim: Dimension of the control space\n",
    "            device: Device to run the network on\n",
    "        \"\"\"\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        # Store dimensions\n",
    "        self.state_dim = state_dim\n",
    "        self.control_dim = control_dim\n",
    "        self.device = device\n",
    "        \n",
    "        # Define layers for time input\n",
    "        self.time_layer1 = nn.Linear(1, hidden_size).to(device)\n",
    "        self.time_layer2 = nn.Linear(hidden_size, hidden_size).to(device)\n",
    "        \n",
    "        # Define layers for phi matrix\n",
    "        # Phi maps from state to control mean\n",
    "        self.phi_output = nn.Linear(hidden_size, state_dim * control_dim).to(device)\n",
    "        \n",
    "        # Define layers for the L matrix of the covariance\n",
    "        # We use a lower triangular matrix L to ensure positive-definiteness via Σ = LL^T\n",
    "        self.L_output = nn.Linear(\n",
    "            hidden_size, control_dim * (control_dim + 1) // 2\n",
    "        ).to(device)\n",
    "        \n",
    "        # Precompute indices for the lower triangular matrix\n",
    "        self.tril_indices = torch.tril_indices(control_dim, control_dim).to(device)\n",
    "    \n",
    "    def forward(self, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass through the network to get the policy parameters.\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (phi, Sigma) for the policy\n",
    "        \"\"\"\n",
    "        # Ensure t is a column vector\n",
    "        t = t.view(-1, 1).to(self.device)\n",
    "        \n",
    "        # Forward pass through the hidden layers\n",
    "        hidden = torch.relu(self.time_layer1(t))\n",
    "        hidden = torch.relu(self.time_layer2(hidden))\n",
    "        \n",
    "        # Compute phi matrix\n",
    "        phi_flat = self.phi_output(hidden)\n",
    "        phi = phi_flat.view(-1, self.control_dim, self.state_dim)\n",
    "        \n",
    "        # Compute L matrix for covariance\n",
    "        L_flat = self.L_output(hidden)\n",
    "        \n",
    "        # Create lower triangular matrices L for each batch element\n",
    "        batch_size = t.shape[0]\n",
    "        L = torch.zeros(batch_size, self.control_dim, self.control_dim, \n",
    "                      device=self.device, dtype=t.dtype)\n",
    "        \n",
    "        # Fill the lower triangular part\n",
    "        L[:, self.tril_indices[0], self.tril_indices[1]] = L_flat\n",
    "        \n",
    "        # Compute Sigma = LL^T to ensure positive-definiteness\n",
    "        Sigma = torch.bmm(L, L.transpose(1, 2))\n",
    "        \n",
    "        # Add small constant to diagonal for stability\n",
    "        eye = torch.eye(self.control_dim, device=self.device).unsqueeze(0)\n",
    "        eye = eye.expand(batch_size, -1, -1)\n",
    "        Sigma = Sigma + 1e-3 * eye\n",
    "        \n",
    "        return phi, Sigma\n",
    "    \n",
    "    def get_action_distribution(self, t: torch.Tensor, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get the action distribution parameters for given time and state.\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            x: State tensor (batch x state_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (mean, covariance) for the control distribution\n",
    "        \"\"\"\n",
    "        # Get policy parameters\n",
    "        phi, Sigma = self.forward(t)\n",
    "        \n",
    "        # Compute mean = -phi @ x for each batch element\n",
    "        batch_size = x.shape[0]\n",
    "        means = torch.zeros(batch_size, self.control_dim, device=self.device, dtype=x.dtype)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            means[i] = -torch.mv(phi[i], x[i])\n",
    "        \n",
    "        return means, Sigma\n",
    "    \n",
    "    def sample_action(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample actions from the policy distribution.\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            x: State tensor (batch x state_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Sampled actions (batch x control_dim)\n",
    "        \"\"\"\n",
    "        # Get distribution parameters\n",
    "        means, covariances = self.get_action_distribution(t, x)\n",
    "        \n",
    "        # Sample from multivariate normal\n",
    "        batch_size = means.shape[0]\n",
    "        samples = torch.zeros_like(means)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Compute Cholesky decomposition\n",
    "            L = torch.linalg.cholesky(covariances[i])\n",
    "            \n",
    "            # Generate standard normal samples\n",
    "            z = torch.randn(self.control_dim, device=self.device, dtype=means.dtype)\n",
    "            \n",
    "            # Transform to multivariate normal\n",
    "            samples[i] = means[i] + torch.mv(L, z)\n",
    "        \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorAlgorithm:\n",
    "    def __init__(self, soft_lqr: SoftLQR, policy_network: PolicyNetwork, \n",
    "                 learning_rate: float = 1e-4, device: str = \"cpu\"):\n",
    "        \"\"\"\n",
    "        Actor algorithm to learn the optimal control policy using the optimal value function.\n",
    "        \n",
    "        Args:\n",
    "            soft_lqr: Soft LQR instance with the optimal value function\n",
    "            policy_network: Neural network to approximate the policy\n",
    "            learning_rate: Learning rate for the optimizer\n",
    "            device: Device to run the algorithm on\n",
    "        \"\"\"\n",
    "        self.soft_lqr = soft_lqr\n",
    "        self.policy_network = policy_network\n",
    "        self.device = device\n",
    "        \n",
    "        # Move soft_lqr matrices to the device if needed\n",
    "        self.soft_lqr.H = self.soft_lqr.H.to(device)\n",
    "        self.soft_lqr.M = self.soft_lqr.M.to(device)\n",
    "        self.soft_lqr.sigma = self.soft_lqr.sigma.to(device)\n",
    "        self.soft_lqr.C = self.soft_lqr.C.to(device)\n",
    "        self.soft_lqr.D = self.soft_lqr.D.to(device)\n",
    "        self.soft_lqr.R = self.soft_lqr.R.to(device)\n",
    "        self.soft_lqr.D_inv = self.soft_lqr.D_inv.to(device)\n",
    "        self.soft_lqr.sigma_inv = self.soft_lqr.sigma_inv.to(device)\n",
    "        self.soft_lqr.sigma_term = self.soft_lqr.sigma_term.to(device)\n",
    "        if hasattr(self.soft_lqr, 'S_grid') and self.soft_lqr.S_grid is not None:\n",
    "            self.soft_lqr.S_grid = self.soft_lqr.S_grid.to(device)\n",
    "        if hasattr(self.soft_lqr, 'int_term_grid') and self.soft_lqr.int_term_grid is not None:\n",
    "            self.soft_lqr.int_term_grid = self.soft_lqr.int_term_grid.to(device)\n",
    "        \n",
    "        # Setup optimizer\n",
    "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def compute_value_gradient(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the gradient of the value function with respect to the state.\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            x: State tensor (batch x state_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Gradient of value function (batch x state_dim)\n",
    "        \"\"\"\n",
    "        # Get S matrices at the specified times\n",
    "        S_matrices = self.soft_lqr.get_S_at_time(t)\n",
    "        \n",
    "        # Compute gradient: ∇_x v(t,x) = 2 * S(t) * x\n",
    "        batch_size = x.shape[0]\n",
    "        gradients = torch.zeros_like(x)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            gradients[i] = 2 * torch.mv(S_matrices[i], x[i])\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def simulate_trajectory(self, initial_states: torch.Tensor, num_steps: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Simulate trajectory using the current policy.\n",
    "        \n",
    "        Args:\n",
    "            initial_states: Initial states tensor (batch x state_dim)\n",
    "            num_steps: Number of time steps for simulation\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (states, actions, time_points)\n",
    "        \"\"\"\n",
    "        batch_size = initial_states.shape[0]\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Generate time grid\n",
    "        time_grid = torch.linspace(0, self.soft_lqr.T, num_steps + 1, \n",
    "                                  device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Initialize states and actions\n",
    "        states = torch.zeros((batch_size, num_steps + 1, self.policy_network.state_dim), \n",
    "                           device=self.device, dtype=torch.float64)\n",
    "        states[:, 0, :] = initial_states\n",
    "        \n",
    "        actions = torch.zeros((batch_size, num_steps, self.policy_network.control_dim), \n",
    "                            device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Generate Brownian increments\n",
    "        dW = torch.randn((batch_size, num_steps, self.soft_lqr.sigma.shape[1]), \n",
    "                        device=self.device, dtype=torch.float64) * np.sqrt(dt)\n",
    "        \n",
    "        # Simulate trajectory using explicit scheme\n",
    "        for n in range(num_steps):\n",
    "            # Current time and states\n",
    "            t_n = time_grid[n]\n",
    "            x_n = states[:, n, :]\n",
    "            \n",
    "            # Sample actions from policy\n",
    "            t_batch = t_n.repeat(batch_size)\n",
    "            actions[:, n, :] = self.policy_network.sample_action(t_batch, x_n)\n",
    "            \n",
    "            # Update states\n",
    "            for i in range(batch_size):\n",
    "                # Compute drift term: HX + Ma\n",
    "                drift = self.soft_lqr.H @ states[i, n, :] + self.soft_lqr.M @ actions[i, n, :]\n",
    "                \n",
    "                # Update state using explicit scheme\n",
    "                states[i, n+1, :] = states[i, n, :] + drift * dt + self.soft_lqr.sigma @ dW[i, n, :]\n",
    "        \n",
    "        return states, actions, time_grid\n",
    "    \n",
    "    def compute_policy_loss(self, states: torch.Tensor, actions: torch.Tensor, \n",
    "                           time_grid: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the loss for policy optimization.\n",
    "        \n",
    "        Args:\n",
    "            states: States tensor (batch x time_steps+1 x state_dim)\n",
    "            actions: Actions tensor (batch x time_steps x control_dim)\n",
    "            time_grid: Time grid tensor (time_steps+1)\n",
    "            \n",
    "        Returns:\n",
    "            Policy loss\n",
    "        \"\"\"\n",
    "        batch_size = states.shape[0]\n",
    "        num_steps = actions.shape[1]\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Initialize loss\n",
    "        total_loss = torch.tensor(0.0, device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Compute loss at each time step\n",
    "        for n in range(num_steps):\n",
    "            # Current time, states, and actions\n",
    "            t_n = time_grid[n]\n",
    "            x_n = states[:, n, :]\n",
    "            a_n = actions[:, n, :]\n",
    "            \n",
    "            # Create batch of times\n",
    "            t_batch = t_n.repeat(batch_size)\n",
    "            \n",
    "            # 1. Compute state cost: x^T C x\n",
    "            state_costs = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "            for i in range(batch_size):\n",
    "                state_costs[i] = x_n[i] @ self.soft_lqr.C @ x_n[i]\n",
    "            \n",
    "            # 2. Compute control cost: a^T D a\n",
    "            control_costs = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "            for i in range(batch_size):\n",
    "                control_costs[i] = a_n[i] @ self.soft_lqr.D @ a_n[i]\n",
    "            \n",
    "            # 3. Compute value function gradient\n",
    "            value_gradients = self.compute_value_gradient(t_batch, x_n)\n",
    "            \n",
    "            # 4. Compute policy distribution parameters\n",
    "            means, covariances = self.policy_network.get_action_distribution(t_batch, x_n)\n",
    "            \n",
    "            # 5. Compute log probability of the actions\n",
    "            log_probs = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "            for i in range(batch_size):\n",
    "                # Log probability of multivariate normal\n",
    "                diff = a_n[i] - means[i]\n",
    "                inv_cov = torch.inverse(covariances[i])\n",
    "                \n",
    "                # Compute quadratic term: (a - μ)^T Σ^(-1) (a - μ)\n",
    "                quad_term = diff @ inv_cov @ diff\n",
    "                \n",
    "                # Compute log determinant term: log(det(2π Σ))\n",
    "                log_det_term = torch.log(torch.det(2 * np.pi * covariances[i]))\n",
    "                \n",
    "                # Log probability\n",
    "                log_probs[i] = -0.5 * (quad_term + log_det_term)\n",
    "            \n",
    "            # 6. Compute entropy regularization term\n",
    "            entropy_terms = self.soft_lqr.tau * (-log_probs)\n",
    "            \n",
    "            # 7. Compute value function drift term for each state-action pair\n",
    "            drift_terms = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "            for i in range(batch_size):\n",
    "                # Compute drift: Hx + Ma\n",
    "                drift = self.soft_lqr.H @ x_n[i] + self.soft_lqr.M @ a_n[i]\n",
    "                \n",
    "                # Compute drift term: ∇_x v(t,x)^T (Hx + Ma)\n",
    "                drift_terms[i] = value_gradients[i] @ drift\n",
    "            \n",
    "            # 8. Compute total loss for this time step\n",
    "            step_loss = (state_costs + control_costs + entropy_terms + drift_terms).mean()\n",
    "            \n",
    "            # 9. Accumulate loss with time step\n",
    "            total_loss = total_loss + step_loss * dt\n",
    "        \n",
    "        # Terminal cost is not included in the policy loss since it doesn't depend on the policy\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def train_step(self, initial_states_dist, num_states: int, num_steps: int) -> float:\n",
    "        \"\"\"\n",
    "        Perform one training step of the actor algorithm.\n",
    "        \n",
    "        Args:\n",
    "            initial_states_dist: Distribution to sample initial states from\n",
    "            num_states: Number of initial states to sample\n",
    "            num_steps: Number of time steps for simulation\n",
    "            \n",
    "        Returns:\n",
    "            Loss value for this step\n",
    "        \"\"\"\n",
    "        # Sample initial states\n",
    "        initial_states = initial_states_dist.sample((num_states,)).to(self.device)\n",
    "        \n",
    "        # Simulate trajectory using current policy\n",
    "        states, actions, time_grid = self.simulate_trajectory(initial_states, num_steps)\n",
    "        \n",
    "        # Compute policy loss\n",
    "        loss = self.compute_policy_loss(states, actions, time_grid)\n",
    "        \n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def train(self, epochs: int, batch_size: int, num_steps: int, \n",
    "             initial_states_dist, eval_freq: int = 10, verbose: bool = True) -> List[float]:\n",
    "        \"\"\"\n",
    "        Train the actor algorithm for multiple epochs.\n",
    "        \n",
    "        Args:\n",
    "            epochs: Number of training epochs\n",
    "            batch_size: Number of initial states per batch\n",
    "            num_steps: Number of time steps for simulation\n",
    "            initial_states_dist: Distribution to sample initial states from\n",
    "            eval_freq: Frequency of evaluation\n",
    "            verbose: Whether to print progress\n",
    "            \n",
    "        Returns:\n",
    "            List of loss values during training\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Perform one training step\n",
    "            loss = self.train_step(initial_states_dist, batch_size, num_steps)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch+1) % eval_freq == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.6f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def evaluate_policy(self, test_states: torch.Tensor, plot: bool = True) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the learned policy against the optimal policy from soft LQR.\n",
    "        \n",
    "        Args:\n",
    "            test_states: Test states tensor (batch x state_dim)\n",
    "            plot: Whether to plot the results\n",
    "            \n",
    "        Returns:\n",
    "            Mean error between learned and optimal policies\n",
    "        \"\"\"\n",
    "        # Set evaluation mode\n",
    "        self.policy_network.eval()\n",
    "        \n",
    "        batch_size = test_states.shape[0]\n",
    "        t0 = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Get parameters from learned policy\n",
    "        learned_means, learned_covs = self.policy_network.get_action_distribution(t0, test_states)\n",
    "        \n",
    "        # Get parameters from optimal policy\n",
    "        optimal_means, optimal_cov = self.soft_lqr.optimal_control_distribution(t0, test_states)\n",
    "        \n",
    "        # Compute errors in means\n",
    "        mean_errors = torch.norm(learned_means - optimal_means, dim=1)\n",
    "        mean_error_avg = mean_errors.mean().item()\n",
    "        \n",
    "        # Compute errors in covariances (Frobenius norm)\n",
    "        cov_errors = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "        for i in range(batch_size):\n",
    "            diff = learned_covs[i] - optimal_cov\n",
    "            cov_errors[i] = torch.norm(diff, p='fro')\n",
    "        cov_error_avg = cov_errors.mean().item()\n",
    "        \n",
    "        print(f\"Evaluation results:\")\n",
    "        print(f\"  Mean error in policy means: {mean_error_avg:.6f}\")\n",
    "        print(f\"  Mean error in policy covariances: {cov_error_avg:.6f}\")\n",
    "        \n",
    "        if plot:\n",
    "            # Convert to numpy for plotting\n",
    "            test_states_np = test_states.cpu().numpy()\n",
    "            learned_means_np = learned_means.detach().cpu().numpy()\n",
    "            optimal_means_np = optimal_means.detach().cpu().numpy()\n",
    "            mean_errors_np = mean_errors.detach().cpu().numpy()\n",
    "            \n",
    "            # Create scatter plot of errors vs state norm\n",
    "            state_norms = np.linalg.norm(test_states_np, axis=1)\n",
    "            \n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            # Plot mean errors vs state norm\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.scatter(state_norms, mean_errors_np, alpha=0.7)\n",
    "            plt.title('Policy Mean Errors vs State Norm')\n",
    "            plt.xlabel('State Norm')\n",
    "            plt.ylabel('Mean Error')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            # Plot learned vs optimal means for first control dimension\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.scatter(optimal_means_np[:, 0], learned_means_np[:, 0], alpha=0.7)\n",
    "            min_val = min(optimal_means_np[:, 0].min(), learned_means_np[:, 0].min())\n",
    "            max_val = max(optimal_means_np[:, 0].max(), learned_means_np[:, 0].max())\n",
    "            plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "            plt.title('Learned vs Optimal Policy Means (First Control Dimension)')\n",
    "            plt.xlabel('Optimal Mean')\n",
    "            plt.ylabel('Learned Mean')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # If state dimension is 2, visualize the policy in state space\n",
    "            if test_states.shape[1] == 2 and isinstance(test_states_np, np.ndarray):\n",
    "                # Check if states form a grid\n",
    "                unique_x1 = np.unique(test_states_np[:, 0])\n",
    "                unique_x2 = np.unique(test_states_np[:, 1])\n",
    "                \n",
    "                if len(unique_x1) * len(unique_x2) == test_states_np.shape[0]:\n",
    "                    # Reshape for grid\n",
    "                    X1, X2 = np.meshgrid(unique_x1, unique_x2)\n",
    "                    \n",
    "                    # Reshape means for quiver plot\n",
    "                    optimal_u = optimal_means_np[:, 0].reshape(len(unique_x2), len(unique_x1))\n",
    "                    optimal_v = optimal_means_np[:, 1].reshape(len(unique_x2), len(unique_x1))\n",
    "                    \n",
    "                    learned_u = learned_means_np[:, 0].reshape(len(unique_x2), len(unique_x1))\n",
    "                    learned_v = learned_means_np[:, 1].reshape(len(unique_x2), len(unique_x1))\n",
    "                    \n",
    "                    plt.figure(figsize=(12, 5))\n",
    "                    \n",
    "                    # Plot optimal policy\n",
    "                    plt.subplot(1, 2, 1)\n",
    "                    plt.quiver(X1, X2, optimal_u, optimal_v)\n",
    "                    plt.title('Optimal Policy')\n",
    "                    plt.xlabel('x1')\n",
    "                    plt.ylabel('x2')\n",
    "                    plt.grid(True)\n",
    "                    \n",
    "                    # Plot learned policy\n",
    "                    plt.subplot(1, 2, 2)\n",
    "                    plt.quiver(X1, X2, learned_u, learned_v)\n",
    "                    plt.title('Learned Policy')\n",
    "                    plt.xlabel('x1')\n",
    "                    plt.ylabel('x2')\n",
    "                    plt.grid(True)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "        \n",
    "        # Switch back to training mode\n",
    "        self.policy_network.train()\n",
    "        \n",
    "        return mean_error_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUActorAlgorithm(ActorAlgorithm):\n",
    "    def __init__(self, soft_lqr: SoftLQR, policy_network: PolicyNetwork, \n",
    "                 learning_rate: float = 1e-4, device: str = \"cuda\"):\n",
    "        \"\"\"\n",
    "        GPU-optimized actor algorithm for the soft LQR problem.\n",
    "        \n",
    "        Args:\n",
    "            soft_lqr: Soft LQR instance with the optimal value function\n",
    "            policy_network: Neural network to approximate the policy\n",
    "            learning_rate: Learning rate for the optimizer\n",
    "            device: Device to run the algorithm on (default: \"cuda\")\n",
    "        \"\"\"\n",
    "        super().__init__(soft_lqr, policy_network, learning_rate, device)\n",
    "        \n",
    "        # Create gradient scaler for mixed precision training\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "        # Precompute Cholesky for optimal covariance if it's constant\n",
    "        self.optimal_cov = self.soft_lqr.tau * self.soft_lqr.sigma_term\n",
    "        self.optimal_L = torch.linalg.cholesky(self.optimal_cov)\n",
    "        \n",
    "        # Create optimizer with weight decay for regularization\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.policy_network.parameters(), \n",
    "            lr=learning_rate,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "        \n",
    "        # Create learning rate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, 'min', factor=0.5, patience=20, verbose=True\n",
    "        )\n",
    "    \n",
    "    def compute_value_gradient(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        GPU-optimized computation of value function gradient.\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            x: State tensor (batch x state_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Gradient of value function (batch x state_dim)\n",
    "        \"\"\"\n",
    "        # Get S matrices at the specified times\n",
    "        S_matrices = self.soft_lqr.get_S_at_time(t)\n",
    "        \n",
    "        # Vectorized computation: ∇_x v(t,x) = 2 * S(t) * x\n",
    "        # Reshape S_matrices to batch x state_dim x state_dim\n",
    "        S_reshaped = S_matrices.view(-1, self.policy_network.state_dim, self.policy_network.state_dim)\n",
    "        \n",
    "        # Compute gradient using batched matrix-vector multiplication\n",
    "        x_unsqueezed = x.unsqueeze(2)  # batch x state_dim x 1\n",
    "        gradients = 2 * torch.bmm(S_reshaped, x_unsqueezed).squeeze(2)  # batch x state_dim\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def simulate_trajectory(self, initial_states: torch.Tensor, num_steps: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        GPU-optimized trajectory simulation using vectorized operations.\n",
    "        \n",
    "        Args:\n",
    "            initial_states: Initial states tensor (batch x state_dim)\n",
    "            num_steps: Number of time steps for simulation\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (states, actions, time_points)\n",
    "        \"\"\"\n",
    "        batch_size = initial_states.shape[0]\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Generate time grid\n",
    "        time_grid = torch.linspace(0, self.soft_lqr.T, num_steps + 1, \n",
    "                                  device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Initialize states and actions\n",
    "        states = torch.zeros((batch_size, num_steps + 1, self.policy_network.state_dim), \n",
    "                           device=self.device, dtype=torch.float64)\n",
    "        states[:, 0, :] = initial_states\n",
    "        \n",
    "        actions = torch.zeros((batch_size, num_steps, self.policy_network.control_dim), \n",
    "                            device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Generate all Brownian increments at once\n",
    "        dW = torch.randn((batch_size, num_steps, self.soft_lqr.sigma.shape[1]), \n",
    "                        device=self.device, dtype=torch.float64) * np.sqrt(dt)\n",
    "        \n",
    "        # Generate all standard normal samples for action sampling at once\n",
    "        z_samples = torch.randn(\n",
    "            (batch_size, num_steps, self.policy_network.control_dim), \n",
    "            device=self.device, dtype=torch.float64\n",
    "        )\n",
    "        \n",
    "        # Simulate trajectory using explicit scheme with vectorized operations\n",
    "        for n in range(num_steps):\n",
    "            # Current time and states\n",
    "            t_n = time_grid[n]\n",
    "            x_n = states[:, n, :]\n",
    "            \n",
    "            # Create batch of times\n",
    "            t_batch = torch.full((batch_size,), t_n, device=self.device, dtype=torch.float64)\n",
    "            \n",
    "            # Get policy distribution parameters\n",
    "            means, covariances = self.policy_network.get_action_distribution(t_batch, x_n)\n",
    "            \n",
    "            # Sample actions using pre-computed noise\n",
    "            # For each batch element, compute Cholesky and transform\n",
    "            for i in range(batch_size):\n",
    "                L = torch.linalg.cholesky(covariances[i])\n",
    "                actions[i, n, :] = means[i] + torch.mv(L, z_samples[i, n, :])\n",
    "            \n",
    "            # Compute all drifts at once using broadcasting\n",
    "            # Reshape for batch matrix multiplication\n",
    "            H_expanded = self.soft_lqr.H.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            M_expanded = self.soft_lqr.M.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            \n",
    "            # Compute Hx for all batch elements\n",
    "            x_unsqueezed = x_n.unsqueeze(2)  # batch x state_dim x 1\n",
    "            Hx = torch.bmm(H_expanded, x_unsqueezed).squeeze(2)  # batch x state_dim\n",
    "            \n",
    "            # Compute Ma for all batch elements\n",
    "            a_unsqueezed = actions[:, n, :].unsqueeze(2)  # batch x control_dim x 1\n",
    "            Ma = torch.bmm(M_expanded, a_unsqueezed).squeeze(2)  # batch x state_dim\n",
    "            \n",
    "            # Compute drift = Hx + Ma\n",
    "            drift = Hx + Ma\n",
    "            \n",
    "            # Update states using vectorized operations\n",
    "            states[:, n+1, :] = x_n + drift * dt + torch.bmm(\n",
    "                dW[:, n, :].unsqueeze(1), \n",
    "                self.soft_lqr.sigma.t().unsqueeze(0)\n",
    "            ).squeeze(1)\n",
    "        \n",
    "        return states, actions, time_grid\n",
    "    \n",
    "    def compute_policy_loss(self, states: torch.Tensor, actions: torch.Tensor, \n",
    "                           time_grid: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        GPU-optimized computation of policy loss using vectorized operations.\n",
    "        \n",
    "        Args:\n",
    "            states: States tensor (batch x time_steps+1 x state_dim)\n",
    "            actions: Actions tensor (batch x time_steps x control_dim)\n",
    "            time_grid: Time grid tensor (time_steps+1)\n",
    "            \n",
    "        Returns:\n",
    "            Policy loss\n",
    "        \"\"\"\n",
    "        batch_size = states.shape[0]\n",
    "        num_steps = actions.shape[1]\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Initialize loss\n",
    "        total_loss = torch.tensor(0.0, device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Prepare constant matrices for vectorized operations\n",
    "        C_expanded = self.soft_lqr.C.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        D_expanded = self.soft_lqr.D.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        H_expanded = self.soft_lqr.H.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        M_expanded = self.soft_lqr.M.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Process all time steps in smaller batches to avoid memory issues\n",
    "        step_batch_size = min(num_steps, 10)  # Process 10 steps at a time\n",
    "        \n",
    "        for batch_start in range(0, num_steps, step_batch_size):\n",
    "            batch_end = min(batch_start + step_batch_size, num_steps)\n",
    "            batch_steps = batch_end - batch_start\n",
    "            \n",
    "            # Get states and actions for this batch of steps\n",
    "            x_batch = states[:, batch_start:batch_end, :]  # batch x batch_steps x state_dim\n",
    "            a_batch = actions[:, batch_start:batch_end, :]  # batch x batch_steps x control_dim\n",
    "            t_batch = time_grid[batch_start:batch_end]  # batch_steps\n",
    "            \n",
    "            # Reshape for vectorized operations\n",
    "            x_flat = x_batch.reshape(-1, self.policy_network.state_dim)  # (batch*batch_steps) x state_dim\n",
    "            a_flat = a_batch.reshape(-1, self.policy_network.control_dim)  # (batch*batch_steps) x control_dim\n",
    "            t_flat = t_batch.repeat(batch_size)  # (batch*batch_steps)\n",
    "            \n",
    "            # 1. Compute state costs: x^T C x - vectorized\n",
    "            x_unsqueezed = x_flat.unsqueeze(2)  # (batch*batch_steps) x state_dim x 1\n",
    "            C_expanded_flat = C_expanded.repeat(batch_steps, 1, 1)  # (batch*batch_steps) x state_dim x state_dim\n",
    "            Cx = torch.bmm(C_expanded_flat, x_unsqueezed)  # (batch*batch_steps) x state_dim x 1\n",
    "            state_costs = torch.bmm(x_unsqueezed.transpose(1, 2), Cx).squeeze()  # (batch*batch_steps)\n",
    "            \n",
    "            # 2. Compute control costs: a^T D a - vectorized\n",
    "            a_unsqueezed = a_flat.unsqueeze(2)  # (batch*batch_steps) x control_dim x 1\n",
    "            D_expanded_flat = D_expanded.repeat(batch_steps, 1, 1)  # (batch*batch_steps) x control_dim x control_dim\n",
    "            Da = torch.bmm(D_expanded_flat, a_unsqueezed)  # (batch*batch_steps) x control_dim x 1\n",
    "            control_costs = torch.bmm(a_unsqueezed.transpose(1, 2), Da).squeeze()  # (batch*batch_steps)\n",
    "            \n",
    "            # 3. Compute value function gradient\n",
    "            value_gradients = self.compute_value_gradient(t_flat, x_flat)  # (batch*batch_steps) x state_dim\n",
    "            \n",
    "            # 4. Compute policy distribution parameters\n",
    "            means, covariances = self.policy_network.get_action_distribution(t_flat, x_flat)\n",
    "            \n",
    "            # 5. Compute log probability of the actions - vectorized\n",
    "            log_probs = torch.zeros(batch_size * batch_steps, device=self.device, dtype=torch.float64)\n",
    "            \n",
    "            for i in range(batch_size * batch_steps):\n",
    "                # Log probability of multivariate normal\n",
    "                diff = a_flat[i] - means[i]\n",
    "                inv_cov = torch.inverse(covariances[i])\n",
    "                \n",
    "                # Compute quadratic term: (a - μ)^T Σ^(-1) (a - μ)\n",
    "                quad_term = diff @ inv_cov @ diff\n",
    "                \n",
    "                # Compute log determinant term: log(det(2π Σ))\n",
    "                log_det_term = torch.log(torch.det(2 * np.pi * covariances[i]))\n",
    "                \n",
    "                # Log probability\n",
    "                log_probs[i] = -0.5 * (quad_term + log_det_term)\n",
    "            \n",
    "            # 6. Compute entropy regularization term\n",
    "            entropy_terms = self.soft_lqr.tau * (-log_probs)\n",
    "            \n",
    "            # 7. Compute value function drift term - vectorized\n",
    "            # Reshape for batch matrix operations\n",
    "            H_expanded_flat = H_expanded.repeat(batch_steps, 1, 1)  # (batch*batch_steps) x state_dim x state_dim\n",
    "            M_expanded_flat = M_expanded.repeat(batch_steps, 1, 1)  # (batch*batch_steps) x state_dim x control_dim\n",
    "            \n",
    "            # Compute Hx\n",
    "            Hx = torch.bmm(H_expanded_flat, x_unsqueezed).squeeze(2)  # (batch*batch_steps) x state_dim\n",
    "            \n",
    "            # Compute Ma\n",
    "            Ma = torch.bmm(M_expanded_flat, a_unsqueezed).squeeze(2)  # (batch*batch_steps) x state_dim\n",
    "            \n",
    "            # Compute drift = Hx + Ma\n",
    "            drift = Hx + Ma  # (batch*batch_steps) x state_dim\n",
    "            \n",
    "            # Compute drift term: ∇_x v(t,x)^T (Hx + Ma)\n",
    "            drift_terms = torch.sum(value_gradients * drift, dim=1)  # (batch*batch_steps)\n",
    "            \n",
    "            # 8. Compute step loss and reshape back to batch x batch_steps\n",
    "            step_costs = state_costs + control_costs + entropy_terms + drift_terms\n",
    "            step_costs = step_costs.reshape(batch_size, batch_steps)\n",
    "            \n",
    "            # 9. Average over batch and accumulate with time step\n",
    "            step_loss = step_costs.mean() * dt * batch_steps\n",
    "            total_loss = total_loss + step_loss\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def train_step(self, initial_states_dist, num_states: int, num_steps: int) -> float:\n",
    "        \"\"\"\n",
    "        GPU-optimized training step with mixed precision.\n",
    "        \n",
    "        Args:\n",
    "            initial_states_dist: Distribution to sample initial states from\n",
    "            num_states: Number of initial states to sample\n",
    "            num_steps: Number of time steps for simulation\n",
    "            \n",
    "        Returns:\n",
    "            Loss value for this step\n",
    "        \"\"\"\n",
    "        # Sample initial states\n",
    "        initial_states = initial_states_dist.sample((num_states,)).to(self.device)\n",
    "        \n",
    "        # Simulate trajectory using current policy (still in float64 for accuracy)\n",
    "        states, actions, time_grid = self.simulate_trajectory(initial_states, num_steps)\n",
    "        \n",
    "        # Use mixed precision for the neural network part\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Cast to float32 for network computation\n",
    "            states_fp32 = states.to(torch.float32)\n",
    "            actions_fp32 = actions.to(torch.float32)\n",
    "            time_grid_fp32 = time_grid.to(torch.float32)\n",
    "            \n",
    "            # Compute policy loss\n",
    "            loss = self.compute_policy_loss(states_fp32, actions_fp32, time_grid_fp32)\n",
    "        \n",
    "        # Backpropagation with gradient scaling\n",
    "        self.optimizer.zero_grad()\n",
    "        self.scaler.scale(loss).backward()\n",
    "        \n",
    "        # Clip gradients to avoid exploding gradients\n",
    "        self.scaler.unscale_(self.optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_network.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update weights and scaler\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def train(self, epochs: int, batch_size: int, num_steps: int, \n",
    "             initial_states_dist, eval_freq: int = 10, verbose: bool = True,\n",
    "             evaluation_states: Optional[torch.Tensor] = None) -> List[float]:\n",
    "        \"\"\"\n",
    "        GPU-optimized training with periodic evaluation.\n",
    "        \n",
    "        Args:\n",
    "            epochs: Number of training epochs\n",
    "            batch_size: Number of initial states per batch\n",
    "            num_steps: Number of time steps for simulation\n",
    "            initial_states_dist: Distribution to sample initial states from\n",
    "            eval_freq: Frequency of evaluation\n",
    "            verbose: Whether to print progress\n",
    "            evaluation_states: Optional tensor of states for periodic evaluation\n",
    "            \n",
    "        Returns:\n",
    "            List of loss values during training\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        eval_errors = []\n",
    "        \n",
    "        # Create event for timing on GPU\n",
    "        if torch.cuda.is_available():\n",
    "            start_event = torch.cuda.Event(enable_timing=True)\n",
    "            end_event = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Measure epoch time for GPU\n",
    "            if torch.cuda.is_available():\n",
    "                start_event.record()\n",
    "            \n",
    "            # Perform one training step\n",
    "            loss = self.train_step(initial_states_dist, batch_size, num_steps)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Update learning rate\n",
    "            self.scheduler.step(loss)\n",
    "            \n",
    "            # Record time and print progress\n",
    "            if torch.cuda.is_available():\n",
    "                end_event.record()\n",
    "                torch.cuda.synchronize()\n",
    "                epoch_time = start_event.elapsed_time(end_event) / 1000.0  # in seconds\n",
    "            else:\n",
    "                epoch_time = 0.0\n",
    "            \n",
    "            # Evaluate periodically\n",
    "            if (epoch+1) % eval_freq == 0:\n",
    "                if evaluation_states is not None:\n",
    "                    with torch.no_grad():\n",
    "                        error = self.evaluate_policy(evaluation_states, plot=(epoch+1) == epochs)\n",
    "                    eval_errors.append(error)\n",
    "                    \n",
    "                    if verbose:\n",
    "                        current_lr = self.optimizer.param_groups[0]['lr']\n",
    "                        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.6f}, Error: {error:.6f}, LR: {current_lr:.6f}, Time: {epoch_time:.3f}s\")\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        current_lr = self.optimizer.param_groups[0]['lr']\n",
    "                        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.6f}, LR: {current_lr:.6f}, Time: {epoch_time:.3f}s\")\n",
    "        \n",
    "        # Plot training curves\n",
    "        if len(eval_errors) > 0:\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.semilogy(losses)\n",
    "            plt.title('Policy Loss During Training')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.semilogy(range(eval_freq-1, epochs, eval_freq), eval_errors)\n",
    "            plt.title('Policy Error During Training')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Mean Error')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_actor():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Problem setup as specified in the exercise\n",
    "    H = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64) * 0.5\n",
    "    M = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64)\n",
    "    sigma = torch.eye(2, dtype=torch.float64) * 0.5\n",
    "    C = torch.tensor([[1.0, 0.1], [0.1, 1.0]], dtype=torch.float64) * 1.0\n",
    "    D = torch.eye(2, dtype=torch.float64)  # D just identity, as specified\n",
    "    R = torch.tensor([[1.0, 0.3], [0.3, 1.0]], dtype=torch.float64) * 10.0\n",
    "    \n",
    "    # Set terminal time and time grid\n",
    "    T = 0.5\n",
    "    grid_size = 1000\n",
    "    time_grid = torch.linspace(0, T, grid_size, dtype=torch.float64)\n",
    "    \n",
    "    # Set entropy regularization parameters\n",
    "    tau = 0.5  # τ = 1/2 as specified\n",
    "    gamma = 1.0  # γ = 1 as specified\n",
    "    \n",
    "    # Create soft LQR instance\n",
    "    soft_lqr = SoftLQR(H, M, sigma, C, D, R, T, time_grid, tau, gamma)\n",
    "    \n",
    "    # Solve Ricatti ODE\n",
    "    print(\"Solving Ricatti ODE...\")\n",
    "    soft_lqr.solve_ricatti()\n",
    "    \n",
    "    # Create policy network\n",
    "    hidden_size = 256  # As specified\n",
    "    policy_network = PolicyNetwork(\n",
    "        hidden_size=hidden_size, \n",
    "        state_dim=2, \n",
    "        control_dim=2, \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create actor algorithm based on device\n",
    "    learning_rate = 1e-4  # As specified\n",
    "    \n",
    "    if device.type == \"cuda\":\n",
    "        print(\"Using GPU-optimized actor algorithm...\")\n",
    "        actor = GPUActorAlgorithm(soft_lqr, policy_network, learning_rate, device)\n",
    "    else:\n",
    "        print(\"Using CPU-based actor algorithm...\")\n",
    "        actor = ActorAlgorithm(soft_lqr, policy_network, learning_rate, device)\n",
    "    \n",
    "    # Define initial state distribution\n",
    "    # Uniform distribution in [-3, 3] x [-3, 3]\n",
    "    class UniformInitialState:\n",
    "        def __init__(self, low=-3.0, high=3.0, dim=2, device=device):\n",
    "            self.low = low\n",
    "            self.high = high\n",
    "            self.dim = dim\n",
    "            self.device = device\n",
    "        \n",
    "        def sample(self, shape):\n",
    "            return torch.empty((*shape, self.dim), dtype=torch.float64, device=self.device).uniform_(self.low, self.high)\n",
    "    \n",
    "    initial_states_dist = UniformInitialState()\n",
    "    \n",
    "    # Create evaluation grid for periodic assessment\n",
    "    x1 = torch.linspace(-3, 3, 11, device=device, dtype=torch.float64)\n",
    "    x2 = torch.linspace(-3, 3, 11, device=device, dtype=torch.float64)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    eval_states = torch.stack([X1.flatten(), X2.flatten()], dim=1)\n",
    "    \n",
    "    # Training parameters\n",
    "    if device.type == \"cuda\":\n",
    "        epochs = 500\n",
    "        batch_size = 128  # Larger for GPU\n",
    "        num_steps = 100  # N = 100 as specified\n",
    "        eval_freq = 25\n",
    "    else:\n",
    "        epochs = 300  # Fewer epochs for CPU\n",
    "        batch_size = 64  # Smaller for CPU\n",
    "        num_steps = 100  # N = 100 as specified\n",
    "        eval_freq = 25\n",
    "    \n",
    "    # Train the actor algorithm\n",
    "    print(\"Training actor algorithm...\")\n",
    "    losses = actor.train(\n",
    "        epochs, batch_size, num_steps, initial_states_dist, \n",
    "        eval_freq, verbose=True, evaluation_states=eval_states\n",
    "    )\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.semilogy(losses)\n",
    "    plt.title('Actor Loss During Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"Final evaluation of learned policy...\")\n",
    "    actor.evaluate_policy(eval_states, plot=True)\n",
    "    \n",
    "    # Visualize policy quiver plot for state space\n",
    "    print(\"Visualizing policies...\")\n",
    "    \n",
    "    # Sample grid of states\n",
    "    x1_dense = torch.linspace(-3, 3, 21, device=device, dtype=torch.float64)\n",
    "    x2_dense = torch.linspace(-3, 3, 21, device=device, dtype=torch.float64)\n",
    "    X1_dense, X2_dense = torch.meshgrid(x1_dense, x2_dense, indexing='ij')\n",
    "    grid_states = torch.stack([X1_dense.flatten(), X2_dense.flatten()], dim=1)\n",
    "    \n",
    "    # Set time to zero\n",
    "    t0 = torch.zeros(grid_states.shape[0], device=device, dtype=torch.float64)\n",
    "    \n",
    "    # Get means from both policies\n",
    "    with torch.no_grad():\n",
    "        learned_means, _ = policy_network.get_action_distribution(t0, grid_states)\n",
    "        optimal_means, _ = soft_lqr.optimal_control_distribution(t0, grid_states)\n",
    "    \n",
    "    # Convert to numpy for plotting\n",
    "    X1_np = X1_dense.cpu().numpy()\n",
    "    X2_np = X2_dense.cpu().numpy()\n",
    "    \n",
    "    learned_u = learned_means.cpu().numpy()[:, 0].reshape(X1_np.shape)\n",
    "    learned_v = learned_means.cpu().numpy()[:, 1].reshape(X1_np.shape)\n",
    "    \n",
    "    optimal_u = optimal_means.cpu().numpy()[:, 0].reshape(X1_np.shape)\n",
    "    optimal_v = optimal_means.cpu().numpy()[:, 1].reshape(X1_np.shape)\n",
    "    \n",
    "    # Create quiver plots\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot optimal policy\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.quiver(X1_np, X2_np, optimal_u, optimal_v)\n",
    "    plt.title('Optimal Policy Control Means')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot learned policy\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.quiver(X1_np, X2_np, learned_u, learned_v)\n",
    "    plt.title('Learned Policy Control Means')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Test policy on trajectory\n",
    "    print(\"Simulating trajectories with learned policy...\")\n",
    "    \n",
    "    # Sample test initial states\n",
    "    test_initial_states = torch.tensor([\n",
    "        [2.0, 2.0],\n",
    "        [2.0, -2.0],\n",
    "        [-2.0, -2.0],\n",
    "        [-2.0, 2.0]\n",
    "    ], device=device, dtype=torch.float64)\n",
    "    \n",
    "    # Simulate trajectories with both policies\n",
    "    num_steps_sim = 200\n",
    "    dt = T / num_steps_sim\n",
    "    \n",
    "    # Generate same noise for both simulations\n",
    "    dW = torch.randn((4, num_steps_sim, sigma.shape[1]), \n",
    "                    device=device, dtype=torch.float64) * np.sqrt(dt)\n",
    "    \n",
    "    # Time grid for simulation\n",
    "    t_sim = torch.linspace(0, T, num_steps_sim + 1, device=device, dtype=torch.float64)\n",
    "    \n",
    "    # Initialize trajectories\n",
    "    opt_traj = torch.zeros((4, num_steps_sim + 1, 2), device=device, dtype=torch.float64)\n",
    "    learned_traj = torch.zeros((4, num_steps_sim + 1, 2), device=device, dtype=torch.float64)\n",
    "    \n",
    "    # Set initial states\n",
    "    opt_traj[:, 0, :] = test_initial_states\n",
    "    learned_traj[:, 0, :] = test_initial_states\n",
    "    \n",
    "    # Simulate trajectories\n",
    "    for n in range(num_steps_sim):\n",
    "        # Current time and states\n",
    "        t_n = t_sim[n]\n",
    "        t_batch = torch.full((4,), t_n, device=device, dtype=torch.float64)\n",
    "        \n",
    "        # Get control from optimal policy\n",
    "        opt_means, opt_cov = soft_lqr.optimal_control_distribution(t_batch, opt_traj[:, n, :])\n",
    "        opt_L = torch.linalg.cholesky(opt_cov)\n",
    "        \n",
    "        # Get control from learned policy\n",
    "        learned_means, learned_covs = policy_network.get_action_distribution(t_batch, learned_traj[:, n, :])\n",
    "        \n",
    "        # Use same noise for both policies\n",
    "        z = torch.randn((4, 2), device=device, dtype=torch.float64)\n",
    "        \n",
    "        # Sample controls\n",
    "        opt_actions = torch.zeros((4, 2), device=device, dtype=torch.float64)\n",
    "        learned_actions = torch.zeros((4, 2), device=device, dtype=torch.float64)\n",
    "        \n",
    "        for i in range(4):\n",
    "            opt_actions[i] = opt_means[i] + torch.mv(opt_L, z[i])\n",
    "            \n",
    "            # For learned policy, need to compute Cholesky for each covariance\n",
    "            learned_L = torch.linalg.cholesky(learned_covs[i])\n",
    "            learned_actions[i] = learned_means[i] + torch.mv(learned_L, z[i])\n",
    "        \n",
    "        # Update states\n",
    "        for i in range(4):\n",
    "            # Optimal trajectory update\n",
    "            opt_drift = H @ opt_traj[i, n, :] + M @ opt_actions[i]\n",
    "            opt_traj[i, n+1, :] = opt_traj[i, n, :] + opt_drift * dt + sigma @ dW[i, n, :]\n",
    "            \n",
    "            # Learned trajectory update\n",
    "            learned_drift = H @ learned_traj[i, n, :] + M @ learned_actions[i]\n",
    "            learned_traj[i, n+1, :] = learned_traj[i, n, :] + learned_drift * dt + sigma @ dW[i, n, :]\n",
    "    \n",
    "    # Convert to numpy for plotting\n",
    "    opt_traj_np = opt_traj.cpu().numpy()\n",
    "    learned_traj_np = learned_traj.cpu().numpy()\n",
    "    \n",
    "    # Plot trajectories\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    for i in range(4):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.plot(opt_traj_np[i, :, 0], opt_traj_np[i, :, 1], 'b-', label='Optimal Policy')\n",
    "        plt.plot(learned_traj_np[i, :, 0], learned_traj_np[i, :, 1], 'r-', label='Learned Policy')\n",
    "        plt.scatter(test_initial_states[i, 0].cpu().numpy(), test_initial_states[i, 1].cpu().numpy(), \n",
    "                   c='g', s=100, marker='o', label='Initial State')\n",
    "        plt.title(f'Trajectory from Initial State {test_initial_states[i].cpu().numpy()}')\n",
    "        plt.xlabel('x1')\n",
    "        plt.ylabel('x2')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_actor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "\n",
    "class ActorCriticAlgorithm:\n",
    "    def __init__(self, soft_lqr: SoftLQR, policy_network: PolicyNetwork, value_network: ValueNetwork,\n",
    "                 actor_lr: float = 1e-4, critic_lr: float = 1e-3, device: str = \"cpu\"):\n",
    "        \"\"\"\n",
    "        Actor-Critic algorithm for the soft LQR problem.\n",
    "        \n",
    "        Args:\n",
    "            soft_lqr: Soft LQR instance (used for system dynamics and cost parameters)\n",
    "            policy_network: Neural network to approximate the policy (actor)\n",
    "            value_network: Neural network to approximate the value function (critic)\n",
    "            actor_lr: Learning rate for the actor optimizer\n",
    "            critic_lr: Learning rate for the critic optimizer\n",
    "            device: Device to run the algorithm on\n",
    "        \"\"\"\n",
    "        self.soft_lqr = soft_lqr\n",
    "        self.policy_network = policy_network\n",
    "        self.value_network = value_network\n",
    "        self.device = device\n",
    "        \n",
    "        # Move soft_lqr matrices to the device\n",
    "        self.soft_lqr.H = self.soft_lqr.H.to(device)\n",
    "        self.soft_lqr.M = self.soft_lqr.M.to(device)\n",
    "        self.soft_lqr.sigma = self.soft_lqr.sigma.to(device)\n",
    "        self.soft_lqr.C = self.soft_lqr.C.to(device)\n",
    "        self.soft_lqr.D = self.soft_lqr.D.to(device)\n",
    "        self.soft_lqr.R = self.soft_lqr.R.to(device)\n",
    "        self.soft_lqr.D_inv = self.soft_lqr.D_inv.to(device)\n",
    "        if hasattr(self.soft_lqr, 'sigma_inv'):\n",
    "            self.soft_lqr.sigma_inv = self.soft_lqr.sigma_inv.to(device)\n",
    "        if hasattr(self.soft_lqr, 'sigma_term'):\n",
    "            self.soft_lqr.sigma_term = self.soft_lqr.sigma_term.to(device)\n",
    "        \n",
    "        # Setup optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.policy_network.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.value_network.parameters(), lr=critic_lr)\n",
    "    \n",
    "    def compute_value_gradient(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the gradient of the value function with respect to the state.\n",
    "        Uses the critic network to differentiate through automatic differentiation.\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            x: State tensor (batch x state_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Gradient of value function (batch x state_dim)\n",
    "        \"\"\"\n",
    "        # Ensure x requires gradient\n",
    "        x_with_grad = x.detach().clone().requires_grad_(True)\n",
    "        \n",
    "        # Forward pass through the value network\n",
    "        values = self.value_network.value_function(t, x_with_grad)\n",
    "        \n",
    "        # Initialize gradients\n",
    "        gradients = torch.zeros_like(x)\n",
    "        \n",
    "        # Compute gradient for each element in the batch\n",
    "        for i in range(values.shape[0]):\n",
    "            # Zero existing gradients\n",
    "            if x_with_grad.grad is not None:\n",
    "                x_with_grad.grad.zero_()\n",
    "            \n",
    "            # Backpropagate for this element\n",
    "            values[i].backward(retain_graph=(i < values.shape[0] - 1))\n",
    "            \n",
    "            # Store gradient\n",
    "            if x_with_grad.grad is not None:\n",
    "                gradients[i] = x_with_grad.grad[i].clone()\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def simulate_trajectory(self, initial_states: torch.Tensor, \n",
    "                           num_steps: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Simulate trajectory using the current policy.\n",
    "        \n",
    "        Args:\n",
    "            initial_states: Initial states tensor (batch x state_dim)\n",
    "            num_steps: Number of time steps for simulation\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (states, actions, time_points)\n",
    "        \"\"\"\n",
    "        batch_size = initial_states.shape[0]\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Generate time grid\n",
    "        time_grid = torch.linspace(0, self.soft_lqr.T, num_steps + 1, \n",
    "                                  device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Initialize states and actions\n",
    "        states = torch.zeros((batch_size, num_steps + 1, self.policy_network.state_dim), \n",
    "                           device=self.device, dtype=torch.float64)\n",
    "        states[:, 0, :] = initial_states\n",
    "        \n",
    "        actions = torch.zeros((batch_size, num_steps, self.policy_network.control_dim), \n",
    "                            device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Generate Brownian increments\n",
    "        dW = torch.randn((batch_size, num_steps, self.soft_lqr.sigma.shape[1]), \n",
    "                        device=self.device, dtype=torch.float64) * np.sqrt(dt)\n",
    "        \n",
    "        # Simulate trajectory using explicit scheme\n",
    "        for n in range(num_steps):\n",
    "            # Current time and states\n",
    "            t_n = time_grid[n]\n",
    "            x_n = states[:, n, :]\n",
    "            \n",
    "            # Sample actions from policy\n",
    "            t_batch = t_n.repeat(batch_size)\n",
    "            actions[:, n, :] = self.policy_network.sample_action(t_batch, x_n)\n",
    "            \n",
    "            # Update states\n",
    "            for i in range(batch_size):\n",
    "                # Compute drift term: HX + Ma\n",
    "                drift = self.soft_lqr.H @ states[i, n, :] + self.soft_lqr.M @ actions[i, n, :]\n",
    "                \n",
    "                # Update state using explicit scheme\n",
    "                states[i, n+1, :] = states[i, n, :] + drift * dt + self.soft_lqr.sigma @ dW[i, n, :]\n",
    "        \n",
    "        return states, actions, time_grid\n",
    "    \n",
    "    def compute_critic_targets(self, states: torch.Tensor, actions: torch.Tensor, \n",
    "                              time_grid: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute target values for critic training using Monte Carlo estimation.\n",
    "        \n",
    "        Args:\n",
    "            states: States tensor (batch x time_steps+1 x state_dim)\n",
    "            actions: Actions tensor (batch x time_steps x control_dim)\n",
    "            time_grid: Time grid tensor (time_steps+1)\n",
    "            \n",
    "        Returns:\n",
    "            Target values for each state and time step (batch x time_steps+1)\n",
    "        \"\"\"\n",
    "        batch_size = states.shape[0]\n",
    "        num_steps = time_grid.shape[0] - 1\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Initialize target values\n",
    "        targets = torch.zeros((batch_size, num_steps + 1), device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Compute costs at each time step\n",
    "        for n in range(num_steps):\n",
    "            # Current states and actions\n",
    "            x_n = states[:, n, :]\n",
    "            a_n = actions[:, n, :]\n",
    "            \n",
    "            # Get distribution parameters for entropy calculation\n",
    "            t_n = time_grid[n]\n",
    "            t_batch = t_n.repeat(batch_size)\n",
    "            means, covariances = self.policy_network.get_action_distribution(t_batch, x_n)\n",
    "            \n",
    "            # Compute costs for each batch element\n",
    "            for i in range(batch_size):\n",
    "                # State cost: x^T C x\n",
    "                state_cost = x_n[i] @ self.soft_lqr.C @ x_n[i]\n",
    "                \n",
    "                # Control cost: a^T D a\n",
    "                control_cost = a_n[i] @ self.soft_lqr.D @ a_n[i]\n",
    "                \n",
    "                # Entropy term\n",
    "                diff = a_n[i] - means[i]\n",
    "                inv_cov = torch.inverse(covariances[i])\n",
    "                quad_term = diff @ inv_cov @ diff\n",
    "                log_det_term = torch.log(torch.det(2 * np.pi * covariances[i]))\n",
    "                log_prob = -0.5 * (quad_term + log_det_term)\n",
    "                entropy_term = self.soft_lqr.tau * (-log_prob)\n",
    "                \n",
    "                # Add costs for this time step\n",
    "                targets[i, n+1:] += (state_cost + control_cost + entropy_term) * dt\n",
    "        \n",
    "        # Add terminal costs\n",
    "        x_T = states[:, -1, :]\n",
    "        for i in range(batch_size):\n",
    "            terminal_cost = x_T[i] @ self.soft_lqr.R @ x_T[i]\n",
    "            targets[i, -1] += terminal_cost\n",
    "        \n",
    "        return targets\n",
    "    \n",
    "    def compute_actor_loss(self, states: torch.Tensor, actions: torch.Tensor, \n",
    "                          time_grid: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the loss for policy optimization using the critic network.\n",
    "        \n",
    "        Args:\n",
    "            states: States tensor (batch x time_steps+1 x state_dim)\n",
    "            actions: Actions tensor (batch x time_steps x control_dim)\n",
    "            time_grid: Time grid tensor (time_steps+1)\n",
    "            \n",
    "        Returns:\n",
    "            Policy loss\n",
    "        \"\"\"\n",
    "        batch_size = states.shape[0]\n",
    "        num_steps = actions.shape[1]\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Initialize loss\n",
    "        total_loss = torch.tensor(0.0, device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Compute loss at each time step\n",
    "        for n in range(num_steps):\n",
    "            # Current time, states, and actions\n",
    "            t_n = time_grid[n]\n",
    "            x_n = states[:, n, :]\n",
    "            a_n = actions[:, n, :]\n",
    "            \n",
    "            # Create batch of times\n",
    "            t_batch = t_n.repeat(batch_size)\n",
    "            \n",
    "            # 1. Compute state cost: x^T C x\n",
    "            state_costs = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "            for i in range(batch_size):\n",
    "                state_costs[i] = x_n[i] @ self.soft_lqr.C @ x_n[i]\n",
    "            \n",
    "            # 2. Compute control cost: a^T D a\n",
    "            control_costs = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "            for i in range(batch_size):\n",
    "                control_costs[i] = a_n[i] @ self.soft_lqr.D @ a_n[i]\n",
    "            \n",
    "            # 3. Compute value function gradient using critic network\n",
    "            value_gradients = self.compute_value_gradient(t_batch, x_n)\n",
    "            \n",
    "            # 4. Compute policy distribution parameters\n",
    "            means, covariances = self.policy_network.get_action_distribution(t_batch, x_n)\n",
    "            \n",
    "            # 5. Compute log probability of the actions\n",
    "            log_probs = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "            for i in range(batch_size):\n",
    "                # Log probability of multivariate normal\n",
    "                diff = a_n[i] - means[i]\n",
    "                inv_cov = torch.inverse(covariances[i])\n",
    "                \n",
    "                # Compute quadratic term: (a - μ)^T Σ^(-1) (a - μ)\n",
    "                quad_term = diff @ inv_cov @ diff\n",
    "                \n",
    "                # Compute log determinant term: log(det(2π Σ))\n",
    "                log_det_term = torch.log(torch.det(2 * np.pi * covariances[i]))\n",
    "                \n",
    "                # Log probability\n",
    "                log_probs[i] = -0.5 * (quad_term + log_det_term)\n",
    "            \n",
    "            # 6. Compute entropy regularization term\n",
    "            entropy_terms = self.soft_lqr.tau * (-log_probs)\n",
    "            \n",
    "            # 7. Compute value function drift term for each state-action pair\n",
    "            drift_terms = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "            for i in range(batch_size):\n",
    "                # Compute drift: Hx + Ma\n",
    "                drift = self.soft_lqr.H @ x_n[i] + self.soft_lqr.M @ a_n[i]\n",
    "                \n",
    "                # Compute drift term: ∇_x v(t,x)^T (Hx + Ma)\n",
    "                drift_terms[i] = value_gradients[i] @ drift\n",
    "            \n",
    "            # 8. Compute total loss for this time step\n",
    "            step_loss = (state_costs + control_costs + entropy_terms + drift_terms).mean()\n",
    "            \n",
    "            # 9. Accumulate loss with time step\n",
    "            total_loss = total_loss + step_loss * dt\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def compute_critic_loss(self, states: torch.Tensor, targets: torch.Tensor, \n",
    "                           time_grid: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the loss for value function optimization.\n",
    "        \n",
    "        Args:\n",
    "            states: States tensor (batch x time_steps+1 x state_dim)\n",
    "            targets: Target values (batch x time_steps+1)\n",
    "            time_grid: Time grid tensor (time_steps+1)\n",
    "            \n",
    "        Returns:\n",
    "            Critic loss\n",
    "        \"\"\"\n",
    "        batch_size = states.shape[0]\n",
    "        num_steps = states.shape[1] - 1\n",
    "        \n",
    "        # Initialize loss\n",
    "        total_loss = torch.tensor(0.0, device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Compute loss at each time step\n",
    "        for n in range(num_steps + 1):\n",
    "            # Current time and states\n",
    "            t_n = time_grid[n]\n",
    "            x_n = states[:, n, :]\n",
    "            \n",
    "            # Create batch of times\n",
    "            t_batch = t_n.repeat(batch_size)\n",
    "            \n",
    "            # Predict values using critic network\n",
    "            predicted = self.value_network.value_function(t_batch, x_n)\n",
    "            \n",
    "            # Target values at this time step\n",
    "            target = targets[:, n]\n",
    "            \n",
    "            # Compute MSE loss\n",
    "            step_loss = ((predicted - target) ** 2).mean()\n",
    "            \n",
    "            # Accumulate loss\n",
    "            total_loss = total_loss + step_loss\n",
    "        \n",
    "        # Average loss over all time steps\n",
    "        total_loss = total_loss / (num_steps + 1)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def train_step(self, initial_states_dist, num_states: int, num_steps: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Perform one training step of the actor-critic algorithm.\n",
    "        \n",
    "        Args:\n",
    "            initial_states_dist: Distribution to sample initial states from\n",
    "            num_states: Number of initial states to sample\n",
    "            num_steps: Number of time steps for simulation\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of loss values for this step\n",
    "        \"\"\"\n",
    "        # Sample initial states\n",
    "        initial_states = initial_states_dist.sample((num_states,)).to(self.device)\n",
    "        \n",
    "        # Simulate trajectory using current policy\n",
    "        states, actions, time_grid = self.simulate_trajectory(initial_states, num_steps)\n",
    "        \n",
    "        # Compute critic targets\n",
    "        targets = self.compute_critic_targets(states, actions, time_grid)\n",
    "        \n",
    "        # Update critic\n",
    "        critic_loss = self.compute_critic_loss(states, targets, time_grid)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Simulate new trajectory with updated critic\n",
    "        states, actions, time_grid = self.simulate_trajectory(initial_states, num_steps)\n",
    "        \n",
    "        # Update actor\n",
    "        actor_loss = self.compute_actor_loss(states, actions, time_grid)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            'critic_loss': critic_loss.item(),\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'total_loss': critic_loss.item() + actor_loss.item()\n",
    "        }\n",
    "    \n",
    "    def train(self, epochs: int, batch_size: int, num_steps: int, \n",
    "             initial_states_dist, eval_freq: int = 10, verbose: bool = True,\n",
    "             evaluation_states: Optional[torch.Tensor] = None) -> Dict[str, List[float]]:\n",
    "        \"\"\"\n",
    "        Train the actor-critic algorithm for multiple epochs.\n",
    "        \n",
    "        Args:\n",
    "            epochs: Number of training epochs\n",
    "            batch_size: Number of initial states per batch\n",
    "            num_steps: Number of time steps for simulation\n",
    "            initial_states_dist: Distribution to sample initial states from\n",
    "            eval_freq: Frequency of evaluation\n",
    "            verbose: Whether to print progress\n",
    "            evaluation_states: Optional tensor of states for periodic evaluation\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of lists of loss values during training\n",
    "        \"\"\"\n",
    "        losses = {\n",
    "            'critic_loss': [],\n",
    "            'actor_loss': [],\n",
    "            'total_loss': []\n",
    "        }\n",
    "        eval_errors = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Perform one training step\n",
    "            step_losses = self.train_step(initial_states_dist, batch_size, num_steps)\n",
    "            \n",
    "            # Record losses\n",
    "            for key in losses.keys():\n",
    "                losses[key].append(step_losses[key])\n",
    "            \n",
    "            # Evaluate periodically\n",
    "            if (epoch+1) % eval_freq == 0 and evaluation_states is not None:\n",
    "                with torch.no_grad():\n",
    "                    if hasattr(self.soft_lqr, 'optimal_control_distribution'):\n",
    "                        # Compute error between learned and optimal policy\n",
    "                        t0 = torch.zeros(evaluation_states.shape[0], device=self.device, dtype=torch.float64)\n",
    "                        learned_means, _ = self.policy_network.get_action_distribution(t0, evaluation_states)\n",
    "                        optimal_means, _ = self.soft_lqr.optimal_control_distribution(t0, evaluation_states)\n",
    "                        \n",
    "                        # Mean error in control means\n",
    "                        mean_error = torch.norm(learned_means - optimal_means, dim=1).mean().item()\n",
    "                        eval_errors.append(mean_error)\n",
    "                        \n",
    "                        if verbose:\n",
    "                            print(f\"Epoch {epoch+1}/{epochs}, Critic Loss: {step_losses['critic_loss']:.6f}, \"\n",
    "                                 f\"Actor Loss: {step_losses['actor_loss']:.6f}, Mean Error: {mean_error:.6f}\")\n",
    "                    else:\n",
    "                        if verbose:\n",
    "                            print(f\"Epoch {epoch+1}/{epochs}, Critic Loss: {step_losses['critic_loss']:.6f}, \"\n",
    "                                 f\"Actor Loss: {step_losses['actor_loss']:.6f}\")\n",
    "            elif verbose and (epoch+1) % eval_freq == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Critic Loss: {step_losses['critic_loss']:.6f}, \"\n",
    "                     f\"Actor Loss: {step_losses['actor_loss']:.6f}\")\n",
    "        \n",
    "        # Plot training curves\n",
    "        if verbose:\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.semilogy(losses['critic_loss'], label='Critic Loss')\n",
    "            plt.semilogy(losses['actor_loss'], label='Actor Loss')\n",
    "            plt.title('Loss During Training')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss (log scale)')\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            \n",
    "            if len(eval_errors) > 0:\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.semilogy(range(eval_freq-1, epochs, eval_freq), eval_errors)\n",
    "                plt.title('Policy Mean Error During Training')\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Mean Error (log scale)')\n",
    "                plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def evaluate(self, test_states: torch.Tensor, plot: bool = True) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate the trained actor-critic algorithm.\n",
    "        \n",
    "        Args:\n",
    "            test_states: Test states tensor (batch x state_dim)\n",
    "            plot: Whether to plot the results\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        # Set evaluation mode\n",
    "        self.policy_network.eval()\n",
    "        self.value_network.eval()\n",
    "        \n",
    "        metrics = {}\n",
    "        batch_size = test_states.shape[0]\n",
    "        t0 = torch.zeros(batch_size, device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get value function predictions\n",
    "            predicted_values = self.value_network.value_function(t0, test_states)\n",
    "            \n",
    "            # Get policy distribution parameters\n",
    "            learned_means, learned_covs = self.policy_network.get_action_distribution(t0, test_states)\n",
    "            \n",
    "            # If we have the optimal policy and value function, compare\n",
    "            if hasattr(self.soft_lqr, 'value_function') and hasattr(self.soft_lqr, 'optimal_control_distribution'):\n",
    "                # Get optimal value function\n",
    "                true_values = self.soft_lqr.value_function(t0, test_states)\n",
    "                \n",
    "                # Get optimal policy parameters\n",
    "                optimal_means, optimal_cov = self.soft_lqr.optimal_control_distribution(t0, test_states)\n",
    "                \n",
    "                # Compute value function error\n",
    "                value_errors = torch.abs(predicted_values - true_values)\n",
    "                metrics['mean_value_error'] = value_errors.mean().item()\n",
    "                metrics['max_value_error'] = value_errors.max().item()\n",
    "                \n",
    "                # Compute policy mean error\n",
    "                mean_errors = torch.norm(learned_means - optimal_means, dim=1)\n",
    "                metrics['mean_policy_error'] = mean_errors.mean().item()\n",
    "                metrics['max_policy_error'] = mean_errors.max().item()\n",
    "                \n",
    "                print(f\"Evaluation results:\")\n",
    "                print(f\"  Value function - Mean error: {metrics['mean_value_error']:.6f}, Max error: {metrics['max_value_error']:.6f}\")\n",
    "                print(f\"  Policy means - Mean error: {metrics['mean_policy_error']:.6f}, Max error: {metrics['max_policy_error']:.6f}\")\n",
    "                \n",
    "                if plot:\n",
    "                    # Convert to numpy for plotting\n",
    "                    test_states_np = test_states.cpu().numpy()\n",
    "                    true_values_np = true_values.cpu().numpy()\n",
    "                    predicted_values_np = predicted_values.cpu().numpy()\n",
    "                    value_errors_np = value_errors.cpu().numpy()\n",
    "                    \n",
    "                    optimal_means_np = optimal_means.cpu().numpy()\n",
    "                    learned_means_np = learned_means.cpu().numpy()\n",
    "                    mean_errors_np = mean_errors.cpu().numpy()\n",
    "                    \n",
    "                    # Create plots\n",
    "                    plt.figure(figsize=(15, 10))\n",
    "                    \n",
    "                    # Value function plots\n",
    "                    plt.subplot(2, 2, 1)\n",
    "                    plt.scatter(true_values_np, predicted_values_np, alpha=0.7)\n",
    "                    min_val = min(true_values_np.min(), predicted_values_np.min())\n",
    "                    max_val = max(true_values_np.max(), predicted_values_np.max())\n",
    "                    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "                    plt.title('True vs Predicted Values')\n",
    "                    plt.xlabel('True Value')\n",
    "                    plt.ylabel('Predicted Value')\n",
    "                    plt.grid(True)\n",
    "                    \n",
    "                    plt.subplot(2, 2, 2)\n",
    "                    plt.scatter(true_values_np, value_errors_np, alpha=0.7)\n",
    "                    plt.title('Value Function Errors')\n",
    "                    plt.xlabel('True Value')\n",
    "                    plt.ylabel('Absolute Error')\n",
    "                    plt.grid(True)\n",
    "                    \n",
    "                    # Policy plots\n",
    "                    plt.subplot(2, 2, 3)\n",
    "                    plt.scatter(optimal_means_np[:, 0], learned_means_np[:, 0], alpha=0.7)\n",
    "                    min_val = min(optimal_means_np[:, 0].min(), learned_means_np[:, 0].min())\n",
    "                    max_val = max(optimal_means_np[:, 0].max(), learned_means_np[:, 0].max())\n",
    "                    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "                    plt.title('Optimal vs Learned Policy Means (First Dimension)')\n",
    "                    plt.xlabel('Optimal Mean')\n",
    "                    plt.ylabel('Learned Mean')\n",
    "                    plt.grid(True)\n",
    "                    \n",
    "                    plt.subplot(2, 2, 4)\n",
    "                    plt.scatter(np.linalg.norm(test_states_np, axis=1), mean_errors_np, alpha=0.7)\n",
    "                    plt.title('Policy Mean Errors vs State Norm')\n",
    "                    plt.xlabel('State Norm')\n",
    "                    plt.ylabel('Policy Mean Error')\n",
    "                    plt.grid(True)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "            else:\n",
    "                print(\"No optimal solution available for comparison. Skipping error metrics.\")\n",
    "        \n",
    "        # Switch back to training mode\n",
    "        self.policy_network.train()\n",
    "        self.value_network.train()\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUActorCriticAlgorithm(ActorCriticAlgorithm):\n",
    "    def __init__(self, soft_lqr: SoftLQR, policy_network: PolicyNetwork, value_network: ValueNetwork,\n",
    "                 actor_lr: float = 1e-4, critic_lr: float = 1e-3, device: str = \"cuda\"):\n",
    "        \"\"\"\n",
    "        GPU-optimized actor-critic algorithm for the soft LQR problem.\n",
    "        \n",
    "        Args:\n",
    "            soft_lqr: Soft LQR instance (used for system dynamics and cost parameters)\n",
    "            policy_network: Neural network to approximate the policy (actor)\n",
    "            value_network: Neural network to approximate the value function (critic)\n",
    "            actor_lr: Learning rate for the actor optimizer\n",
    "            critic_lr: Learning rate for the critic optimizer\n",
    "            device: Device to run the algorithm on (default: \"cuda\")\n",
    "        \"\"\"\n",
    "        super().__init__(soft_lqr, policy_network, value_network, actor_lr, critic_lr, device)\n",
    "        \n",
    "        # Create gradient scalers for mixed precision training\n",
    "        self.actor_scaler = torch.cuda.amp.GradScaler()\n",
    "        self.critic_scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "        # Enhanced optimizers with weight decay and momentum\n",
    "        self.actor_optimizer = optim.AdamW(\n",
    "            self.policy_network.parameters(), \n",
    "            lr=actor_lr,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "        \n",
    "        self.critic_optimizer = optim.AdamW(\n",
    "            self.value_network.parameters(), \n",
    "            lr=critic_lr,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "        \n",
    "        # Create learning rate schedulers\n",
    "        self.actor_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.actor_optimizer, 'min', factor=0.5, patience=20, verbose=True\n",
    "        )\n",
    "        \n",
    "        self.critic_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.critic_optimizer, 'min', factor=0.5, patience=20, verbose=True\n",
    "        )\n",
    "    \n",
    "    def compute_value_gradient(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        GPU-optimized computation of value function gradient.\n",
    "        Uses vectorized autograd for efficiency.\n",
    "        \n",
    "        Args:\n",
    "            t: Time tensor (batch)\n",
    "            x: State tensor (batch x state_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Gradient of value function (batch x state_dim)\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Use autodiff for gradient calculation - more GPU friendly\n",
    "        # Create clone with gradient tracking for each batch element\n",
    "        gradients = torch.zeros_like(x)\n",
    "        \n",
    "        # Process in smaller batches to avoid memory issues\n",
    "        max_batch = 64  # Maximum batch size for gradient computation\n",
    "        \n",
    "        for i in range(0, batch_size, max_batch):\n",
    "            end_idx = min(i + max_batch, batch_size)\n",
    "            sub_batch_size = end_idx - i\n",
    "            \n",
    "            # Get sub-batch\n",
    "            t_sub = t[i:end_idx]\n",
    "            x_sub = x[i:end_idx].detach().clone().requires_grad_(True)\n",
    "            \n",
    "            # Compute value function\n",
    "            values = self.value_network.value_function(t_sub, x_sub)\n",
    "            \n",
    "            # Compute gradients for all elements in sub-batch at once\n",
    "            grad_outputs = torch.ones_like(values)\n",
    "            all_grads = torch.autograd.grad(\n",
    "                outputs=values,\n",
    "                inputs=x_sub,\n",
    "                grad_outputs=grad_outputs,\n",
    "                create_graph=False,\n",
    "                retain_graph=False,\n",
    "                only_inputs=True\n",
    "            )[0]\n",
    "            \n",
    "            # Store gradients\n",
    "            gradients[i:end_idx] = all_grads\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def simulate_trajectory(self, initial_states: torch.Tensor, \n",
    "                           num_steps: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        GPU-optimized trajectory simulation using vectorized operations.\n",
    "        \n",
    "        Args:\n",
    "            initial_states: Initial states tensor (batch x state_dim)\n",
    "            num_steps: Number of time steps for simulation\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (states, actions, time_points)\n",
    "        \"\"\"\n",
    "        batch_size = initial_states.shape[0]\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Generate time grid\n",
    "        time_grid = torch.linspace(0, self.soft_lqr.T, num_steps + 1, \n",
    "                                  device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Initialize states and actions\n",
    "        states = torch.zeros((batch_size, num_steps + 1, self.policy_network.state_dim), \n",
    "                           device=self.device, dtype=torch.float64)\n",
    "        states[:, 0, :] = initial_states\n",
    "        \n",
    "        actions = torch.zeros((batch_size, num_steps, self.policy_network.control_dim), \n",
    "                            device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Generate all Brownian increments at once\n",
    "        dW = torch.randn((batch_size, num_steps, self.soft_lqr.sigma.shape[1]), \n",
    "                        device=self.device, dtype=torch.float64) * np.sqrt(dt)\n",
    "        \n",
    "        # Pre-expand matrices for batched operations\n",
    "        H_expanded = self.soft_lqr.H.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        M_expanded = self.soft_lqr.M.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        sigma_expanded = self.soft_lqr.sigma.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Simulate trajectory using explicit scheme with vectorized operations\n",
    "        for n in range(num_steps):\n",
    "            # Current time and states\n",
    "            t_n = time_grid[n]\n",
    "            x_n = states[:, n, :]\n",
    "            \n",
    "            # Create batch of times\n",
    "            t_batch = torch.full((batch_size,), t_n, device=self.device, dtype=torch.float64)\n",
    "            \n",
    "            # Get policy distribution parameters\n",
    "            means, covariances = self.policy_network.get_action_distribution(t_batch, x_n)\n",
    "            \n",
    "            # Sample actions efficiently\n",
    "            # Generate standard normal samples\n",
    "            z = torch.randn((batch_size, self.policy_network.control_dim), \n",
    "                            device=self.device, dtype=torch.float64)\n",
    "            \n",
    "            # Transform to multivariate normal for each batch element\n",
    "            for i in range(batch_size):\n",
    "                # Compute Cholesky decomposition\n",
    "                L = torch.linalg.cholesky(covariances[i])\n",
    "                \n",
    "                # Apply transformation\n",
    "                actions[i, n, :] = means[i] + torch.mv(L, z[i])\n",
    "            \n",
    "            # Compute all drifts at once using batched matrix multiplication\n",
    "            # Reshape for bmm: [batch_size, state_dim, 1]\n",
    "            x_n_expanded = x_n.unsqueeze(2)\n",
    "            a_n_expanded = actions[:, n, :].unsqueeze(2)\n",
    "            \n",
    "            # Compute Hx: [batch_size, state_dim, 1]\n",
    "            Hx = torch.bmm(H_expanded, x_n_expanded)\n",
    "            \n",
    "            # Compute Ma: [batch_size, state_dim, 1]\n",
    "            Ma = torch.bmm(M_expanded, a_n_expanded)\n",
    "            \n",
    "            # Compute drift: [batch_size, state_dim, 1]\n",
    "            drift = Hx + Ma\n",
    "            \n",
    "            # Apply diffusion term: [batch_size, state_dim, noise_dim] x [batch_size, noise_dim, 1]\n",
    "            diffusion = torch.bmm(sigma_expanded, dW[:, n, :].unsqueeze(2))\n",
    "            \n",
    "            # Update states: [batch_size, state_dim]\n",
    "            states[:, n+1, :] = x_n + drift.squeeze(2) * dt + diffusion.squeeze(2)\n",
    "        \n",
    "        return states, actions, time_grid\n",
    "    \n",
    "    def compute_critic_targets(self, states: torch.Tensor, actions: torch.Tensor, \n",
    "                              time_grid: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        GPU-optimized computation of critic targets using vectorized operations.\n",
    "        \n",
    "        Args:\n",
    "            states: States tensor (batch x time_steps+1 x state_dim)\n",
    "            actions: Actions tensor (batch x time_steps x control_dim)\n",
    "            time_grid: Time grid tensor (time_steps+1)\n",
    "            \n",
    "        Returns:\n",
    "            Target values for each state and time step (batch x time_steps+1)\n",
    "        \"\"\"\n",
    "        batch_size = states.shape[0]\n",
    "        num_steps = time_grid.shape[0] - 1\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Initialize accumulated costs (will be transformed to targets)\n",
    "        accumulated_costs = torch.zeros((batch_size, num_steps + 1), \n",
    "                                       device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Pre-expand cost matrices for batched operations\n",
    "        C_expanded = self.soft_lqr.C.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        D_expanded = self.soft_lqr.D.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        R_expanded = self.soft_lqr.R.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Process in batches of time steps to avoid memory issues\n",
    "        step_batch_size = min(num_steps, 20)  # Process 20 steps at a time\n",
    "        \n",
    "        for step_start in range(0, num_steps, step_batch_size):\n",
    "            step_end = min(step_start + step_batch_size, num_steps)\n",
    "            \n",
    "            # Get states and actions for this batch of steps\n",
    "            batch_states = states[:, step_start:step_end, :]\n",
    "            batch_actions = actions[:, step_start:step_end, :]\n",
    "            batch_times = time_grid[step_start:step_end]\n",
    "            \n",
    "            # Compute state costs for all steps in batch\n",
    "            # Reshape for batched matrix multiplication: [batch_size, steps, state_dim, 1]\n",
    "            x_expanded = batch_states.unsqueeze(3)\n",
    "            \n",
    "            # Compute x^T C x for all states: [batch_size, steps]\n",
    "            Cx = torch.matmul(C_expanded.unsqueeze(1), x_expanded)\n",
    "            state_costs = torch.matmul(x_expanded.transpose(2, 3), Cx).squeeze()\n",
    "            \n",
    "            # Compute control costs for all steps in batch\n",
    "            # Reshape for batched matrix multiplication: [batch_size, steps, control_dim, 1]\n",
    "            a_expanded = batch_actions.unsqueeze(3)\n",
    "            \n",
    "            # Compute a^T D a for all actions: [batch_size, steps]\n",
    "            Da = torch.matmul(D_expanded.unsqueeze(1), a_expanded)\n",
    "            control_costs = torch.matmul(a_expanded.transpose(2, 3), Da).squeeze()\n",
    "            \n",
    "            # Compute entropy terms for all steps in batch\n",
    "            entropy_terms = torch.zeros((batch_size, step_end - step_start), \n",
    "                                      device=self.device, dtype=torch.float64)\n",
    "            \n",
    "            for t_idx, t in enumerate(batch_times):\n",
    "                # Get policy distribution parameters\n",
    "                t_batch = torch.full((batch_size,), t, device=self.device, dtype=torch.float64)\n",
    "                means, covariances = self.policy_network.get_action_distribution(t_batch, batch_states[:, t_idx, :])\n",
    "                \n",
    "                # Compute log probabilities\n",
    "                for i in range(batch_size):\n",
    "                    # Compute log probability of the action\n",
    "                    diff = batch_actions[i, t_idx, :] - means[i]\n",
    "                    inv_cov = torch.inverse(covariances[i])\n",
    "                    quad_term = torch.dot(diff, torch.mv(inv_cov, diff))\n",
    "                    log_det_term = torch.log(torch.det(2 * np.pi * covariances[i]))\n",
    "                    log_prob = -0.5 * (quad_term + log_det_term)\n",
    "                    \n",
    "                    # Entropy term is -τ * log_prob\n",
    "                    entropy_terms[i, t_idx] = self.soft_lqr.tau * (-log_prob)\n",
    "            \n",
    "            # Compute total costs for this batch of steps\n",
    "            step_costs = state_costs + control_costs + entropy_terms\n",
    "            \n",
    "            # Accumulate costs into running tally\n",
    "            for i in range(batch_size):\n",
    "                for j in range(step_start, step_end):\n",
    "                    accumulated_costs[i, j:] += step_costs[i, j-step_start] * dt\n",
    "        \n",
    "        # Add terminal costs\n",
    "        x_T = states[:, -1, :]\n",
    "        x_T_expanded = x_T.unsqueeze(2)  # [batch_size, state_dim, 1]\n",
    "        R_x_T = torch.bmm(R_expanded, x_T_expanded)  # [batch_size, state_dim, 1]\n",
    "        terminal_costs = torch.bmm(x_T_expanded.transpose(1, 2), R_x_T).squeeze()  # [batch_size]\n",
    "        accumulated_costs[:, -1] += terminal_costs\n",
    "        \n",
    "        return accumulated_costs\n",
    "    \n",
    "    def compute_actor_loss(self, states: torch.Tensor, actions: torch.Tensor, \n",
    "                          time_grid: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        GPU-optimized computation of actor loss using vectorized operations.\n",
    "        \n",
    "        Args:\n",
    "            states: States tensor (batch x time_steps+1 x state_dim)\n",
    "            actions: Actions tensor (batch x time_steps x control_dim)\n",
    "            time_grid: Time grid tensor (time_steps+1)\n",
    "            \n",
    "        Returns:\n",
    "            Policy loss\n",
    "        \"\"\"\n",
    "        batch_size = states.shape[0]\n",
    "        num_steps = actions.shape[1]\n",
    "        dt = self.soft_lqr.T / num_steps\n",
    "        \n",
    "        # Initialize loss\n",
    "        total_loss = torch.tensor(0.0, device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Pre-expand matrices for vectorized operations\n",
    "        C_expanded = self.soft_lqr.C.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        D_expanded = self.soft_lqr.D.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        H_expanded = self.soft_lqr.H.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        M_expanded = self.soft_lqr.M.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Process in batches of time steps to avoid memory issues\n",
    "        step_batch_size = min(num_steps, 10)  # Process 10 steps at a time\n",
    "        \n",
    "        for step_start in range(0, num_steps, step_batch_size):\n",
    "            step_end = min(step_start + step_batch_size, num_steps)\n",
    "            step_count = step_end - step_start\n",
    "            \n",
    "            # Get states and actions for this batch of steps\n",
    "            batch_states = states[:, step_start:step_end, :]  # [batch_size, step_count, state_dim]\n",
    "            batch_actions = actions[:, step_start:step_end, :]  # [batch_size, step_count, control_dim]\n",
    "            batch_times = time_grid[step_start:step_end]  # [step_count]\n",
    "            \n",
    "            # Reshape for vectorized operations\n",
    "            # [batch_size * step_count, state_dim]\n",
    "            flat_states = batch_states.reshape(-1, self.policy_network.state_dim)\n",
    "            # [batch_size * step_count, control_dim]\n",
    "            flat_actions = batch_actions.reshape(-1, self.policy_network.control_dim)\n",
    "            # [batch_size * step_count]\n",
    "            flat_times = torch.repeat_interleave(batch_times, batch_size)\n",
    "            \n",
    "            # Compute state costs: x^T C x\n",
    "            # Reshape for batched matrix multiplication: [batch_size*step_count, state_dim, 1]\n",
    "            x_expanded = flat_states.unsqueeze(2)\n",
    "            # Expand C: [batch_size*step_count, state_dim, state_dim]\n",
    "            C_flat = C_expanded.repeat_interleave(step_count, dim=0)\n",
    "            # Compute Cx: [batch_size*step_count, state_dim, 1]\n",
    "            Cx = torch.bmm(C_flat, x_expanded)\n",
    "            # Compute x^T C x: [batch_size*step_count]\n",
    "            state_costs = torch.bmm(x_expanded.transpose(1, 2), Cx).squeeze()\n",
    "            \n",
    "            # Compute control costs: a^T D a\n",
    "            # Reshape for batched matrix multiplication: [batch_size*step_count, control_dim, 1]\n",
    "            a_expanded = flat_actions.unsqueeze(2)\n",
    "            # Expand D: [batch_size*step_count, control_dim, control_dim]\n",
    "            D_flat = D_expanded.repeat_interleave(step_count, dim=0)\n",
    "            # Compute Da: [batch_size*step_count, control_dim, 1]\n",
    "            Da = torch.bmm(D_flat, a_expanded)\n",
    "            # Compute a^T D a: [batch_size*step_count]\n",
    "            control_costs = torch.bmm(a_expanded.transpose(1, 2), Da).squeeze()\n",
    "            \n",
    "            # Compute value function gradients for all states at once\n",
    "            value_gradients = self.compute_value_gradient(flat_times, flat_states)\n",
    "            \n",
    "            # Get policy distribution parameters for all states at once\n",
    "            means, covariances = self.policy_network.get_action_distribution(flat_times, flat_states)\n",
    "            \n",
    "            # Compute log probabilities and entropy terms\n",
    "            log_probs = torch.zeros(batch_size * step_count, device=self.device, dtype=torch.float64)\n",
    "            \n",
    "            for i in range(batch_size * step_count):\n",
    "                # Compute log probability of the action\n",
    "                diff = flat_actions[i] - means[i]\n",
    "                inv_cov = torch.inverse(covariances[i])\n",
    "                quad_term = torch.dot(diff, torch.mv(inv_cov, diff))\n",
    "                log_det_term = torch.log(torch.det(2 * np.pi * covariances[i]))\n",
    "                log_probs[i] = -0.5 * (quad_term + log_det_term)\n",
    "            \n",
    "            # Entropy terms: -τ * log_prob\n",
    "            entropy_terms = self.soft_lqr.tau * (-log_probs)\n",
    "            \n",
    "            # Compute drift terms\n",
    "            # Expand H and M matrices: [batch_size*step_count, state_dim, state_dim/control_dim]\n",
    "            H_flat = H_expanded.repeat_interleave(step_count, dim=0)\n",
    "            M_flat = M_expanded.repeat_interleave(step_count, dim=0)\n",
    "            \n",
    "            # Compute Hx: [batch_size*step_count, state_dim]\n",
    "            Hx = torch.bmm(H_flat, x_expanded).squeeze()\n",
    "            \n",
    "            # Compute Ma: [batch_size*step_count, state_dim]\n",
    "            Ma = torch.bmm(M_flat, a_expanded).squeeze()\n",
    "            \n",
    "            # Compute total drift: [batch_size*step_count, state_dim]\n",
    "            drift = Hx + Ma\n",
    "            \n",
    "            # Compute drift term: ∇_x v(t,x)^T (Hx + Ma)\n",
    "            # [batch_size*step_count]\n",
    "            drift_terms = torch.sum(value_gradients * drift, dim=1)\n",
    "            \n",
    "            # Total costs for all states and time steps: [batch_size*step_count]\n",
    "            total_costs = state_costs + control_costs + entropy_terms + drift_terms\n",
    "            \n",
    "            # Reshape back to [batch_size, step_count]\n",
    "            reshaped_costs = total_costs.view(batch_size, step_count)\n",
    "            \n",
    "            # Average over batch and accumulate with time step weight\n",
    "            step_loss = reshaped_costs.mean() * dt * step_count\n",
    "            total_loss = total_loss + step_loss\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def compute_critic_loss(self, states: torch.Tensor, targets: torch.Tensor, \n",
    "                           time_grid: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        GPU-optimized computation of critic loss using vectorized operations.\n",
    "        \n",
    "        Args:\n",
    "            states: States tensor (batch x time_steps+1 x state_dim)\n",
    "            targets: Target values (batch x time_steps+1)\n",
    "            time_grid: Time grid tensor (time_steps+1)\n",
    "            \n",
    "        Returns:\n",
    "            Critic loss\n",
    "        \"\"\"\n",
    "        batch_size = states.shape[0]\n",
    "        num_steps = states.shape[1] - 1\n",
    "        \n",
    "        # Initialize total loss\n",
    "        total_loss = torch.tensor(0.0, device=self.device, dtype=torch.float64)\n",
    "        \n",
    "        # Process in batches of time steps to avoid memory issues\n",
    "        step_batch_size = min(num_steps + 1, 20)  # Process 20 steps at a time\n",
    "        \n",
    "        for step_start in range(0, num_steps + 1, step_batch_size):\n",
    "            step_end = min(step_start + step_batch_size, num_steps + 1)\n",
    "            step_count = step_end - step_start\n",
    "            \n",
    "            # Get states and targets for this batch of steps\n",
    "            batch_states = states[:, step_start:step_end, :]  # [batch_size, step_count, state_dim]\n",
    "            batch_targets = targets[:, step_start:step_end]  # [batch_size, step_count]\n",
    "            batch_times = time_grid[step_start:step_end]  # [step_count]\n",
    "            \n",
    "            # Reshape for vectorized operations\n",
    "            # [batch_size * step_count, state_dim]\n",
    "            flat_states = batch_states.reshape(-1, self.policy_network.state_dim)\n",
    "            # [batch_size * step_count]\n",
    "            flat_targets = batch_targets.reshape(-1)\n",
    "            # [batch_size * step_count]\n",
    "            flat_times = torch.repeat_interleave(batch_times, batch_size)\n",
    "            \n",
    "            # Predict values for all states at once\n",
    "            flat_predicted = self.value_network.value_function(flat_times, flat_states)\n",
    "            \n",
    "            # Compute MSE loss for all states at once\n",
    "            step_loss = ((flat_predicted - flat_targets) ** 2).mean()\n",
    "            \n",
    "            # Add to total loss\n",
    "            total_loss = total_loss + step_loss * step_count\n",
    "        \n",
    "        # Average loss over all time steps\n",
    "        total_loss = total_loss / (num_steps + 1)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def train_step(self, initial_states_dist, num_states: int, num_steps: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        GPU-optimized training step with mixed precision.\n",
    "        \n",
    "        Args:\n",
    "            initial_states_dist: Distribution to sample initial states from\n",
    "            num_states: Number of initial states to sample\n",
    "            num_steps: Number of time steps for simulation\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of loss values for this step\n",
    "        \"\"\"\n",
    "        # Sample initial states\n",
    "        initial_states = initial_states_dist.sample((num_states,)).to(self.device)\n",
    "        \n",
    "        # Simulate trajectory using current policy (in float64 for accuracy)\n",
    "        states, actions, time_grid = self.simulate_trajectory(initial_states, num_steps)\n",
    "        \n",
    "        # Compute critic targets (in float64 for accuracy)\n",
    "        targets = self.compute_critic_targets(states, actions, time_grid)\n",
    "        \n",
    "        # Update critic with mixed precision\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Cast to float32 for network computation\n",
    "            states_fp32 = states.to(torch.float32)\n",
    "            targets_fp32 = targets.to(torch.float32)\n",
    "            time_grid_fp32 = time_grid.to(torch.float32)\n",
    "            \n",
    "            # Compute critic loss\n",
    "            critic_loss = self.compute_critic_loss(states_fp32, targets_fp32, time_grid_fp32)\n",
    "        \n",
    "        # Backpropagation with gradient scaling for critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        self.critic_scaler.scale(critic_loss).backward()\n",
    "        \n",
    "        # Clip gradients to avoid exploding gradients\n",
    "        self.critic_scaler.unscale_(self.critic_optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(self.value_network.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update critic weights and scaler\n",
    "        self.critic_scaler.step(self.critic_optimizer)\n",
    "        self.critic_scaler.update()\n",
    "        \n",
    "        # After updating the critic, simulate new trajectory\n",
    "        states, actions, time_grid = self.simulate_trajectory(initial_states, num_steps)\n",
    "        \n",
    "        # Update actor with mixed precision\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Cast to float32 for network computation\n",
    "            states_fp32 = states.to(torch.float32)\n",
    "            actions_fp32 = actions.to(torch.float32)\n",
    "            time_grid_fp32 = time_grid.to(torch.float32)\n",
    "            \n",
    "            # Compute actor loss\n",
    "            actor_loss = self.compute_actor_loss(states_fp32, actions_fp32, time_grid_fp32)\n",
    "        \n",
    "        # Backpropagation with gradient scaling for actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.actor_scaler.scale(actor_loss).backward()\n",
    "        \n",
    "        # Clip gradients to avoid exploding gradients\n",
    "        self.actor_scaler.unscale_(self.actor_optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_network.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update actor weights and scaler\n",
    "        self.actor_scaler.step(self.actor_optimizer)\n",
    "        self.actor_scaler.update()\n",
    "        \n",
    "        return {\n",
    "            'critic_loss': critic_loss.item(),\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'total_loss': critic_loss.item() + actor_loss.item()\n",
    "        }\n",
    "    \n",
    "    def train(self, epochs: int, batch_size: int, num_steps: int, \n",
    "             initial_states_dist, eval_freq: int = 10, verbose: bool = True,\n",
    "             evaluation_states: Optional[torch.Tensor] = None) -> Dict[str, List[float]]:\n",
    "        \"\"\"\n",
    "        GPU-optimized training with timing and learning rate scheduling.\n",
    "        \n",
    "        Args:\n",
    "            epochs: Number of training epochs\n",
    "            batch_size: Number of initial states per batch\n",
    "            num_steps: Number of time steps for simulation\n",
    "            initial_states_dist: Distribution to sample initial states from\n",
    "            eval_freq: Frequency of evaluation\n",
    "            verbose: Whether to print progress\n",
    "            evaluation_states: Optional tensor of states for periodic evaluation\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of lists of loss values during training\n",
    "        \"\"\"\n",
    "        losses = {\n",
    "            'critic_loss': [],\n",
    "            'actor_loss': [],\n",
    "            'total_loss': []\n",
    "        }\n",
    "        eval_errors = []\n",
    "        \n",
    "        # Create events for timing on GPU\n",
    "        if torch.cuda.is_available():\n",
    "            start_event = torch.cuda.Event(enable_timing=True)\n",
    "            end_event = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Start timing for this epoch\n",
    "            if torch.cuda.is_available():\n",
    "                start_event.record()\n",
    "            \n",
    "            # Perform one training step\n",
    "            step_losses = self.train_step(initial_states_dist, batch_size, num_steps)\n",
    "            \n",
    "            # Record losses\n",
    "            for key in losses.keys():\n",
    "                losses[key].append(step_losses[key])\n",
    "            \n",
    "            # Update learning rate schedulers\n",
    "            self.actor_scheduler.step(step_losses['actor_loss'])\n",
    "            self.critic_scheduler.step(step_losses['critic_loss'])\n",
    "            \n",
    "            # End timing for this epoch\n",
    "            if torch.cuda.is_available():\n",
    "                end_event.record()\n",
    "                torch.cuda.synchronize()\n",
    "                epoch_time = start_event.elapsed_time(end_event) / 1000.0  # in seconds\n",
    "            else:\n",
    "                epoch_time = 0.0\n",
    "            \n",
    "            # Evaluate periodically\n",
    "            if (epoch+1) % eval_freq == 0:\n",
    "                if evaluation_states is not None and hasattr(self.soft_lqr, 'optimal_control_distribution'):\n",
    "                    with torch.no_grad():\n",
    "                        # Compute error between learned and optimal policy\n",
    "                        t0 = torch.zeros(evaluation_states.shape[0], device=self.device, dtype=torch.float64)\n",
    "                        \n",
    "                        # Use pure FP64 for accurate evaluation\n",
    "                        learned_means, _ = self.policy_network.get_action_distribution(t0, evaluation_states)\n",
    "                        optimal_means, _ = self.soft_lqr.optimal_control_distribution(t0, evaluation_states)\n",
    "                        \n",
    "                        # Mean error in control means\n",
    "                        mean_error = torch.norm(learned_means - optimal_means, dim=1).mean().item()\n",
    "                        eval_errors.append(mean_error)\n",
    "                        \n",
    "                        if verbose:\n",
    "                            current_actor_lr = self.actor_optimizer.param_groups[0]['lr']\n",
    "                            current_critic_lr = self.critic_optimizer.param_groups[0]['lr']\n",
    "                            print(f\"Epoch {epoch+1}/{epochs}, Critic Loss: {step_losses['critic_loss']:.6f}, \"\n",
    "                                 f\"Actor Loss: {step_losses['actor_loss']:.6f}, Mean Error: {mean_error:.6f}, \"\n",
    "                                 f\"Actor LR: {current_actor_lr:.6f}, Critic LR: {current_critic_lr:.6f}, \"\n",
    "                                 f\"Time: {epoch_time:.3f}s\")\n",
    "                elif verbose:\n",
    "                    current_actor_lr = self.actor_optimizer.param_groups[0]['lr']\n",
    "                    current_critic_lr = self.critic_optimizer.param_groups[0]['lr']\n",
    "                    print(f\"Epoch {epoch+1}/{epochs}, Critic Loss: {step_losses['critic_loss']:.6f}, \"\n",
    "                         f\"Actor Loss: {step_losses['actor_loss']:.6f}, \"\n",
    "                         f\"Actor LR: {current_actor_lr:.6f}, Critic LR: {current_critic_lr:.6f}, \"\n",
    "                         f\"Time: {epoch_time:.3f}s\")\n",
    "        \n",
    "        # Plot training curves\n",
    "        if verbose:\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.semilogy(losses['critic_loss'], label='Critic Loss')\n",
    "            plt.semilogy(losses['actor_loss'], label='Actor Loss')\n",
    "            plt.title('Loss During Training (GPU Accelerated)')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss (log scale)')\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            \n",
    "            if len(eval_errors) > 0:\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.semilogy(range(eval_freq-1, epochs, eval_freq), eval_errors)\n",
    "                plt.title('Policy Mean Error During Training')\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Mean Error (log scale)')\n",
    "                plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_actor_critic():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Problem setup as specified in the exercise\n",
    "    H = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64) * 0.5\n",
    "    M = torch.tensor([[1.0, 1.0], [0.0, 1.0]], dtype=torch.float64)\n",
    "    sigma = torch.eye(2, dtype=torch.float64) * 0.5\n",
    "    C = torch.tensor([[1.0, 0.1], [0.1, 1.0]], dtype=torch.float64) * 1.0\n",
    "    D = torch.eye(2, dtype=torch.float64)  # D just identity, as specified\n",
    "    R = torch.tensor([[1.0, 0.3], [0.3, 1.0]], dtype=torch.float64) * 10.0\n",
    "    \n",
    "    # Set terminal time and time grid\n",
    "    T = 0.5\n",
    "    grid_size = 1000\n",
    "    time_grid = torch.linspace(0, T, grid_size, dtype=torch.float64)\n",
    "    \n",
    "    # Set entropy regularization parameters\n",
    "    tau = 0.5  # τ = 1/2 as specified\n",
    "    gamma = 1.0  # γ = 1 as specified\n",
    "    \n",
    "    # Create soft LQR instance for reference solution\n",
    "    soft_lqr = SoftLQR(H, M, sigma, C, D, R, T, time_grid, tau, gamma)\n",
    "    \n",
    "    # Solve Ricatti ODE for reference solution\n",
    "    print(\"Computing reference solution...\")\n",
    "    soft_lqr.solve_ricatti()\n",
    "    \n",
    "    # Create policy network (actor)\n",
    "    actor_hidden_size = 256\n",
    "    policy_network = PolicyNetwork(\n",
    "        hidden_size=actor_hidden_size, \n",
    "        state_dim=2, \n",
    "        control_dim=2, \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create value network (critic)\n",
    "    critic_hidden_size = 512\n",
    "    value_network = ValueNetwork(\n",
    "        hidden_size=critic_hidden_size,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Set learning rates\n",
    "    actor_lr = 1e-4\n",
    "    critic_lr = 1e-3\n",
    "    \n",
    "    # Create actor-critic algorithm based on device\n",
    "    if device.type == \"cuda\":\n",
    "        print(\"Using GPU-optimized actor-critic algorithm...\")\n",
    "        actor_critic = GPUActorCriticAlgorithm(\n",
    "            soft_lqr, policy_network, value_network, \n",
    "            actor_lr, critic_lr, device\n",
    "        )\n",
    "    else:\n",
    "        print(\"Using CPU-based actor-critic algorithm...\")\n",
    "        actor_critic = ActorCriticAlgorithm(\n",
    "            soft_lqr, policy_network, value_network, \n",
    "            actor_lr, critic_lr, device\n",
    "        )\n",
    "    \n",
    "    # Define initial state distribution\n",
    "    # Uniform distribution in [-3, 3] x [-3, 3]\n",
    "    class UniformInitialState:\n",
    "        def __init__(self, low=-3.0, high=3.0, dim=2, device=device):\n",
    "            self.low = low\n",
    "            self.high = high\n",
    "            self.dim = dim\n",
    "            self.device = device\n",
    "        \n",
    "        def sample(self, shape):\n",
    "            return torch.empty((*shape, self.dim), dtype=torch.float64, device=self.device).uniform_(self.low, self.high)\n",
    "    \n",
    "    initial_states_dist = UniformInitialState()\n",
    "    \n",
    "    # Create evaluation grid for periodic assessment\n",
    "    x1 = torch.linspace(-3, 3, 11, device=device, dtype=torch.float64)\n",
    "    x2 = torch.linspace(-3, 3, 11, device=device, dtype=torch.float64)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "    eval_states = torch.stack([X1.flatten(), X2.flatten()], dim=1)\n",
    "    \n",
    "    # Training parameters\n",
    "    if device.type == \"cuda\":\n",
    "        epochs = 500\n",
    "        batch_size = 128  # Larger for GPU\n",
    "        num_steps = 100  # N = 100 as specified\n",
    "        eval_freq = 25\n",
    "    else:\n",
    "        epochs = 300  # Fewer epochs for CPU\n",
    "        batch_size = 64  # Smaller for CPU\n",
    "        num_steps = 100  # N = 100 as specified\n",
    "        eval_freq = 25\n",
    "    \n",
    "    # Train the actor-critic algorithm\n",
    "    print(\"Training actor-critic algorithm...\")\n",
    "    losses = actor_critic.train(\n",
    "        epochs, batch_size, num_steps, initial_states_dist, \n",
    "        eval_freq, verbose=True, evaluation_states=eval_states\n",
    "    )\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"Final evaluation of actor-critic algorithm...\")\n",
    "    actor_critic.evaluate(eval_states, plot=True)\n",
    "    \n",
    "    # Visualize final policies\n",
    "    # Create denser grid for smoother visualization\n",
    "    x1_dense = torch.linspace(-3, 3, 21, device=device, dtype=torch.float64)\n",
    "    x2_dense = torch.linspace(-3, 3, 21, device=device, dtype=torch.float64)\n",
    "    X1_dense, X2_dense = torch.meshgrid(x1_dense, x2_dense, indexing='ij')\n",
    "    dense_states = torch.stack([X1_dense.flatten(), X2_dense.flatten()], dim=1)\n",
    "    \n",
    "    # Set time to zero\n",
    "    t0 = torch.zeros(dense_states.shape[0], device=device, dtype=torch.float64)\n",
    "    \n",
    "    # Get learned and optimal policy means\n",
    "    with torch.no_grad():\n",
    "        learned_means, learned_covs = policy_network.get_action_distribution(t0, dense_states)\n",
    "        optimal_means, optimal_cov = soft_lqr.optimal_control_distribution(t0, dense_states)\n",
    "    \n",
    "    # Convert to numpy for plotting\n",
    "    X1_np = X1_dense.cpu().numpy()\n",
    "    X2_np = X2_dense.cpu().numpy()\n",
    "    \n",
    "    learned_means_np = learned_means.cpu().numpy()\n",
    "    optimal_means_np = optimal_means.cpu().numpy()\n",
    "    \n",
    "    # Reshape for quiver plots\n",
    "    learned_u = learned_means_np[:, 0].reshape(X1_np.shape)\n",
    "    learned_v = learned_means_np[:, 1].reshape(X1_np.shape)\n",
    "    \n",
    "    optimal_u = optimal_means_np[:, 0].reshape(X1_np.shape)\n",
    "    optimal_v = optimal_means_np[:, 1].reshape(X1_np.shape)\n",
    "    \n",
    "    # Create control policy quiver plots\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.quiver(X1_np, X2_np, optimal_u, optimal_v)\n",
    "    plt.title('Optimal Policy (Soft LQR Solution)')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.quiver(X1_np, X2_np, learned_u, learned_v)\n",
    "    plt.title('Learned Policy (Actor-Critic)')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize value function\n",
    "    with torch.no_grad():\n",
    "        learned_values = value_network.value_function(t0, dense_states)\n",
    "        optimal_values = soft_lqr.value_function(t0, dense_states)\n",
    "    \n",
    "    # Convert to numpy for plotting\n",
    "    learned_values_np = learned_values.cpu().numpy().reshape(X1_np.shape)\n",
    "    optimal_values_np = optimal_values.cpu().numpy().reshape(X1_np.shape)\n",
    "    \n",
    "    # Create value function contour plots\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    contour1 = plt.contourf(X1_np, X2_np, optimal_values_np, 20, cmap='viridis')\n",
    "    plt.colorbar(contour1)\n",
    "    plt.title('Optimal Value Function (Soft LQR Solution)')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    contour2 = plt.contourf(X1_np, X2_np, learned_values_np, 20, cmap='viridis')\n",
    "    plt.colorbar(contour2)\n",
    "    plt.title('Learned Value Function (Actor-Critic)')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Test policy on trajectory simulations\n",
    "    print(\"Simulating trajectories with actor-critic policy...\")\n",
    "    \n",
    "    # Sample test initial states\n",
    "    test_initial_states = torch.tensor([\n",
    "        [2.0, 2.0],\n",
    "        [2.0, -2.0],\n",
    "        [-2.0, -2.0],\n",
    "        [-2.0, 2.0]\n",
    "    ], device=device, dtype=torch.float64)\n",
    "    \n",
    "    # Simulate trajectories with both policies\n",
    "    num_steps_sim = 200\n",
    "    dt = T / num_steps_sim\n",
    "    \n",
    "    # Generate same noise for both simulations\n",
    "    dW = torch.randn((4, num_steps_sim, sigma.shape[1]), \n",
    "                    device=device, dtype=torch.float64) * np.sqrt(dt)\n",
    "    \n",
    "    # Time grid for simulation\n",
    "    t_sim = torch.linspace(0, T, num_steps_sim + 1, device=device, dtype=torch.float64)\n",
    "    \n",
    "    # Initialize trajectories\n",
    "    opt_traj = torch.zeros((4, num_steps_sim + 1, 2), device=device, dtype=torch.float64)\n",
    "    learned_traj = torch.zeros((4, num_steps_sim + 1, 2), device=device, dtype=torch.float64)\n",
    "    \n",
    "    # Set initial states\n",
    "    opt_traj[:, 0, :] = test_initial_states\n",
    "    learned_traj[:, 0, :] = test_initial_states\n",
    "    \n",
    "    # Simulate trajectories\n",
    "    for n in range(num_steps_sim):\n",
    "        # Current time and states\n",
    "        t_n = t_sim[n]\n",
    "        t_batch = torch.full((4,), t_n, device=device, dtype=torch.float64)\n",
    "        \n",
    "        # Get control from optimal policy\n",
    "        opt_means, opt_cov = soft_lqr.optimal_control_distribution(t_batch, opt_traj[:, n, :])\n",
    "        opt_L = torch.linalg.cholesky(opt_cov)\n",
    "        \n",
    "        # Get control from learned policy\n",
    "        learned_means, learned_covs = policy_network.get_action_distribution(t_batch, learned_traj[:, n, :])\n",
    "        \n",
    "        # Use same noise for both policies\n",
    "        z = torch.randn((4, 2), device=device, dtype=torch.float64)\n",
    "        \n",
    "        # Sample controls\n",
    "        opt_actions = torch.zeros((4, 2), device=device, dtype=torch.float64)\n",
    "        learned_actions = torch.zeros((4, 2), device=device, dtype=torch.float64)\n",
    "        \n",
    "        for i in range(4):\n",
    "            opt_actions[i] = opt_means[i] + torch.mv(opt_L, z[i])\n",
    "            \n",
    "            # For learned policy, need to compute Cholesky for each covariance\n",
    "            learned_L = torch.linalg.cholesky(learned_covs[i])\n",
    "            learned_actions[i] = learned_means[i] + torch.mv(learned_L, z[i])\n",
    "        \n",
    "        # Update states\n",
    "        for i in range(4):\n",
    "            # Optimal trajectory update\n",
    "            opt_drift = H @ opt_traj[i, n, :] + M @ opt_actions[i]\n",
    "            opt_traj[i, n+1, :] = opt_traj[i, n, :] + opt_drift * dt + sigma @ dW[i, n, :]\n",
    "            \n",
    "            # Learned trajectory update\n",
    "            learned_drift = H @ learned_traj[i, n, :] + M @ learned_actions[i]\n",
    "            learned_traj[i, n+1, :] = learned_traj[i, n, :] + learned_drift * dt + sigma @ dW[i, n, :]\n",
    "    \n",
    "    # Convert to numpy for plotting\n",
    "    opt_traj_np = opt_traj.cpu().numpy()\n",
    "    learned_traj_np = learned_traj.cpu().numpy()\n",
    "    \n",
    "    # Plot trajectories\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    for i in range(4):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.plot(opt_traj_np[i, :, 0], opt_traj_np[i, :, 1], 'b-', label='Optimal Policy')\n",
    "        plt.plot(learned_traj_np[i, :, 0], learned_traj_np[i, :, 1], 'r-', label='Actor-Critic Policy')\n",
    "        plt.scatter(test_initial_states[i, 0].cpu().numpy(), test_initial_states[i, 1].cpu().numpy(), \n",
    "                   c='g', s=100, marker='o', label='Initial State')\n",
    "        plt.title(f'Trajectory from Initial State {test_initial_states[i].cpu().numpy()}')\n",
    "        plt.xlabel('x1')\n",
    "        plt.ylabel('x2')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Actor-Critic training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_actor_critic()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
